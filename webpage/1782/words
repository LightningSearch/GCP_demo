<!--mvp-fly-logo--
>
<!--mvp-fly-top-in--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-fly-top-out--
>
<!--mvp-fly-menu-top--
>
News
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
Data
Science
COVID-19
Cybersecurity
Deep
Learning
Deepfakes
Education
Environment
Ethics
Facial
Recognition
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Decision
Tree
Data
Science
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Edge
AI
&#038;
Edge
Computing
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
Generative
vs
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Linear
Regression
Long
Short-Term
Memory
(LSTM)
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Courses
Conferences
AI
Conferences
Cybersecurity
Conferences
Robotics
Conferences
Interviews
Thought
Leaders
Newsletters
Organizations
Meet
the
Team
Our
Charter
Contact
Us
<!--mvp-fly-menu-wrap--
>
Connect
with
us
<!--mvp-fly-soc-wrap--
>
<!--mvp-fly-wrap--
>
<!--mvp-search-box--
>
<!--mvp-search-but-wrap--
>
<!--mvp-search-wrap--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-nav-small-left--
>
Unite.AI
<!--mvp-nav-small-logo--
>
What
is
Transfer
Learning?
<!--mvp-drop-nav-title--
>
News
A
-
C
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
COVID-19
Cybersecurity
D
-
F
Data
Science
Deepfakes
Deep
Learning
Education
Ethics
Environment
Facial
Recognition
G
-
Q
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
R
-
Z
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
A
-
D
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
D
-
K
Dimensionality
Reduction
Edge
AI
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
(GAN)
Generative
vs.
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
L
-
Q
Linear
Regression
Long
Short-Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
R
-
Z
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Learning
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Conferences
Artificial
Intelligence
Cybersecurity
Robotics
Interviews
Thought
Leaders
Meet
the
Team
Contact
<!--mvp-nav-menu--
>
<!--mvp-nav-small-mid-right--
>
<!--mvp-nav-small-mid--
>
<!--mvp-nav-small-left-in--
>
<!--mvp-nav-small-left-out--
>
<!--mvp-nav-small-cont--
>
<!--mvp-nav-small-right-in--
>
<!--mvp-nav-small-right--
>
<!--mvp-nav-small-right-out--
>
<!--mvp-nav-small-wrap--
>
<!--mvp-main-box--
>
<!--mvp-main-nav-small-cont--
>
<!--mvp-main-nav-small--
>
<!--mvp-main-nav-wrap--
>
<!--mvp-main-head-wrap--
>
AI
Masterclass:
Terminology
(A
to
D)
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Terminology
(E
to
K)
Edge
AI
Ensemble
Learning
Federated
Learning
Generative
Adversarial
Network
Generative
vs.
Discriminative
Gradient
Boosting
Gradient
Descent
Few-Shot
Learning
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Terminology
(L
to
Q)
Linear
Regression
Long-Short
Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Terminology
(R
to
Z)
Reinforcement
Learning
Robotic
Process
Automation
Structured
vs
Unstructured
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
AI
101
What
is
Transfer
Learning?
<!--mvp-author-info-thumb--
>
Updated
5
months
ago
&nbsp;on
October
17,
2020
<!--mvp-author-info-date--
>
By
Daniel
Nelson
<!--mvp-author-info-name--
>
<!--mvp-author-info-text--
>
<!--mvp-author-info-wrap--
>
Table
Of
Contents
<!--mvp-post-img-hide--
>
What
is
Transfer
Learning?
When
practicing
machine
learning
,
training
a
model
can
take
a
long
time.
Creating
a
model
architecture
from
scratch,
training
the
model,
and
then
tweaking
the
model
is
a
massive
amount
of
time
and
effort.
A
far
more
efficient
way
to
train
a
machine
learning
model
is
to
use
an
architecture
that
has
already
been
defined,
potentially
with
weights
that
have
already
been
calculated.
This
is
the
main
idea
behind
transfer
learning
,
taking
a
model
that
has
already
been
used
and
repurposing
it
for
a
new
task.
Before
delving
into
the
different
ways
that
transfer
learning
can
be
used,
let’s
take
a
moment
to
understand
why
transfer
learning
is
such
a
powerful
and
useful
technique.
Solving
a
Deep
Learning
Problem
When
you
are
attempting
to
solve
a
deep
learning
problem,
like
building
an
image
classifier,
you
have
to
create
a
model
architecture
and
then
train
the
model
on
your
data.
Training
the
model
classifier
involves
adjusting
the
weights
of
the
network,
a
process
that
can
take
hours
or
even
days
depending
on
the
complexity
of
both
the
model
and
the
dataset.
The
training
time
will
scale
in
accordance
with
the
size
of
the
dataset
and
the
complexity
of
the
model
architecture.
If
the
model
does
not
achieve
the
kind
of
accuracy
needed
for
the
task,
tweaking
of
the
model
will
likely
need
to
be
done
and
then
the
model
will
need
to
be
retrained.
This
means
more
hours
of
training
until
an
optimal
architecture,
training
length,
and
dataset
partition
can
be
found.
When
you
consider
how
many
variables
must
be
aligned
with
one
another
for
a
classifier
to
be
useful,
it
makes
sense
that
machine
learning
engineers
are
always
looking
for
easier,
more
efficient
ways
to
train
and
implement
models.
For
this
reason,
the
transfer
learning
technique
was
created.
After
designing
and
testing
a
model,
if
the
model
proved
useful,
it
can
be
saved
and
reused
later
for
similar
problems.
Types
Of
Transfer
Learning
In
general,
there
are
two
different
kinds
of
transfer
learning
:
developing
a
model
from
scratch
and
using
a
pre-trained
model.
When
you
develop
a
model
from
scratch,
you’ll
need
to
create
a
model
architecture
capable
of
interpreting
your
training
data
and
extracting
patterns
from
it.
After
the
model
is
trained
for
the
first
time,
you’ll
probably
need
to
make
changes
to
it
in
order
to
get
the
optimal
performance
out
of
the
model.
You
can
then
save
the
model
architecture
and
use
it
as
a
starting
point
for
a
model
that
will
be
used
on
a
similar
task.
In
the
second
condition
&#8211;
the
use
of
a
pre-trained
model
&#8211;
you
merely
have
to
select
a
pre-trained
model
to
utilize.
Many
universities
and
research
teams
will
make
the
specifications
of
their
model
available
for
general
use.
The
architecture
of
the
model
can
be
downloaded
along
with
the
weights.
When
conducting
transfer
learning,
the
entire
model
architecture
and
weights
can
be
used
for
the
task
at
hand,
or
just
certain
portions/layers
of
the
model
can
be
used.
Using
only
some
of
the
pre-trained
model
and
training
the
rest
of
the
model
is
referred
to
as
fine-tuning.
Finetuning
a
Network
Finetuning
a
network
describes
the
process
of
training
just
some
of
the
layers
in
a
network.
If
a
new
training
dataset
is
much
like
the
dataset
used
to
train
the
original
model,
many
of
the
same
weights
can
be
used.
The
number
of
layers
in
the
network
that
should
be
unfrozen
and
retrained
should
scale
in
accordance
with
the
size
of
the
new
dataset.
If
the
dataset
that
is
being
trained
on
is
small,
it
is
a
better
practice
to
hold
the
majority
of
the
layers
as
they
are
and
train
just
the
final
few
layers.
This
is
to
prevent
the
network
from
overfitting.
Alternatively,
the
final
layers
of
the
pre-trained
network
can
be
removed
and
new
layers
are
added,
which
are
then
trained.
In
contrast,
if
the
dataset
is
a
large
dataset,
potentially
larger
than
the
original
dataset,
the
entire
network
should
be
retrained.
To
use
the
network
as
a
fixed
feature
extractor
,
the
majority
of
the
network
can
be
used
to
extract
the
features
while
just
the
final
layer
of
the
network
can
be
unfrozen
and
trained.
When
you
are
finetuning
a
network,
just
remember
that
the
earlier
layers
of
the
ConvNet
are
what
contain
the
information
representing
the
more
generic
features
of
the
images.
These
are
features
like
edges
and
colors.
In
contrast,
the
ConvNet’s
later
layers
hold
the
details
that
are
more
specific
to
the
individual
classes
held
within
the
dataset
that
the
model
was
initially
trained
on.
If
you
are
training
a
model
on
a
dataset
that
is
quite
different
from
the
original
dataset,
you’ll
probably
want
to
use
the
initial
layers
of
the
model
to
extract
features
and
just
retrain
the
rest
of
the
model.
Transfer
Learning
Examples
The
most
common
applications
of
transfer
learning
are
probably
those
that
use
image
data
as
inputs.
These
are
often
prediction/classification
tasks.
The
way
Convolutional
Neural
Networks
interpret
image
data
lends
itself
to
reusing
aspects
of
models,
as
the
convolutional
layers
often
distinguish
very
similar
features.
One
example
of
a
common
transfer
learning
problem
is
the
ImageNet
1000
task,
a
massive
dataset
full
of
1000
different
classes
of
objects.
Companies
who
develop
models
that
achieve
high
performance
on
this
dataset
often
release
their
models
under
licenses
that
let
others
reuse
them.
Some
of
the
models
that
have
resulted
from
this
process
include
the
Microsoft
ResNet
model
,
the
Google
Inception
Model,
and
the
Oxford
VGG
Model
group.
<!--mvp-content-main--
>
Related
Topics:
101
Transfer
Learning
<!--mvp-post-tags--
>
<!--posts-nav-link--
>
Up
Next
What
is
NLP
(Natural
Language
Processing)?
<!--mvp-prev-next-text--
>
<!--mvp-next-cont-in--
>
<!--mvp-prev-next-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-next-post-wrap--
>
Don&#039;t
Miss
What
is
a
Decision
Tree?
<!--mvp-prev-next-text--
>
<!--mvp-prev-cont-in--
>
<!--mvp-prev-cont-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-prev-post-wrap--
>
<!--mvp-prev-next-wrap--
>
<!--mvp-author-box-img--
>
Daniel
Nelson
<!--mvp-author-box-soc-wrap--
>
<!--mvp-author-box-head--
>
<!--mvp-author-box-in--
>
<!--mvp-author-box-out--
>
Blogger
and
programmer
with
specialties
in
Machine
Learning
and
Deep
Learning
topics.
Daniel
hopes
to
help
others
use
the
power
of
AI
for
social
good.
<!--mvp-author-box-text--
>
<!--author__item--
>
<!--mvp-author-box-wrap--
>
<!--mvp-org-logo--
>
<!--mvp-org-wrap--
>
<!--mvp-content-bot--
>
<!--mvp-content-body-top--
>
You
may
like
<!--mvp-related-img--
>
What
is
Few-Shot
Learning?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
Are
Transformer
Neural
Networks?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
Edge
AI
&#038;
Edge
Computing?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
Ensemble
Learning?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
Dimensionality
Reduction?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
a
Generative
Adversarial
Network
(GAN)?
<!--mvp-related-text--
>
<!--mvp-related-posts--
>
<!--mvp-cont-read-wrap--
>
<!--mvp-content-body--
>
<!--mvp-post-soc-in--
>
<!--mvp-post-soc-out--
>
<!--mvp-content-wrap--
>
<!--mvp-post-content--
>
<!--mvp-post-main-in--
>
<!--mvp-post-main-out--
>
<!--mvp-post-main--
>
<!--mvp-main-box--
>
<!--mvp-article-cont--
>
<!--mvp-article-wrap--
>
<!--mvp-main-body-wrap--
>
Meet
the
Team
Our
Charter
Press
Tools
Contact
Us
Advertiser
Disclosure
:
Unite.AI
is
committed
to
rigorous
editorial
standards
to
provide
our
readers
with
accurate
information
and
news.
We
may
receive
compensation
when
you
click
on
links
to
products
we
reviewed.
Copyright
©
2021
Unite.AI
Editorial
Policy
Privacy
Policy
Terms
and
Conditions
<!--mvp-site-main--
>
<!--mvp-site-wall--
>
<!--mvp-site--
>
<!--mvp-fly-top--
>
<!--mvp-fly-fade--
>

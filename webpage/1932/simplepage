<!DOCTYPE html>
<html>
<title>
Unite.AI Researchers Aim To Boost Drug Discovery Speed By Calculating Binding Efficiencies With AI Kushal Chakrabarti, VP of Research and Data Science at Opendoor &#8211; Interview Series Researchers Develop AI-Backed Method of Seed Analysis DataGen Secures $18 Million in Investments to Create Synthetic Data for AIs Alex Sappok, CEO of RaySecur &#8211; Interview Series IBM Recognizes 40 Innovative Female Leaders in Annual Women Leaders in AI Program Researchers Developing Self-Walking Robotic Exoskeletons  Dr. George Aronoff, Chief Medical Officer at Dosis, Inc &#8211; Interview Series Tobias Rijken, Co-Founder &#038; CTO at Kheiron Medical Technologies &#8211; Interview Series Quantum Technology Can Speed-Up Learning Process of Machines Researchers Create AI-Powered Real-Time 3D Holograms On Smartphones Julianna Ianni, Vice President, AI Research &#038; Development, Proscia &#8211; Interview Series MIT Research Team Designs AI Network To Resist Adversarial Examples Researchers Use Brain-Machine Interface To Generate Attractive Faces Based On Personal Preferences Intel-Powered Netra.AI Uses Deep Learning to Reduce Diabetic Vision Loss 
</title>

<body style="font-family:Verdana,Arial,Helvetica,sans-serif;font-size:0.9em">
<h2>
Body text
</h2>
<p>
<?xml version="1.0" encoding="UTF-8"?> <rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" > <channel > <atom:link href="https://www.unite.ai/feed/" rel="self" type="application/rss+xml" /> https://www.unite.ai <description >- AI News <lastBuildDate >Tue, 23 Mar 2021 20:45:03 +0000 <language >en-US <sy:updatePeriod > hourly <sy:updateFrequency > 1 <generator >https://wordpress.org/?v=5.7 <item > https://www.unite.ai/researchers-aim-to-boost-drug-discovery-speed-by-calculating-binding-efficiencies-with-ai/ <dc:creator > <![CDATA[Daniel Nelson]]> <pubDate >Tue, 23 Mar 2021 20:45:03 +0000 <category > <![CDATA[Data Science]]> <category > <![CDATA[Healthcare]] > <category > <![CDATA[DeepBAR]] > <category > <![CDATA[drug discovery]]> <category > <![CDATA[drugs]] > <guid isPermaLink="false">https://www.unite.ai/?p=174823 <description > <![CDATA[<img width="740" height="416" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:1116/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/molecules-1818492_1280.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Researchers from MIT have recently developed a new AI-driven technique to speed up the discovery of drugs by increasing the speed of calculations used to assess the molecular binding affinity of a drug. A drug must be able to stick to proteins in order to carry out the task it was designed for. Assessing the [&#8230;] The post Researchers Aim To Boost Drug Discovery Speed By Calculating Binding Efficiencies With AI appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="416" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:1116/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/molecules-1818492_1280.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Researchers from MIT have recently developed a new AI-driven technique to speed up the discovery of drugs by increasing the speed of calculations used to assess the molecular binding affinity of a drug . A drug must be able to stick to proteins in order to carry out the task it was designed for. Assessing the ability of a drug to stick to proteins is a major part of the drug discovery and screening process, and machine learning techniques could reduce the amount of time spent assessing this important drug attribute. The MIT research team responsible for developing the new drug assessment technique calls it DeepBAR. DeepBAR combines machine learning algorithms with traditional chemistry calculations. DeepBAR calculates the binding potential of a given candidate drug and that drug’s target proteins. The new analysis technique delivers estimates of a drug’s binding capability substantially faster than traditional methods used to assess binding affinities, and it’s hoped that the technique can enhance the speed of drug discovery. The binding potential of a drug is quantified through a metric called binding free energy, where a smaller number indicates greater binding potential. A low binding free energy score means that a drug has a great ability to compete with other molecules, filling the roles of those molecules and disrupting the normal function of a protein. There’s a high correlation between the binding free energy of a drug candidate and the effectiveness of that drug. However, measuring binding free energy can be quite difficult. There are two typical techniques used to measure free binding energies. One method is calculating the exact quantity of binding free energy, while the other is estimating the quantity of binding free energy. Estimates are less computationally expensive than exact measurements, but they obviously come with a tradeoff inaccuracy. The DeepBAR method uses a fraction of the computational power of exact measuring methods but delivers highly accurate estimates of binding energies. DeepBAR employs the “Bennett acceptance ratio”, which is the algorithm typically used to calculate binding free energy. The Bennet acceptance ratio requires the use of two baseline/endpoint states and a variety of intermediates states (which are states of partial binding). The DeepBAR approach attempts to reduce the number of calculations needed to estimate binding energies by utilizing the Bennett acceptance ratio alongside machine-learning frameworks and deep generative models. The machine learning models generate a reference state for each endpoint and these endpoints are accurate enough to the real endpoints that a Bennett acceptance ratio can be deployed. The deep generative model designed by the MIT research team is based on computer vision techniques. Essentially, DeepBAR treats every molecular structure it analyzes an as image, analyzing the features of the “image” to learn for them. The research team had to make slight changes to the algorithm to accommodate analysis of the 3D structures, as computer vision algorithms typically operate on 2D images. In initial tests, DeepBAR was able to calculate binding free energy approximately 50 times faster than traditional techniques. There is still work to be done on the model. It has to be validated against more complex, experimental data than the rather simple data it was initially tested on, which involved fairly simple data. The MIT research team aims to improve DeepBar’s ability to calculate binding free energies for large proteins by refining the model using recent advances in computer science. DeepBAR is far from the first attempt to apply AI to the drug discovery pipeline with the goal of increasing the speed of drug discovery. Many other research projects have also used AI to automate aspect of the drug discovery pipeline and improve their efficiency. However, there could be a natural bottleneck that limits the effectiveness of these strategies. As Derek Lowe recently argued in a blog on ScienceMag.org , if the goal is to enhance the speed of drug discovery, it’s important to “attack the right problems”. The evaluation of the clinical effectiveness and safety of drugs takes a substantial amount of time and finding ways to use AI to reduce clinical failure rates is difficult. Ultimately, there could be a lower bound on the amount of time AI methods can save in terms of drug discovery, at least until AI can be meaningfully integrated into the clinical evaluation process. Nonetheless, improvements are improvements and the more research like DeepBAR is done, the more time scientists will have to consider ways to use AI in other areas of the drug discovery pipeline. The post Researchers Aim To Boost Drug Discovery Speed By Calculating Binding Efficiencies With AI appeared first on Unite.AI . ]]> <item > https://www.unite.ai/kushal-chakrabarti-vp-of-research-and-data-science-at-opendoor-interview-series/ <dc:creator > <![CDATA[Antoine Tardif]]> <pubDate >Tue, 23 Mar 2021 18:05:13 +0000 <category > <![CDATA[Interviews]] > <category > <![CDATA[Interview]] > <category > <![CDATA[opendoor]] > <guid isPermaLink="false">https://www.unite.ai/?p=174714 <description > <![CDATA[<img width="740" height="492" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/Kushal.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Kushal Chakrabarti is the Vice President of Research and Data Science at Opendoor, a leading digital platform for residential real estate. What initially attracted you to machine learning and data science? I’ve always seen the world in numbers, but it wasn’t until college that I realized data science was my true calling. And I can [&#8230;] The post Kushal Chakrabarti, VP of Research and Data Science at Opendoor &#8211; Interview Series appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="492" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/Kushal.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Kushal Chakrabarti is the Vice President of Research and Data Science at Opendoor , a leading digital platform for residential real estate. What initially attracted you to machine learning and data science? I’ve always seen the world in numbers, but it wasn’t until college that I realized data science was my true calling. And I can pretty much pinpoint the moment: “Introduction to Programming with MATLAB.” Seeing that 3D contour plot on the MATLAB splash screen was “Wait a minute…” It was like love at first sight for me. To me, data science is the closest thing I’ll ever get to seeing the “mind of God.” Regardless of whether you’re religious or not, there’s clearly an underlying mechanism to how the world works. We don’t have the privilege of seeing it directly, but we do get to observe its artifacts — data. And the science and art of reconstructing that underlying mechanism is data science. Could you discuss some of the evolution that you have personally witnessed in the data science field over the years? There’ve been two broad trends I’ve noticed in my nearly two decades in the field. First is the creation of sub-disciplines. When I came up through the ranks, there wasn’t a distinction of data engineering vs. decision science vs. machine learning. If you wanted to do the fancy math, you had to do the rest as well — there simply wasn’t any other option. Today, given how far the field has come, people are now starting to specialize in sub-disciplines. That’s inevitable in any maturing field — there were only a few different kinds of doctors a hundred years ago, but dozens exist today. Second is the broad democratization of the field. Today, a great array of tools exist that make the field exponentially more accessible to newcomers. Although I certainly appreciate not having to write C++ code for the forward-backward algorithm, it’s actually a double-edged sword: many practitioners today treat those tools as black boxes and don’t understand why certain things were designed to work in certain ways, and hence have difficulty in choosing the right tool for the right job. What is your current vision for the future of data science at Opendoor? What gets me up every morning is building technology to help everyday Americans in the real world. Buying or selling a home is one of the biggest milestones in a person’s lifetime, and the work we do helps people across the U.S. achieve homeownership in a simpler, more certain and faster way. At  Opendoor , we make hundreds of data-driven decisions every day. Our teams use everything from causal inference to structural econometric models to deep learning that drive our state-of-the-art pricing models. But when you peek under the hood, there are a lot of heuristics. People use heuristics because they work, but heuristics work in the middle and fail at the edges. When you think deeply about the underlying mechanisms of how people behave, you can start abstracting out principled frameworks that strictly generalize those heuristics. It’s my vision and hope that we build out those principled frameworks so we can unlock easier, better and faster homeownership for millions more Americans. On your LinkedIn profile you invite data scientists to work with you to solve a trillion-dollar data science problem that matters to a hundred million everyday Americans. What do you look for in potential applicants? We fundamentally look for people with two distinguishing traits: a superpower to quantitatively and systematically explain the world, and an ability to pragmatically work backwards from the customer. I deeply believe in managing superpowers. In my experience, the best data scientists are T-shaped: they know a little about a lot, and a lot about little. And in my case, a lot about very little! We don’t expect people to know everything. We do, however, expect people to be exceptional at something. And if you set things up in the right ways, you can assemble a team of people with complementary superpowers that — together — can make magic happen. That’s not a cliche. As the Isaac Asimov quote goes: “Any sufficiently advanced technology is indistinguishable from magic.” Real estate is a trillion-dollar industry that has gone unchanged for decades. We’ve only begun to scratch the surface, but we’ve already set the standard for years to come. As we bring on superpowered teams and drive forward our vision, we will make magic. Check out our open roles  here . Is there anything else that you would like to share about Opendoor? Given my rather odd background — computational biology research, Amazon personalization, two-time startup founder and advisor to a dozen-plus other startups — I’ve probably seen the inside of 15-20 data science organizations. I can unequivocally say that Opendoor has the most fascinating technical challenges of any organization I’ve seen. Coupled with that is the sheer scale and impact of what we do. There simply aren’t many trillion-dollar problems in our world. There certainly aren’t many that are operating in such an archaic status quo. In just a few years, we’ve turned that model on its head and shown that there’s a faster, simpler, and more certain way to do things that combines the best of sophisticated data science and customer-centric operations. But, we’ve just begun. Figuring out how we can use new datasets and world-class data science to get to the next level is an exhilarating technical challenge that’ll help millions of Americans across the United States. The post Kushal Chakrabarti, VP of Research and Data Science at Opendoor &#8211; Interview Series appeared first on Unite.AI . ]]> <item > https://www.unite.ai/researchers-develop-ai-backed-method-of-seed-analysis/ <dc:creator > <![CDATA[Daniel Nelson]]> <pubDate >Mon, 22 Mar 2021 02:39:38 +0000 <category > <![CDATA[Data Science]]> <category > <![CDATA[agriculture]] > <category > <![CDATA[seed quality analysis]]> <category > <![CDATA[seeds]] > <guid isPermaLink="false">https://www.unite.ai/?p=174818 <description > <![CDATA[<img width="740" height="555" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:837/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/corn-264520_1280.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> A team of researchers from Brazil’s Center for Nuclear Energy in Agriculture (CENA) and the Luiz de Quieroz College of Agriculture (ESALQ) have created an AI-driven method of seed quality analysis, dramatically reducing the time needed to determine the quality of agricultural seeds. According to Phys.org, the research team collected images of seeds with the [&#8230;] The post Researchers Develop AI-Backed Method of Seed Analysis appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="555" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:837/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/corn-264520_1280.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> A team of researchers from Brazil’s Center for Nuclear Energy in Agriculture (CENA) and the Luiz de Quieroz College of Agriculture (ESALQ) have created an AI-driven method of seed quality analysis , dramatically reducing the time needed to determine the quality of agricultural seeds. According to Phys.org , the research team collected images of seeds with the use of light-based imaging technology. The techniques used by the research team included multispectral imaging and chlorophyll fluorescence. The research team selected carrots and tomatoes to serve as their experimental models, choosing different variants to produce in different countries and under different conditions. The seeds they selected were commercial tomato varieties produced in the US and in Brazil, as well as commercial carrot varieties produced in Italy, Chile, and Brazil. Demand for these crops is rising around the globe, but collecting the seeds for these crops can be difficult. Both carrots and tomatoes have ripening processes that aren’t uniform. Seed production for these crops is also non-synchronous, meaning that seeds lots extracted from these tomatoes and carrots can contain both mature and immature seeds. It isn’t easy to distinguish between mature and immature seeds with the eye, but computer vision systems can make this process easier. Traditionally, seeds are assessed by either germination and vigor tests. Germination tests involve the sowing and germination of seeds, while vigor tests aim to assess how seeds respond to stress. It can two weeks or more to obtain results from these tests, meaning that machine learning techniques are dramatically faster than these traditional seed analysis techniques. After collecting the training images, the researchers used a random forest classifier to automate the interpretation of the seed images. This optical imaging system has many advantages over traditional methods of analyzing seeds, one of which is the fact that the optical imaging technology can be used on entire batches of seeds instead of just small samples of those batches. Another advantage the method has over traditional seed assessment techniques is that the computer vision technique is non-invasive, so it doesn’t destroy any products analyzed. One method of analyzing seed quality the researchers used was chlorophyll fluorescence.  Algorithms developed by the research team made use of the presence of chlorophyll within seeds. Chlorophyll supplies the energy seeds need for development, and if the seed still has large volumes of residual chlorophyll within it, this implies the seed isn’t fully mature. This residual chlorophyll can be detected with multispectral imaging, with red light exciting the chlorophyll and special devices capturing its fluorescence and converting it into an electrical signal. Multispectral imaging involves the use of LEDs to emit light at varying points on the light spectrum. The researchers split the emitted light into 19 different wavelengths and analyzed seed quality based on reflectance for these different wavelengths. They then compared the results they obtained with quality data obtained through typical seed analysis methods. The researchers found that using near-infrared light works best for the assessment of carrot seeds while UV light worked best for the assessment of UV tomato seeds. Seeds contain proteins, sugars, and lipids that absorb certain wavelengths of light while reflecting the rest of the light. A multispectral camera is used to capture the reflective light, and the resulting image data is used to find the seeds within the entire captured image. The more of a given nutrient that a seed contains the more light wavelengths corresponding to that is absorbed. A series of algorithms is used to identify which wavelength does the best at localizing the seeds. This process can be used to provide information about the chemical composition of the seeds being studies, allowing their quality to be inferred. The research team then employed chemometrics, which are mathematical and statistical models used to classify materials, to create the classes that described seed quality. Finally, the researchers were able to use machine learning models to assess the accuracy of the chemometrics models they created. In the case of tomato seeds, the quality classification accuracy ranged from 86% to 95%. In the case of carrot seeds, the accuracy ranged from 88% to 97%. Both the chlorophyll fluorescence technique and the multispectral imaging technique proved reliable and much faster than traditional methods of assessing seed quality. If the method proves reliable, it has the potential to bring higher quality seeds to growers around the world. The post Researchers Develop AI-Backed Method of Seed Analysis appeared first on Unite.AI . ]]> <item > https://www.unite.ai/datagen-secures-18-million-in-investments-to-create-synthetic-data-for-ais/ <dc:creator > <![CDATA[Daniel Nelson]]> <pubDate >Fri, 19 Mar 2021 17:45:30 +0000 <category > <![CDATA[Data Science]]> <category > <![CDATA[data generation]]> <category > <![CDATA[synthetic data]]> <guid isPermaLink="false">https://www.unite.ai/?p=174811 <description > <![CDATA[<img width="740" height="555" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:837/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/background-213649_1280.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> The Israeli startup company DataGen has recently raised $18.5 million dollars to fund the creation of a platform dedicated to producing synthetic data for AI companies. Any artificial intelligence company faces the same core challenge, collecting the data necessary to train its AI models. The need for high-quality training data is so great that it [&#8230;] The post DataGen Secures $18 Million in Investments to Create Synthetic Data for AIs appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="555" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:837/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/background-213649_1280.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> The Israeli startup company DataGen has recently raised $18.5 million dollars to fund the creation of a platform dedicated to producing synthetic data for AI companies. Any artificial intelligence company faces the same core challenge, collecting the data necessary to train its AI models. The need for high-quality training data is so great that it has lead to an entire sub-industry dedicated to providing AI companies with the data they need to train their models. AI and AI-adjacent companies are always looking for new ways to get the data they need. One way to get this training data is to just fabricate or generate the data. As Fortune reported, DataGen specializes in using their own machine learning models to create synthetic data for other companies to train their models, particularly image and video data. The data generated by the company is then utilized by their customers to train their own AI models. According to DataGen’s CEO and founder, Ofir Chakon, the company can create an entirety synthetic dataset for a client company in just a few hours. This is substantially faster than the length of time it typically takes to prepare a dataset for use, which is often weeks or even months of labeling data. There are other reasons that synthetic data is attractive to companies, aside from the relative speed with which it can be prepared. Synthetic data doesn’t come with the kinds of privacy concerns that real data does. As more laws are created to protect people’s data privacy, it becomes more attractive to have synthetic training data. One estimate given by the technology analytics firm Gartner predicts that by 2023 around 65% of the world’s population will have their data protected by some type of data privacy law. Despite the fact that synthetic data isn’t based on real people, it can still be biased. The data generated by a synthetic data model will have the same patterns the original training data had, meaning that if a dataset is biased those biases will exist in the newly generated data. DataGen has strategies for reducing data bias in the generated data. One method for reducing bias in synthetic data is increasing the occurrence rate of relatively rare events, meaning that if one class in the dataset is under-represented its occurrence rate can be boosted up to something more equal. The technique of boosting the occurrence of rare events is incredibly important when creating datasets that involve potentially dangerous scenarios. Consider a dataset used to train an autonomous vehicle. The vehicle must reliably respond to rare events, such as a sinkhole opening up in the road. However, these events are very rare, and getting training data for these events is difficult. For this reason, training data for these rare events often need to be generated. As Chakon explained via Fortune: &#8220;Our customers have full control over all the parameters that go into the data they create. The real-world implication is that, once deployed, you can be sure it&#8217;s going to work well in different domains, with different ethnicities, in different geographic locations or any environment you can imagine.&#8221; DataGen uses Generative Adversarial Networks (GANs) to generate realistic simulations of real-world items and events. Chakon explained that the company can reliably generate realistic examples of anything that involves indoor environments or human perception. For instance, an image dataset generated by DataGen could include examples of objects used to train a robotic picking arm used for warehouse logistics, with the generated images looking indistinguishable from the real thing. DataGen’s software can generate 3D objects by combining a visual meshwork with a physics simulation system. Investors in DataGen include a variety of high-profile individuals and companies. Investors include the directors of Nvidia’s AI research division and the Max Plank Institute for Intelligent Systems, as well as Anthony Goldbloom, CEO of Kaggle. The post DataGen Secures $18 Million in Investments to Create Synthetic Data for AIs appeared first on Unite.AI . ]]> <item > https://www.unite.ai/alex-sappok-ceo-of-raysecur-interview-series/ <dc:creator > <![CDATA[Antoine Tardif]]> <pubDate >Thu, 18 Mar 2021 17:43:43 +0000 <category > <![CDATA[Interviews]] > <category > <![CDATA[Interview]] > <category > <![CDATA[mailsecur]] > <category > <![CDATA[raysecur]] > <guid isPermaLink="false">https://www.unite.ai/?p=174656 <description > <![CDATA[<img width="740" height="483" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/Sappok_Horizontal.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Alex is the CEO of RaySecur, he was previously Founder and CEO of FST, an MIT spin-out company developing advanced sensors acquired by NYSE: CTS. He holds over a dozen patents in the fields of radio frequency sensing and S.M. and Ph.D. degrees in Mechanical Engineering from MIT. Raysecur has developed breakthrough imaging technology based [&#8230;] The post Alex Sappok, CEO of RaySecur &#8211; Interview Series appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="483" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/Sappok_Horizontal.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Alex is the CEO of RaySecur, he was previously Founder and CEO of FST, an MIT spin-out company developing advanced sensors acquired by NYSE: CTS. He holds over a dozen patents in the fields of radio frequency sensing and S.M. and Ph.D. degrees in Mechanical Engineering from MIT. Raysecur has developed breakthrough imaging technology based on millimeter waves. The MailSecur systems provide real-time, 3D imaging to detect even the smallest threats – a vast improvement over traditional static 2D X-ray scanning. What initially attracted you to AI? I was initially attracted to AI partly out of curiosity for what it could enable and partly out of necessity as a tool for solving complex technical problems.  At the time I was leading a team developing a new type of sensing technology with broad applications – from cars to chemical plants.  Calibrating these sensors for a particular application was very time consuming and involved collecting large amounts of real-world data and then manually processing the data to arrive at a calibration.  AI approaches enabled a drastic reduction in calibration time and resulted in improved measurement accuracy, while being flexible enough to adapt to the diverse range of applications for our sensors. Since October 2019, you’ve been the CEO of RaySecur Inc, could you share with us what precisely Raysecur is? RaySecur is an early-stage, venture-backed, technology company based in Boston.  Our mission is to develop safe, effective, and scalable imaging technologies to “see inside” items.  Think all of the benefits of x-ray imaging without any of the drawbacks.  Using very high frequency radio-waves (terahertz) we are able to develop scanners to see inside items, such as the contents of a box, for example.  Unlike x-rays which use harmful ionizing radiation, terahertz imaging is completely safe.  The fact that it’s safe opens the door to a whole range of possibilities from real-time 3D imaging, to the ability for people to interact with an item as they are seeing the contents inside of it. Our flagship product, MailSecur TM , is a desktop-size scanner that looks something like and office copier or printer, but allows you to see inside of items.  Anyone can simply hold an item in the field of view and see a live image of the contents inside of it.  Initial applications of the technology include scanning mail for threats.  Today our systems are used by many of the world’s largest Fortune 500 companies and governments, and even to scan mail for heads of state. Outside of our core enterprise applications, I’m always amazed at the wide range of uses for our systems.   These have ranged from home offices and vacation homes for high-profile executives, to scanning mail-in ballots for threats in the 2020 presidential elections, as well as scanning mail on the campaign trail for presidential candidates. What are some of the risk factors for mail-borne threats? The biggest risk is the fact that there is very little security when it comes to physical items coming into a building.  If you think about it, it’s pretty much impossible to walk into a building today without going through multiple layers of security – whether its checking in at the front desk, badge access points, or security guards.  At the same time, every email you send is scanned multiple times for viruses, malware, and other cyber threats.  Yet when the mail arrives, whether it’s a letter or an Amazon box, it typically gets delivered directly to the recipient without any security screening.  That’s a huge risk. The problem is actually quite widespread – the U.S. Postal Inspection Service (USPIS) reported around 3500 dangerous mail incidents last year or roughly 10 per day –  yet the issue generally gets little attention.  Large companies, government institutions, and high-profile people are often at greatest risk.  Most threats come in small envelopes or packages, small enough to fit in a curbside drop box.  Just last month the New York Times reported on Dr. Anthony Fauci ending up covered in white powder after opening a letter, despite having a dedicated security detail while working on the coronavirus task force. What type of threats can MailSecur detect? Our MailSecur scanners are capable of seeing a wide range of threats including weapons and explosives.  Where the technology really excels is in imaging “soft items” like liquids and powders which make up chemical and biological items as well as hoaxes.  In fact “white powder” threats, like the letter received by Dr. Fauci last month are some of the most common threats received in the mail.  This is one area where x-ray systems struggle since x-rays are high energy and often penetrate right through powders and liquids making them nearly invisible, especially in small quantities like those found in the mail. How does MailSecur compare to traditional X-ray scanners? There are a few key differences.  One of the biggest differences is that MailSecur uses terahertz imaging which is safe, unlike x-ray scanners which use harmful ionizing radiation.  X-ray systems also require radiation permits and certifications before they can be installed, and they tend to be large like the conveyor systems seen at airports, since they contain heavy lead liners to contain the radiation. X-ray scanners also require trained operators to interpret the static 2-D images generated by the system.  On the other hand, MailSecur provides a real-time, 3-D view of the contents concealed inside an item.  Because terahertz imaging is safe, it also allows you to interact with the item you are imaging – moving it or rotating it to see it from multiple angles, just like you would interact with an object in everyday life.   Powders and liquids are easy to see because they move around in the package.  This makes the scanner really intuitive and easy to use with very little training. Another key difference with x-ray scanners, is that the MailSecur scanner is small and portable.  Just like installing a desktop printer, the system can be setup and running in minutes plugged in to any standard wall outlet.  Without the need for permits and radiation safety certifications, MailSecur is also much more scalable than x-ray.  In the past large corporations might only have a few x-ray systems at critical sites, leaving the rest unprotected.  MailSecur is inherently scalable, making it easy to secure all of a company’s facilities no matter where they are. What are some of the machine learning technologies that are used with MailSecur? We currently have an active research program supported by the National Science Foundation (NSF) to explore a number of machine learning approaches, given the unique nature of our terahertz imaging data set.  In this case we have a dynamic data set, with live terahertz video, as opposed to static images, of concealed items in full-motion.  As part of this work, we are also looking at combining the terahertz video data with contextual information like barcode tracking and optical character recognition from the mail item to get smarter about the potential risk of a particular item. Some of the ML approaches we are using include multi-category classification to train the systems to recognize certain features in the terahertz data which might be indicative of a threat to flag the item as suspicious.  Given the fact that our systems need to be capable of real-time classification in the field, we have focused on transfer learning approaches to train the system on an initial data set and further supplement this with data as it becomes available from the client.  Lastly, we also utilize generative neural networks to both enhance and tease out features in the imaging data, which are not obvious to someone looking at the initial image, through a generative process. Our systems contain internal processing capabilities to run some of these more basic models in real-time, but are also internet of things (IOT) devices leveraging AWS cloud services to develop and train the models.  Aside form ML we also apply a number of advanced machine vision approaches to further enhance and augment threat detection. In 2020, the United States Department of Homeland Security (DHS) designated MailSecur as a Qualified Anti-Terrorism Technology (QATT). How important is this for Raysecur, and what does it imply for the company moving forward?   Following 9/11, the SAFETY (Support Anti-Terrorism by Fostering Effective Technologies) Act was created by Congress in 2002 to encourage the creation of anti-terrorism products and services. The Act protects sellers of the technologies — and the companies that buy and use them — with critical liability limitations. For a new technology like MailSecur, achieving US DHS designation is extremely important, not only for the liability protections it provides to us and our customers, but also for the validation it provides in the market.  Specifically, the designation means that DHS has validated the utility and effectiveness of the technology through both scientific studies and field use. Customers may also be able to attain lower insurance premiums in recognition of this significantly reduced risk. Companies using security technologies that have not been approved for SAFETY Act protection would not experience that benefit. Is there anything else that you would like to share about RaySecur or MailSecur? In addition to all the advantages of combining advanced AI approaches with cutting-edge imaging technology, it’s important to also keep in mind the human element.  At the end of the day these systems, whether hardware or software, are tools that help our customers and their employees do their jobs better, more effectively, and most importantly keep them safe. When it comes to security applications, knowing what to do and how to respond based on the information these new tools provides is as important, if not more important than the tools themselves.  We call this creating actionable intelligence.  It is for this reason, that we have built a team of former military threat experts, who can connect to any of our scanners anywhere in the world, any time a suspicious item is identified.  These experts are able to quickly assess the situation based on the available information to escalate or de-escalate as appropriate to handle potential threats safely and efficiently. Thank you for the great interview, readers who wish to learn more should visit Raysecur .  The post Alex Sappok, CEO of RaySecur &#8211; Interview Series appeared first on Unite.AI . ]]> <item > https://www.unite.ai/ibm-recognizes-40-innovative-female-leaders-in-annual-women-leaders-in-ai-program/ <dc:creator > <![CDATA[Alex McFarland]]> <pubDate >Thu, 18 Mar 2021 13:13:11 +0000 <category > <![CDATA[Computing]] > <category > <![CDATA[AI]] > <category > <![CDATA[artificial intelligence]]> <category > <![CDATA[ibm]] > <guid isPermaLink="false">https://www.unite.ai/?p=174798 <description > <![CDATA[<img width="740" height="493" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:942/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/photo-1562705121-e624542c7b9b.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> IBM has recognized 40 innovative female business leaders today as part of the company’s annual Women Leaders in AI program. The women come from 18 different countries and use IBM Watson to help advance industries.  This year, the women leaders include names from The Clorox Company, City of Austin, The Depository Trust and Clearing Corporation, [&#8230;] The post IBM Recognizes 40 Innovative Female Leaders in Annual Women Leaders in AI Program appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="493" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:942/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/photo-1562705121-e624542c7b9b.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> IBM has recognized 40 innovative female business leaders today as part of the company’s annual Women Leaders in AI program. The women come from 18 different countries and use IBM Watson to help advance industries.  This year, the women leaders include names from The Clorox Company, City of Austin, The Depository Trust and Clearing Corporation, EY, Ford Motor Company, Servicenow, and more. According to a newly released study from the IBM Institute for Business Value that surveyed business professionals, 70 percent of global businesses still do not consider gender equality a top priority despite the recent raise in awareness and developments. At the same time, the study found that fewer women surveyed are in the senior vice president, vice president, director, and manager roles this year compared to 2019. Women Leaders in AI Program IBM’s Women Leaders in AI Program was first created in 2019 to help facilitate diverse participation in the field. This year’s women leaders have helped optimize their organizations with better predictions, automated processes, and new efficiencies through the use of natural language processing (NLP), automation, and artificial intelligence.  Ritika Gunnar is Vice President at Expert Labs, IBM Cloud and Cognitive Software.  “As AI adoption continues to accelerate, we believe that diverse teams are needed to help build and implement trustworthy AI that can help mitigate bias and deliver explainable outcomes,” said Gunnar. “Today, we are sharing the incredible stories of 40 women who are paving the way forward in AI for business and impacting how people work and live. Working with clients in an era of global workplace and workforce change, we see the need for women and diverse teams working at the forefront of AI.” According to IBM, the leaders were selected “based upon the ways they are using AI as a transformation agent to help drive results for their organizations and the employees, customers and citizens they serve.” Some of the specific examples include: President and CEO of The Ad Council, Lisa Sherman, used IBM Watson Advertising Accelerator to improve the effectiveness of the Love Has No Labels “Fight for Freedom” initiative. Manoela Morais, Chimka Munkhbayar and Helen Tsai developed Agrolly, a solution that empowers independent farmers with crop forecasts. Poonam Verma, who oversees The Depository Trust and Clearing Corporation’s (DTCC) critical cyber security services function, used IBM services to advance the cyber detection capabilities of the organization. Ekaterina Ostankova, along with her team at Lloyds Banking Group, used IBM Watson to help design and define a solution for the recent rise in customer service calls. Annie Shu, Manager of Strategy and Innovation at Australian bank Westpac, used IBM Watson to help develop an avatar that helps teenagers find their first job opportunities. 2021 IBM Women Leaders in AI Honorees Here is a look at this year’s list of honorees:  Joan Francy, CEO, Admed Inc (United States) Chimka Munkhbayar, Co-founder, Agrolly (Mongolia) Manoela Morais, CEO and Co-founder, Agrolly (Brazil) Helen Tsai, Web Developer Lead and Co-founder, Agrolly (Mongolia) Airei Soh, Marketing IT Strategy and Planning, All Nippon Airways (Japan) Leah Karlin, Director of Machine Learning and Data Engineering, At Point of Care (United States) Fella Benaziza, UX Designer Consultant, Capgemini France (France) Elisabetta Burei, BRM Technology CRM, CheBanca! (Spain) Divya Rathanlal, Emerging Technology Program Manager, City of Austin Texas (United States) Regina Olivares, Principal Management Analyst, Clark County (United States) Siew Choo Soh, Managing Director, DBS (Singapore) Sandra Corkern, Associate Director, Real Estate Technology and Innovation, EY (Ernst &amp; Young LLP) (United States) Jennifer Turner, Senior Director, Digital Innovation, Strategy and Transactions, EY-Parthenon (United States) Sabita Sharma, AVP, Automation and Innovation,Everest Re Group (United States) Tracey Hawes, General Manager, Marketing and Trading, Fine Wine Delivery (New Zealand) Julie Losee, Manager, Equipment Lease Accounting, Ford Motor Company (United States) Denise Stokowski, Group VP, Platform Products, Gainsight (United States) Hagit Tzafrir, VP Healthcare Division, Harel insurance LTD (Israel) Annapurna Vishwanathan, Formerly Head of Digital, HCCB (Hindustan Coca-Cola Beverages Pvt. Ltd.) (India) Curren Katz, Director of Data Science R&amp;D, Highmark Health (United States) Satoko Maeda, Assistant Manager, Hiroshima Prefecture (Japan) Mari Sasaki, Section Chief, Kakuichi (Japan) Anette Böhm, General Manager Corporate HR, KBC Group NB, (Belgium) Ekaterina Ostankova, Product Manager – Virtual Assistant Lab, Lloyds Banking Group (United Kingdom) Kaori Matsue, Manager, RC and Quality Assurance Division, Mitsui Chemical Group (Japan) Alu Rodríguez, SVP Business Transformation, NH Hotel Group (Spain) Jeanette Fürst, Director of Sales, OpenAdvice IT Services (Germany) Donatella Sciuto, Professor of Computer Engineering and Deputy Rector, Politecnico di Milano (Italy) Bharathi Ramadass, IT Product Manager, ServiceNow(United States) Gema Pérez Ramón, Director, Taxation Office, Tax Administration Agency, Madrid Municipality (Spain) Melissa Dorey, Digital Experience Principal, Telstra(Australia) Lisa Sherman, President and CEO, The Ad Council (United States) Pam Griffin, Associate Director, Cleaning Division, The Clorox Company (United States) Poonam Verma, Managing Director, Head of Security Engineering and Operations, The Depository Trust and Clearing Corporation (DTCC) (United States) Supaluck Umpujh, Chairman, The Mall Group(Thailand) Manami Endo, IT Planning Section, Information System Department, TRUSCO Nakayama Corporation (Japan) Mio Sugihara, Assistant Manager, System Management Section, TRUSCO Nakayama Corporation (Japan) Catharine Fennell, CEO, videoBIO (Canada) Amy Oding, Operational Support and Automation Manager, Vodafone (New Zealand, APAC Annie Shu, Manager of Strategy and Innovation, Westpac (Australia) You can learn more about this year’s honorees on IBM’s site .  &nbsp; The post IBM Recognizes 40 Innovative Female Leaders in Annual Women Leaders in AI Program appeared first on Unite.AI . ]]> <item > https://www.unite.ai/researchers-developing-self-walking-robotic-exoskeletons/ <dc:creator > <![CDATA[Alex McFarland]]> <pubDate >Wed, 17 Mar 2021 19:06:05 +0000 <category > <![CDATA[Robotics]] > <category > <![CDATA[AI]] > <category > <![CDATA[artificial intelligence]]> <guid isPermaLink="false">https://www.unite.ai/?p=174791 <description > <![CDATA[<img width="740" height="416" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/robot-2301646__480-2.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Robotics researchers at the University of Waterloo are working on one of the next big developments within the field of robotics: self-walking robotic exoskeletons. The new technology, which includes prosthetic legs, can think and move on its own through the use of artificial intelligence (AI). The newly developed systems rely on a combination of AI [&#8230;] The post Researchers Developing Self-Walking Robotic Exoskeletons  appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="416" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/robot-2301646__480-2.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Robotics researchers at the University of Waterloo are working on one of the next big developments within the field of robotics: self-walking robotic exoskeletons. The new technology, which includes prosthetic legs, can think and move on its own through the use of artificial intelligence (AI). The newly developed systems rely on a combination of AI and computer vision to mimic the way people walk by observing their surroundings and adjusting movements. The research was published in the journal IEEE Transactions on Medical Robotics and Bionics. It was part of a longer series of research papers, and it is titled “ Simulation of Stand-to-Sit Biomechanics for Robotic Exoskeletons and Prostheses with Energy Regeneration .” Brokoslaw Lashowski is a PhD candidate in systems design engineering. She is currently leading the research project, ExoNet, at the university. “We’re giving robotic exoskeletons vision so they can control themselves,&#8221; said Laschowski. Despite their already being exoskeleton legs controlled by motors, they are not ideal given the required manual control by users through smartphone applications or joysticks.  &#8220;That can be inconvenient and cognitively demanding,&#8221; said Laschowski. &#8220;Every time you want to perform a new locomotor activity, you have to stop, take out your smartphone and select the desired mode.&#8221; AI and Wearable Cameras In order to improve this system and overcome some of those limitations, the research team fitted exoskeletons to users and utilized wearable cameras. They also optimized AI computer software so that the video feed could be processed efficiently and accurately, recognizing obstacles like stairs, doors, and various other aspects of the environment.  The research team will now look to send instructions to motors, enabling the robotic exoskeletons to become even more impressive. Once this is achieved, they will be able to climb stairs and avoid various obstacles. On top of that, they will be able to analyze the user’s current movement, along with the upcoming terrain, in order to take appropriate actions. Laschowski is supervised by John McPhee, an engineering professor and the Canada Research Chair in Biomechatronic System Dynamics. &#8220;Our control approach wouldn&#8217;t necessarily require human thought,&#8221; said Laschowski. &#8220;Similar to autonomous cars that drive themselves, we&#8217;re designing autonomous exoskeletons and prosthetic legs that walk for themselves.&#8221; Another one of the researchers’ goals is to improve the energy efficiency of motors for robotic skeletons by using self-charged batteries through human motion.  Advancing Exoskeletons The new work coming from the research team really takes the development of exoskeletons to the next step. Over the last few years, there have been various advances in the field. For example, Honda announced in 2019 that they were developing exoskeletons to augment human strength . At the same time, exoskeletons were being used in Japan to help people continue working as they age . However, self-walking robotic skeletons are far more difficult to achieve, which is what makes this new advancement so exciting and valuable.  &nbsp; The post Researchers Developing Self-Walking Robotic Exoskeletons  appeared first on Unite.AI . ]]> <item > https://www.unite.ai/dr-george-aronoff-chief-medical-officer-at-dosis-inc-interview-series/ <dc:creator > <![CDATA[Antoine Tardif]]> <pubDate >Wed, 17 Mar 2021 16:34:26 +0000 <category > <![CDATA[Healthcare]] > <category > <![CDATA[Interviews]] > <category > <![CDATA[dosis]] > <category > <![CDATA[Interview]] > <guid isPermaLink="false">https://www.unite.ai/?p=174573 <description > <![CDATA[<img width="740" height="541" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/02/Dr.-George-Aronoff.png-take-3.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Dr. George Aronoff  is the Chief Medical Officer at Dosis, and he has over 30 years of experience in nephrology. He was previously Chief of Nephrology &#38; Hypertension at the University of Louisville, where his research with Drs. Brier and Gaweda focused on using AI to dose ESAs in dialysis patients. He received his M.S. [&#8230;] The post Dr. George Aronoff, Chief Medical Officer at Dosis, Inc &#8211; Interview Series appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="541" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/02/Dr.-George-Aronoff.png-take-3.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Dr. George Aronoff  is the Chief Medical Officer at Dosis , and he has over 30 years of experience in nephrology. He was previously Chief of Nephrology &amp; Hypertension at the University of Louisville, where his research with Drs. Brier and Gaweda focused on using AI to dose ESAs in dialysis patients. He received his M.S. in Pharmacology and M.D. from Indiana University. Dosis&#8217;​ first product, Strategic Anemia   Advisor, is a web-based reference tool that personalizes dosing of ESAs, a class of drugs used to treat chronic anemia. Could you begin by explaining what is the innovative AI-based dosing platform Strategic Anemia Advisor (SAA)? Dosis’ Strategic Anemia Advisor (SAA) is an artificial intelligence (AI)-based clinical decision support system that’s been engineered to improve health outcomes of End-Stage Kidney Disease (ESKD) patients and reduce drug costs by an average of 25 percent by personalizing drug dosing. More than 550,000 End-Stage Renal Disease (ESRD) patients in the U.S. are currently undergoing dialysis treatment, and the majority of those patients experience chronic anemia. SAA is based on more than 10 years of research at the University of Louisville and was specifically designed to assist clinical anemia managers with their recommendations for Erythropoiesis Stimulating Agents (ESAs) dosing. What are some of the benefits of offering personalized dosing recommendations? AI helps clinicians determine the minimum dose required to achieve the desired therapeutic outcome, which has both clinical and economic benefits. In the case of dosing ESAs, inefficient dosing can result in significantly higher than necessary drug exposure for patients, and correspondingly elevated costs of care. SAA is focused on fine-tuning dose titrations based on a patient’s demonstrated drug response. As dose changes are made regularly, it is in a patient’s best interest to receive the smallest amount of a drug, as greater exposure to ESA is associated with higher risk of heart attack, stroke, thrombosis, and cancer recurrence. As the leader in this area, Dosis’s SAA delivers a solution that has had proven results, which have allowed it to gain widespread acceptance by top dialysis organizations. To date, SAA has been used to deliver over 2 million dosing recommendations. Drug dosing driven by AI is gaining ground in many areas of medicine, such as dialysis, cancer and transplant medicine. It is in these areas that increasingly precise dosing plays a critical role in achieving favorable outcomes. AI-powered precision dosing is particularly impactful in the management of drugs used to manage chronic conditions, as both the potential for adverse events and the cost of care increase over the months and years that patients are on these drugs. How is artificial intelligence used to identify the recommended dosage amount? SAA uses artificial intelligence to place patients on a spectrum of ESA dose response, from extreme responder (someone who is very sensitive to a drug) to, essentially, non-responder. This estimation is done by evaluating a patient’s historical response to the drug and constructing a unique response profile for each patient. With each subsequent dose and hemoglobin response, SAA refines that estimation to more precisely achieve the target hemoglobin using the lowest possible ESA dose. What type of reduction in medication utilization have clinics seen from this? With consistent use of SAA, clinics have seen on average a 25% reduction in ESA utilization with maintained or improved anemia outcomes, as well as a 75% reduction in time spent managing anemia. Could you discuss how AI-powered precision dosing will likely be the standard of care for chronic disease management in the future? To inform dosing decisions, doctors have historically relied primarily on their clinical experience, knowledge of the medications they are prescribing, and paper-based recommendations for dosing from drug manufacturers and the FDA. However, these recommendations are often imprecise, as they draw from clinical studies that may or may not accurately reflect an individual patient’s response to the medication. Precision dosing has been identified as a crucial method to maximize therapeutic safety and efficacy with significant potential benefits for patients and healthcare providers, and AI-powered solutions have so far proven to be among the most powerful tools to actualize precision dosing. Today, five factors have come together to make AI-powered drug dosing a reality. They include: Technological advancements in computing, which allow us to process large, complex datasets quickly, making AI solutions practical. Public familiarity with artificial intelligence as an effective tool for solving complex problems, which makes physicians comfortable incorporating such tools in clinical settings. Reliable data is now available in electronic medical records and is standardized in a manner that is much more ingestible by algorithms as compared to free-form paper medical records. Big data analytics techniques have also made applying artificial intelligence and control algorithms to complex datasets much more practical and efficient. Today, we can draw on data from millions of patients to design and test algorithms in silicon to predict effectiveness and iterate quickly. This is a vast improvement on expert systems that are based on a clinician’s smaller number of patients, possibly in the thousands or hundreds, that are generally only possible to test in much more costly and risky clinical trials. Increasingly complex and powerful drugs have been developed that impact basic physiologic processes. Drugs that impact multiple physiologic processes and have a narrow therapeutic window (the “sweet spot” between toxicity and ineffective therapy) have become more prevalent. These are the types of drugs for which AI-powered drug dosing can provide the most benefit. Taking it a step beyond precision dosing, what are some of your views on the overall future of personalized medicine? In 10 years, I believe AI-driven dosing models will likely be the standard of care across the healthcare spectrum, used for a wide variety of drugs like warfarin, insulin and immunosuppressives. Basically, any drug that is administered chronically and has a narrow therapeutic range is a good candidate for AI-driven dosing. In addition, as more tools are developed and more opportunities to use those tools are identified, we will see exponential growth in the use of AI to drive therapies, interpret laboratory and radiographic findings, and predict outcomes of therapeutic strategies. Is there anything else that you would like to share about Dosis? Dosis is uniquely positioned to implement AI driven decision support and has a track record of translating high level academic research into practical clinical applications at both small and large scale. Thank you for the great interview, readers who wish to learn more should visit Dosis . The post Dr. George Aronoff, Chief Medical Officer at Dosis, Inc &#8211; Interview Series appeared first on Unite.AI . ]]> <item > https://www.unite.ai/tobias-rijken-co-founder-cto-at-kheiron-medical-technologies-interview-series/ <dc:creator > <![CDATA[Antoine Tardif]]> <pubDate >Tue, 16 Mar 2021 16:42:53 +0000 <category > <![CDATA[Interviews]] > <category > <![CDATA[Interview]] > <category > <![CDATA[kheiron Medical]]> <guid isPermaLink="false">https://www.unite.ai/?p=174575 <description > <![CDATA[<img width="740" height="494" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/02/tobias.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Tobias Rijken is the Co-Founder &#38; CTO at Kheiron Medical Technologies, a medical imaging company that uses advanced machine learning technologies to develop and provide intelligent tools for radiologists, radiology departments, imaging centers and hospitals to improve efficiency, consistency and accuracy of radiology reporting. Kheiron Medical Technologies was founded with the sole focus of helping [&#8230;] The post Tobias Rijken, Co-Founder &#038; CTO at Kheiron Medical Technologies &#8211; Interview Series appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="494" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/02/tobias.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Tobias Rijken is the Co-Founder &amp; CTO at Kheiron Medical Technologies , a medical imaging company that uses advanced machine learning technologies to develop and provide intelligent tools for radiologists, radiology departments, imaging centers and hospitals to improve efficiency, consistency and accuracy of radiology reporting. Kheiron Medical Technologies was founded with the sole focus of helping radiologists detect breast cancer earlier with machine learning software. What initially attracted you to machine learning? My informal introduction to machine learning was when I first started learning programming when I was a young teenager. There was this games engine where you could program your own games. I wanted to play this game, but who do I play against? It could be another player, but it could also be a program. How do you build a program that learns how to play a game? It wasn’t a modern AI, it was rule-based AI. I beat it every time, so it was not a good AI. When I was studying for my master’s degree at University College London, we entered a new era of modern AI. DeepMind had just been acquired, and I was studying machine learning at the place where DeepMind was actually founded. David Silver, one of the leading scientists of DeepMind, was teaching a course on reinforcement learning . As part of the course, I built an agent to learn how to play a game &#8211; a simplified form of blackjack. When I finished the AI and played against my own creation, I couldn&#8217;t beat it anymore. And that was a real revelation. My formal introduction to machine learning came in my first year of university. I was studying math and computer science at Amsterdam University College. At the same time, massive new open online courses were established and I could join AI courses from leading figures in the field. The course that made the biggest impact was with Peter Norvig and Sebastian Thrun from Stanford, and I could study AI with 100,000 people at the same time. Now with Kheiron, what I like so much about machine learning is the application it has on the real world. The opportunity it has to solve real world problems. That’s what really excites me. Could you discuss the genesis story behind Kheiron Medical? Peter and I met in 2016 at the UK accelerator Entrepreneur First. We connected straight away &#8211; probably because of our similar backgrounds. We both come from medical families &#8211; Peter’s mother is a radiologist, I grew up in a medical family, surrounded by doctors, and both of us chose to go down the STEM route. Our backgrounds gave us a clear view of the struggles and challenges that healthcare workers face and an understanding of how AI can improve their lives as well as the lives of patients. We share a belief that AI together with clinicians will change healthcare for the better. There’s a big problem to be solved here. There’s a global shortage of radiologists. A lot of people die from cancer &#8211; more than 40,000 American women die every year from breast cancer. Part of the reason so many people die from cancer is that doctors don’t have the right information to make the right decisions. AI can help with that. First and foremost, Kheiron is here to help the radiologist so the radiologist can help the patients. I personally get very motivated by digging into the problem to understand its intricacies. We all work hard to really understand the radiologists’ work, the data they are working with on a daily basis, their workflow. Then we can figure out how we can deploy AI as part of this workflow. We invest heavily in deeply understanding the users and the data. We have radiologists on our team who help us integrate the user needs into the design and development process. We have a patient involvement initiative that engages directly with women to understand their fears and needs for breast screening. Many people ask me about the name of our company &#8211; Kheiron. Kheiron was a wise, gentle centaur &#8211;  a creature that is part man, part horse in Greek mythology &#8211; who trained the heroes in medicine. This idea of two halves making up something more powerful than the individual parts is the inspiration for Kheiron &#8211; AI and the clinician working together to change what’s possible in the standard of care. Can you discuss the breast cancer screening solutions that are offered? We’re working on several breast cancer solutions along the breast cancer screening pathway. Our first product is called Mia™ &#8211; which stands for mammography intelligent assessment. Mia has been optimised to perform the same task as radiologist &#8211; namely, to determine whether the woman should be called back for further examination or not. The level of performance we’re achieving allows us to rethink the workflow. For American women, this is incredibly meaningful. The U.S. has what is called a “single-reader workflow”. This means that each mammogram is read by one radiologist. However, the U.K. and Europe have a “double-reader workflow”, meaning that every mammogram is read by two radiologists. Many American radiologists consider double reading to be the “gold standard” because it is more likely than a single reader to detect early, small breast cancers. Our clinical studies prove that Mia is a clinically safe and cost-effective option as an independent or concurrent reader. This means that in the U.S., Mia can be used together with the radiologist to achieve that gold standard of a double-reading workflow &#8211; with a single human reader. Like I said earlier, this is truly how we see the power of AI and the human coming together to solve big problems. The first product we’re bringing to the U.S. market is called Mia IQ™, which sits one step earlier in the breast cancer screening pathway. Mia IQ helps the radiography technicians analyze the quality of the image. If we can improve the quality of the image, then the quality of the reading will also go up. And this means that more cancers will be detected sooner, when the outcomes can be better for the woman. This is very important for MQSA quality assurance audits and for continuous training, which is where we expect to see Mia IQ’s first applications. Mia IQ provides screening programs with reports detailing positioning problems. This will allow the program directors to provide spot training for techs and address any broader image quality problems that may affect their quality assurance audits. How many images has the neural network been trained on? For us, the quantity of data is not the point. The good thing about breast cancer screening space is there’s a lot of data available. There’s a lot of historical data because screening programs have been around for quite a while. And because you screen an entire population, there’s a lot of new data being generated. The challenge is that the data is heavily skewed. 99% of the screening population doesn’t have cancer, luckily, which means there’s a skew toward normal images. There are also population differences, with a lack of diversity in the data. So for Kheiron, the challenge isn’t about getting the quantity of data. Instead, where a large part of our success comes from is understanding which data to collect to get the best results, and to be very selective and targeted in where and how we get our data. Our collaboration with Emory University is a great example of how we’re doing this. Are false positives currently an issue? False positives are only part of the issue. The science of screening relies on tradeoffs between false positives and false negatives. For instance, we can achieve a very low false positive rate, but we will miss some cancers. On the other hand, we can have very few missed cancers if we have a high false positive rate. It’s important to understand those trade-offs and to balance your screening service based on priorities and resources. Cultural and policy differences also exist between countries, which influence the way these tradeoffs are made. In some countries, missing a cancer is considered very problematic. In other countries, it’s seen to be more important to minimize unnecessary recalls because of the healthcare cost associated with each recall. Ultimately, it’s about understanding how this tradeoff impacts patients, in terms of the number of unnecessary recalls and further tests experienced by the ‘false positive’ group versus the ‘false negative’ group whose cancers go undetected. It’s a difficult balancing act, but AI promises to raise the bar for both groups. Kheiron Medical Technologies and Emory University recently announced a collaboration to evaluate data from prior mammograms on over 50,000 African American women who have been screened at Emory Healthcare. Could you share more details about this project? Our partnership with Emory University broadens the diversity of data we are using to ensure Mia, our AI breast cancer screening solution, works to the same standard for any woman, regardless of ethnicity. The first project in the collaboration will support the validation of Mia’s performance against Emory’s diverse screening population. This is a multi-site study spanning four screening sites and hospitals. A cohort of more than 200,000 screening mammograms will be included in this project &#8211; approximately 45% of women in that cohort are of African American descent. We announced this collaboration in late November and have made great progress since then, even considering the challenges of operating under a global pandemic and remote working conditions. I think that is a huge credit to the team at Emory, who are also clinicians providing excellent care to patients in the community. Right now our collaboration is investing the upfront energy to establish a strong foundation for our future work. We’re at a stage where we’re working closely with the Emory team to understand clinical workflows. This serves as the necessary foundation for deep analysis and interrogation of potential differences between subpopulations, like ethnic groups. This work is not easy or fast. Not many things in healthcare are. However, it is very rewarding, and we’re lucky to benefit from a set of global partners who are extremely collaborative and aligned with us on our shared mission: to help any woman anywhere have a fighting chance against breast cancer. How large of an issue is the under-representation of non-whites for image data for cancer diagnosis? Historically, the diversity of patient populations has not been mirrored in medical research to the extent that it should be. Those of us developing AI have a responsibility to include ethnic diversity in our studies. Under-represented populations are an issue for breast cancer screening. Outcomes for African American women are an example of this. African American women tend to have higher mortality rates (39% higher than non-Hispanic whites), and this may be attributed to access to health care, delays in diagnosis, and delays in treatment initiation, among other factors. Generalizability of AI across various populations is a key step toward addressing racial disparities in breast screening. I’m excited that our Kheiron team is committed to building a solution that performs equally well on mammograms from racially diverse populations &#8211; and this is a big motivating factor for our work. Our collaboration with Emory and UCSF will help us achieve this vision of generalizability &#8211; making sure that Mia gives every woman, everywhere a better fighting chance against breast cancer. Thank you for the great interview and for working on such important life saving technologies. Readers who wish to learn more should visit Kheiron Medical Technologies. The post Tobias Rijken, Co-Founder &#038; CTO at Kheiron Medical Technologies &#8211; Interview Series appeared first on Unite.AI . ]]> <item > https://www.unite.ai/quantum-technology-can-speed-up-learning-process-of-machines/ <dc:creator > <![CDATA[Alex McFarland]]> <pubDate >Mon, 15 Mar 2021 18:10:11 +0000 <category > <![CDATA[Quantum Computing]]> <category > <![CDATA[AI]] > <category > <![CDATA[artificial intelligence]]> <category > <![CDATA[quantum computers]]> <category > <![CDATA[Robotics]] > <guid isPermaLink="false">https://www.unite.ai/?p=174773 <description > <![CDATA[<img width="740" height="493" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:942/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/physics-3871216_1280.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> A new experiment at the University of Vienna demonstrated how quantum technology can speed up the learning process of machines. The physicists involved in the work used a quantum processor for single photons as a robot. The reseach was published in Nature.  There have been major developments recently within the field of quantum computing, and the [&#8230;] The post Quantum Technology Can Speed-Up Learning Process of Machines appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="493" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:942/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/physics-3871216_1280.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> A new experiment at the University of Vienna demonstrated how quantum technology can speed up the learning process of machines. The physicists involved in the work used a quantum processor for single photons as a robot. The reseach was published in  Nature .  There have been major developments recently within the field of quantum computing, and the power of such technologies is continuously being realized. This has led to the technology being used in real-life applications, and now experts want to merge artificial intelligence (AI) and autonomous machines with quantum physics and algorithms.  Learning Process To achieve this, scientists have been looking into how quantum mechanics can help the learning process of robots, and the other way around. Some of the results have shown how robots can move faster or how quantum experiments can use new learning techniques. Despite moving faster, the robots have still not been able to learn faster, which is needed for the development of complex autonomous machines.  Phillip Walther led an international effort headed by a team of physicists at the university. They were joined by theoreticians from the University of Innsbruck, the Austrian Academy of Sciences, the Leiden University, and the German Aerospace Center. The collaboration succeeded in experimentally proving the speeding up of a robot’s learning time. The team relied on single photons and an integrated photonic quantum processor designed by MIT. The processor was used as a robot, learning how to route single photons to a predefined direction. Valeria Saggio is first author of the publication. &#8220;The experiment could show that the learning time is significantly reduced compared to the case where no quantum physics is used,&#8221; says Saggio. The Superposition Principle The robot can learn by being rewarded for completing the correct move. In a classical world, for example with a left and right turn, only one can be chosen and correct. However, with quantum technology, the robot is able to use the superposition principle, meaning it can take both of those turns at the same time.  Hand Briegel and his team at the University of Innsbruck developed the theoretical ideas on quantum learning agents. &#8220;This key feature enables the implementation of a quantum search algorithm that reduces the number of trials for learning the correct path. As a consequence, an agent that can explore its environment in superposition will learn significantly faster than its classical counterpart,&#8221; says Briegel. According to Walther, “We are just at the beginning of understanding the possibilities of quantum artificial intelligence and thus every new experimental result contributes to the development of this field, which is currently seen as one of the most fertile areas for quantum computing.&#8221; &nbsp; The post Quantum Technology Can Speed-Up Learning Process of Machines appeared first on Unite.AI . ]]> <item > https://www.unite.ai/researchers-create-ai-powered-real-time-3d-holograms-on-smartphones/ <dc:creator > <![CDATA[Daniel Nelson]]> <pubDate >Mon, 15 Mar 2021 15:36:32 +0000 <category > <![CDATA[Augmented Reality]]> <category > <![CDATA[augmented reality]]> <category > <![CDATA[Convolutional neural networks]]> <category > <![CDATA[holograms]] > <guid isPermaLink="false">https://www.unite.ai/?p=174767 <description > <![CDATA[<img width="740" height="416" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:1116/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/research-4170441_1920.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Smartphones could soon be able to generate photorealistic 3D holograms, thanks in part to an AI model developed by researchers at MIT. The AI system developed by the MIT team determines the best way to generate holograms from a series of input images. Researchers from MIT have recently designed AI models that enable the generation [&#8230;] The post Researchers Create AI-Powered Real-Time 3D Holograms On Smartphones appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="416" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:1116/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/research-4170441_1920.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Smartphones could soon be able to generate photorealistic 3D holograms, thanks in part to an AI model developed by researchers at MIT . The AI system developed by the MIT team determines the best way to generate holograms from a series of input images. Researchers from MIT have recently designed AI models that enable the generation of photorealistic 3D holograms. The technology could have applications for VR and AR headsets, and the holograms can even be generated by a smartphone. Unlike traditional 3D and VR displays, which simply produce the illusion of depth and which can cause nausea and headaches, holographic displays can be seen by people without causing eye strain. A major roadblock towards the creation of holographic media is handling the data needed to actually generate the holograph. Every hologram is comprised of a massive amount of data, needed to create the “depth” the hologram has. Because of this, generating holographic typically requires a massive amount of computing power. In order to make holographic technology more practical, the MIT team applied deep convolutional neural networks to the problem, creating a network capable of quickly generating holograms based on input images. The typical approach for generating holograms essentially generated many chunks of holograms and then used physics simulations to combine the chunks into a complete representation of an object or image. This differs from the typical approach used to generate holograms. In the traditional method, images are sliced apart and a series of lookup tables are used to join the hologram chunks together, as the lookup tables mark the boundaries of the different hologram chunks. The process of defining boundaries of holographic chunks with look tables is quite a time-consuming and processing power intensive. According to IEEE Spectrum , The MIT team designed another method of generating holograms. Using the power of deep learning networks, they were able to slice images into chunks that could be re-complied into holograms using far fewer “slices”. The new techniques takes advantage of the ability of convolutional neural networks to analyze images and separate images into discrete chunks. This new method of analyzing and chunking images greatly reduces the number of total operations a system has to carry out. In order to design their AI-powered holographic generator, the research team began by constructing a database comprised of around 4000 computer-generated images, with a corresponding 3D hologram for assigned to each of these images. The convolutional neural network was trained on this dataset, learning how each of the images was tied with its hologram and the best way to use features to generate the holograms. When the AI system was provided unseen data with depth information, it could then generate new holograms from this data. The depth information is supplied through the use either lidar sensors of multi-camera displays and rendered as a computer-generated image. Some new iPhone have these components, meaning that they could potentially generate the holograms if linked to the right type of display. The new AI-driven hologram system needs much less memory than the classic methods. The system can generate 3D holograms at 60 frames a second in full color with a resolution of 1920 x 1080 using around 620 kilobytes of memory while running on a single commonly available GPU. The researchers were able to run their systems on an iPhone 11 producing around 1 hologram a second, while a Google Edge TPU the system could render 2 holograms per second. This suggests that the system could be adapted to smartphone, AR devices, and VR devices in general. The system could also have applications for volumetric 3D printing or in the design of holographic microscopes. In the future, improvements to the technology could introduce eye-tracking hardware and software, enabling holograms to dynamically scale in resolution as the user looks at particular places. The post Researchers Create AI-Powered Real-Time 3D Holograms On Smartphones appeared first on Unite.AI . ]]> <item > https://www.unite.ai/julianna-ianni-vice-president-ai-research-development-proscia-interview-series/ <dc:creator > <![CDATA[Antoine Tardif]]> <pubDate >Mon, 15 Mar 2021 13:43:33 +0000 <category > <![CDATA[Interviews]] > <category > <![CDATA[Interview]] > <category > <![CDATA[pathology]] > <category > <![CDATA[Proscia]] > <guid isPermaLink="false">https://www.unite.ai/?p=174587 <description > <![CDATA[<img width="740" height="462" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/02/Julianna-1.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Julianna Ianni leads Proscia’s AI R&#38;D to develop high-quality computational pathology products. Proscia is pushing labs past the limits of traditional tools with its Concentriq® digital pathology platform and pipeline of computational applications. Operating at the center of the modern digital laboratory, these technologies are transforming the economics and practice of pathology, putting the power [&#8230;] The post Julianna Ianni, Vice President, AI Research &#038; Development, Proscia &#8211; Interview Series appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="462" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/02/Julianna-1.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> Julianna Ianni leads Proscia’s AI R&amp;D to develop high-quality computational pathology products. Proscia is pushing labs past the limits of traditional tools with its Concentriq ® digital pathology platform and pipeline of computational applications. Operating at the center of the modern digital laboratory, these technologies are transforming the economics and practice of pathology, putting the power of modern, data-centric medicine to work in the fight against cancer. Could you share the genesis story behind Proscia? Pathology sits at the core of biomedical research and cancer diagnosis; yet, while we’ve all seen digitization impact almost every other aspect of healthcare, pathology has remained largely unchanged over its 150-year history. It still centers around a pathologist recognizing patterns in tissue under the microscope. This practice is inherently manual and subjective, two challenges that are further compounded by a shrinking pathologist population and a rising cancer burden. Proscia was founded to advance this standard of care. Our founders recognized an opportunity to take on the fight against cancer by driving pathology’s shift from microscope to images. In doing so – in driving digital pathology forward – we are enabling laboratories to streamline operations, achieve meaningful improvements in quality and productivity, and unlock new insights unseen by the human eye. Collectively, these benefits are helping researchers to accelerate breakthroughs and helping pathologists to improve outcomes for patients around the world. Since our early days, we have amassed a customer base of leading laboratories, health systems, life sciences companies, and research organizations. Among our many enterprise customers, we work with Johns Hopkins University, the University of Pennsylvania, and 10 of the top 20 pharmaceutical companies. We also recently announced that LabPON, the first laboratory in the world to reach 100% digital pathology diagnosis, is transitioning to our software platform, and the Joint Pathology Center, which houses the world’s largest repository of human tissue data, is going digital with Proscia. Proscia is a digital and computational pathology company. Can you explain to our readers what this means? At a high level, digital pathology is the practice of digitizing glass microscope slides using a scanner, which can then be viewed, managed, shared, and analyzed using software, including computational applications leveraging AI. Proscia specifically focuses on the software side of things. We offer a core platform, called Concentriq, that provides all of the enterprise functionality that laboratories need for carrying out their routine pathology operations. Concentriq also serves as a launchpad for AI applications, including a suite of solutions that we are building. I highlighted many of the benefits of AI-enabled digital pathology, including driving efficiency and productivity gains and the ability to unlock new insights, above, so let’s dive into some of the specific use cases to see how these play out. One that’s especially top of mind right now is enabling remote operations during COVID. As laboratories have had to adjust to new ways of working to accommodate social distancing, digital pathology has enabled pathologists to continue working and serve patients since it enables them to easily share images and view them on-demand with a platform like Concentriq. The alternative is often for laboratories to have physical glass slides delivered to pathologists’ homes. Think about how time consuming and expensive this could be! More generally, when we look at how digital and computational pathology solutions like those Proscia delivers are enabling pathologists to improve quality and efficiency – whether by making it easier to share images for a second opinion or by helping to eliminate error-prone, manual tasks in the lab – it is ultimately making it possible for patients to receive the right diagnosis faster, and this is important when we consider that early treatment often leads to better outcomes. Can you describe how machine learning is being used in pathology today? Where is it going? That’s a big question! Machine learning has really shown promise in a number of areas in pathology. One common use case for machine learning in pathology is to identify specific regions in an image where tumor tissue is located to draw a pathologist’s attention there. It can also be used to deliver quantitative insights about tissue samples &#8211; for example, counting the number of cells actively undergoing division (a common marker of cancer). Some are also working on classification problems, like being able to categorize images based on diagnosis or particular patterns that they represent, and still others are working on ways to use machine learning to predict patient outcomes or responses to specific therapies. There is so much interesting work going on in the space! Ultimately, in pathology, most of these machine learning use cases are aimed at solving a few overarching clinical and research problems. The first is the volume problem I mentioned earlier. There is an increasing number of cases to review, and there’s this compounding issue of a decreasing number of pathologists available to perform the diagnosis of these cases. Much of machine learning in pathology aims to improve the efficiency of diagnosis at the individual and laboratory-level. The second major one is the quality of diagnosis and care &#8211; how can we improve diagnostic accuracy, how can we improve prognosis, and how can we improve patient outcomes at the end of the day? To answer the second part of your question, I should distinguish between what’s at the research stage today, and what’s actually happening in clinical practice. Right now, most of the work in the field has been research, and it can be extremely challenging to translate some of the findings to clinical practice. That’s where I think machine learning is going and needs to go &#8211; building out the systems and the level of quality needed to actually put some of the fantastic research being done into practice, in a way that holds up to all of the amazing findings we’re seeing in the research setting and delivering these benefits to pathology labs and ultimately their patients. Building AI that works in the “real world” is and has always been Proscia’s approach. Proscia’s DermAI leverages deep learning to pre-screen and automatically classify hundreds of variants of skin disease into pre-diagnostic categories. What were your key considerations in designing and developing this application? First and foremost, we had to consider what we were building the system to do. We wanted it to be able to classify any skin lesion, not just a specific type. And there is quite a lot of variability in skin pathology, as you said, hundreds of variants. So, we needed to make sure we had all of that variation well-represented in our training set. This can actually be quite challenging, as some types of lesions are rarer than others, and it can be difficult to build a dataset that has enough examples of some of these rarer pathologies to train with and ensure that we had enough examples for our model to learn from. Second, we really had to think about the fact that we weren’t just building something that needed to work at a single site, or for images scanned with a particular type of scanner. It was really about building something that could work on any laboratory’s images, on any scanner. There can be quite a lot of variation between sites and scanners in terms of the appearance of the image &#8211; colors, lightness, artifacts etc. We had to develop a system that could account for all of these variations, and not require a massive amount of data to calibrate and get it working for a new site. We had several other considerations to account for in terms of building an AI system that could operate in the “real world.” One more, which was especially important for us, was representing the “unknowns.” After all, we know that AI systems are never perfect, and there’s so much variability in skin lesions. We needed DermAI to know what it didn’t know and be able to deliver that information when it was too uncertain to make a good classification. That’s why we built into the system a method to assign each classification a confidence score, and we took care to design this in such a way that it would be correlated with the system’s performance &#8211; the higher the confidence score, the more likely that classification is to be correct. This means we can basically tune system performance; if the way I’m using this in my lab requires extremely high accuracy, I can set DermAI to deliver high-confidence classifications only. If my use case is a little more tolerant of error and I prefer to have more cases classified, then I can include lower-confidence classifications as well. DermAI was validated in one of pathology’s most comprehensive studies to date. Can you summarize the study and its key takeaways? This was a study of extremely ambitious scope. As I just mentioned, there’s a tremendous amount of variability in skin lesions, which translates into their corresponding pathology images, and we wanted to develop a system that could automatically classify any routinely prepared skin pathology slide – from any lab and any scanner. The idea was that pathologists could use it to sort and triage cases before they sit down to make a diagnosis &#8211; prioritizing cases in an order that makes sense rather than in the random order they naturally come in and making sure the right cases go to the right pathologist rather than having to be sent somewhere else later. And no one had demonstrated anything close to that when we began developing this a few years ago. The system we developed broke down the task of classifying images into multiple stages using a combination of deep learning and basic computer vision techniques &#8211; detecting the tissue on the slide, adapting the image appearance so that it’s in a familiar space to the trained system, detecting the relevant regions of interest, and finally making a classification into four different categories based on the patterns present in the tissue. The important part though, was how we tested it. While we only trained the system on 5,000 images from a single site, we calibrated and tested it on almost three times as many images from three entirely separate institutions whose data our system had never seen. By doing this, we showed it was possible to build an AI application to sort and triage skin biopsies, that could perform well on multiple sites with minimal calibration. Since the data we tested on mimicked each of these sites’ prospective workload, we could be confident that the performance demonstrated here would be comparable to what we’d see if we were to install DermAI in a lab. And because the system is tunable by adjusting the confidence threshold I mentioned before, depending on what percentage of images we classify, we can tune it up to at least 98% accuracy. One of the observations in the study was how deep learning algorithms can be sensitive to image artifacts. What precisely are these image artifacts in this case and what are some solutions to solve this problem? Yes, a few studies have demonstrated the sensitivity of AI systems to image artifacts in pathology as in any other field. These are often simple things that our human brains overlook easily &#8211; dirt on a slide, slight changes in lighting, blurry regions of an image, pen ink that pathologists often use to mark regions of tumor. I’m listing a few examples but there are countless others. AI systems can be easily fooled by these sorts of issues if they haven’t been properly exposed to them. There are really two routes to handling image artifacts for AI systems. The first is by cleaning &#8211; making sure that you’re training on and testing images have been meticulously cleaned, either digitally or physically, so there aren’t any artifacts present. This is sometimes easy to do for a training dataset but much harder to do consistently if you’re looking to install an AI system at many sites. So, we took the second approach: making sure that these kinds of artifacts were well represented in our data. We didn’t have slides cleaned before they were sent to us so we have a lot of representation of some of the strange issues that you might not see in a pristine training set, but that you’ll certainly ultimately encounter in the real world. This way, we could ensure that our system was ready when exposed to these artifacts in images it wasn’t trained on. How are AI applications like DermAI being implemented in the pathology laboratory? That’s a great question. While different laboratories are taking different approaches, we believe that the only way laboratories will truly scale their AI adoption is by leveraging an AI-enabled platform. As I described above, laboratories’ digital pathology operations center around a platform that they use for viewing, managing, and analyzing images. Proscia’s platform, Concentriq, provides all of this functionality and also serves as a launchpad for AI applications. We believe that this approach makes it easy for laboratories to deploy AI in practice, seamlessly integrating it into their day-to-day work so that they can leverage it at scale and realize its true promise. Is there anything else that you would like to share about Proscia? I’m really excited about the work that my team and I are doing. The unfortunate reality today is that we all know someone who has been impacted by cancer and the significant impact that it has on them and their loved ones. Our work has the potential to improve patient outcomes and truly make a meaningful difference. This is something I’m proud to be a part of. To that end, it’s great to see that so many leading organizations also believe in the work that we are doing. Over the past few months alone, the Joint Pathology Center (JPC), which houses the world’s largest repository of human tissue data, selected Proscia to digitize this archive. JPC has several reasons for wanting to go digital, including accelerating the development of AI. LabPON, the first laboratory in the world to reach 100% digital pathology diagnosis, also recently selected Proscia’s platform to scale its pathology operations and lay the foundation for implementing AI. LabPON will also collaborate with us on the development and validation of our AI systems. And finally, we recognize that we can’t transform the practice of pathology alone and are constantly growing our team. If you’re interested in joining us, I encourage you to learn more about Proscia and view our open roles . The post Julianna Ianni, Vice President, AI Research &#038; Development, Proscia &#8211; Interview Series appeared first on Unite.AI . ]]> <item > https://www.unite.ai/mit-research-team-designs-ai-network-to-resist-adversarial-examples/ <dc:creator > <![CDATA[Daniel Nelson]]> <pubDate >Mon, 15 Mar 2021 03:13:00 +0000 <category > <![CDATA[Artificial Neural Networks]]> <category > <![CDATA[adversarial examples]]> <category > <![CDATA[deep learning]]> <category > <![CDATA[deep reinforcement learning]]> <category > <![CDATA[reinforcement learning]]> <guid isPermaLink="false">https://www.unite.ai/?p=174761 <description > <![CDATA[<img width="740" height="407" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:1142/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/question-mark-1872634_1920.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> A team of researchers from MIT has developed a deep-learning algorithm intended to help AIs cope with “adversarial” examples, which can cause an AI to make the wrong predictions and carry out the wrong actions. The algorithm designed by the MIT team can help AI systems maintain their accuracy and avoid making mistakes when faced [&#8230;] The post MIT Research Team Designs AI Network To Resist Adversarial Examples appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="407" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:1142/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/question-mark-1872634_1920.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> A team of researchers from MIT h as developed a deep-learning algorithm intended to help AIs cope with “adversarial” examples, which can cause an AI to make the wrong predictions and carry out the wrong actions. The algorithm designed by the MIT team can help AI systems maintain their accuracy and avoid making mistakes when faced with confusing data points. AI systems analyze the input features of an event to decide how to respond to that event. An AI responsible for maneuvering an autonomous vehicle has to take data from the vehicle’s cameras and decide what to do based on the data contained in those images. However, there’s the chance that the image data being analyzed by the AI isn’t an accurate representation of the real world. A glitch in the camera system could alter some of the pixels, leading to the AI drawing incorrect conclusions about the appropriate course of action. “Adversarial inputs” are like optical illusions for an AI system. They are inputs that confuse an AI in some form. Adversarial inputs can be crafted with the express goal of causing an AI to make mistakes, by representing data in a fashion that makes the AI believe that the contents of an example are one thing instead of another. For instance, it is possible to create an adversarial example for a computer vision system by making slight changes to images of cats, causing the AI to mis-classify the images as computer monitors. The MIT research team designed an algorithm to help guard against adversarial examples by letting the model maintain a degree of “skepticism” about the inputs it receives. The MIT researchers called their approach “Certified Adversarial Robustness for Deep Reinforcement Learning ,” or CARRL. CARRL is composed of a reinforcement learning network and a traditional deep neural network joined together.  Reinforcement learning uses the concept of “rewards” to train a model, giving the model proportionally more reward the closer it comes to hitting its goal. The reinforcement learning model is used to train a Deep Q-Netowrkk, or DQN. DQNs function like traditional neural networks, but they also associate input values with a level of reward, much like reinforcement learning systems. CARRL operates by modeling a range of different possible values for input data. Assuming that the AI is trying to track the position of a dot within a larger image, the AI considers that the dot’s position could be the result of the adversarial influence and considers regions where the dot could be instead. The network then makes decisions based on the worst-case scenario for the dot’s position, settling on the action that would produce the highest reward in this worst-case scenario. The typical method of guarding against adversarial examples involves running slightly altered versions of the input image through the AI network to see if the same decision is always made. If alterations to the image don’t dramatically affect the outcome, there’s a good chance the network is resistant to adversarial examples. However, this isn’t a viable strategy for scenarios where quick decisions need to be made, as these are time-intensive, computationally expensive methods of testing. For this reason, the MIT team set out to create a neural network that could make decisions based on worst-case assumptions, one capable of operating in scenarios where safety is critical. The MIT researchers tested their algorithms by having the AI play a game of Pong. They included adversarial examples by feeding the AI instances where the ball was displayed slightly further down the screen than it actually was. As the influence of the adversarial examples grew, the standard corrective techniques began to fail while CARRL was able to win more games by comparison. CARRL was also tested on a collision avoidance task. The task unfolded in a virtual environment where two different agents tried to switch positions without bumping into each other. The research team altered the first agent’s perception of the second agent and CARRL was able to successfully steer the first agent around the other agent, even in conditions of high uncertainty, although there did come a point where CARRL became too cautious and ended up avoiding its destination altogether. Regardless, MIT Department of Aeronautics and Astronautics Postdoc Michael Everett, who lead the study, explained that the research could have implications for the ability of robots to handle unpredictable situations. As Everett explained via MIT News: “People can be adversarial, like getting in front of a robot to block its sensors, or interacting with them, not necessarily with the best intentions,” Everett says. “How can a robot think of all the things people might try to do, and try to avoid them? What sort of adversarial models do we want to defend against? That’s something we’re thinking about how to do.” The post MIT Research Team Designs AI Network To Resist Adversarial Examples appeared first on Unite.AI . ]]> <item > https://www.unite.ai/researchers-use-brain-machine-interface-to-generate-attractive-faces-based-on-personal-preferences/ <dc:creator > <![CDATA[Daniel Nelson]]> <pubDate >Thu, 11 Mar 2021 21:08:46 +0000 <category > <![CDATA[Brain Machine Interface]]> <category > <![CDATA[brain machine interface]]> <category > <![CDATA[Brain-Computer Interface]]> <category > <![CDATA[GAN]] > <category > <![CDATA[image generation]]> <guid isPermaLink="false">https://www.unite.ai/?p=174754 <description > <![CDATA[<img width="740" height="523" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:889/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/head-663993_960_720.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> A team of researchers from the University of Helsinki has created an AI intended to generate images of attractive faces, based on the features that individuals wearing a Brain-Computer Interface (BCI) finds attractive. The AI generates facial features based on the data collected by the BCI. The research team was a combination of computer scientists [&#8230;] The post Researchers Use Brain-Machine Interface To Generate Attractive Faces Based On Personal Preferences appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="740" height="523" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:889/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/head-663993_960_720.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> A team of researchers from the University of Helsinki has created an AI intended to generate images of attractive face s, based on the features that individuals wearing a Brain-Computer Interface (BCI) finds attractive. The AI generates facial features based on the data collected by the BCI. The research team was a combination of computer scientists and psychologists from the University of Helsinki. The Helsinki research team used electroencephalography (EEG) measurements to determine the facial features different people might find attractive. The EEG signals were correlated with facial features, and then the data was fed to a Generative Adversarial Network (GAN). The machine learning system was then trained on the facial features a wide variety of people found attractive and then was able to reverse engineer these patterns to generate entirely new faces. The researchers had 30 participants sit in front of a screen as images of faces were shown to them. These faces weren’t of real people, they were generated by an AI trained on a dataset of over 200,000 images of celebrities. The participants wore an EEG cap wired up with electrodes to record and analyze their brain activity as they viewed the different faces. The EEG was able to record their reactions to faces they found attractive. The measurements taken by the EEG system were fed to the GAN, which interpreted the EEG signals in terms of how attractive the participants found the face. The GAN was able to generate new faces once trained on this data. The research team then carried out a second experiment. The newly created faces were displayed to the same volunteers who had participated in the earlier viewing session. The participants were asked to rank the faces in terms of attractiveness. When the results of the study were analyzed, the researchers found the participants rated the generated images as attractive approximately 80% of the time. This is in contrast to the original images, which were rated as attractive only 20% of the time. The sample size of the study was rather small, so it’s not clear how robust the method would be when tested on a larger population. However, the results are interesting and they are certainly another example of how behaviors and preferences that seem inscrutable can be quantified with certain AI techniques. Michael Spapé, a senior researcher at the University of Helsinki’s Department of Psychology and Logopedics, explained that the study shows how psychological properties can be demonstrated with information about how the brain responds to stimuli. As Spapé explained in via EurekaAlert: &#8220;The study demonstrates that we are capable of generating images that match personal preference by connecting an artificial neural network to brain responses. Succeeding in assessing attractiveness is especially significant, as this is such a poignant, psychological property of the stimuli. Computer vision has thus far been very successful at categorizing images based on objective patterns. By bringing in brain responses to the mix, we show it is possible to detect and generate images based on psychological properties, like personal taste.&#8221; The researchers argue that the study could have implications for how computers understand subjective preferences. AI solutions and brain-computer interfaces can be used alongside each other to understand complex psychological phenomena. According to Spapé, we may be able to look into other cognitive functions, like decision making and perception, using similar techniques. Assuming the general tactics used to interpret attractiveness hold true for other cognitive functions, a similar system could be developed to identify forms of bias or stereotypes. The post Researchers Use Brain-Machine Interface To Generate Attractive Faces Based On Personal Preferences appeared first on Unite.AI . ]]> <item > https://www.unite.ai/intel-powered-netra-ai-uses-deep-learning-to-reduce-diabetic-vision-loss/ <dc:creator > <![CDATA[Alex McFarland]]> <pubDate >Thu, 11 Mar 2021 19:33:17 +0000 <category > <![CDATA[Healthcare]] > <category > <![CDATA[AI]] > <category > <![CDATA[artificial intelligence]]> <category > <![CDATA[healthcare]] > <guid isPermaLink="false">https://www.unite.ai/?p=174751 <description > <![CDATA[<img width="720" height="480" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/eyes-5248678__480.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> A new development coming from a collaboration between Sankara Eye Foundation and the Singapore-based Leben Care could have big implications for diabetic retinopathy (DR), especially in India. An Intel-powered, cloud-based artificial intelligence (AI) solution relies on deep learning to identify retinal conditions in a short period of time. The accuracy matches that of human doctors, [&#8230;] The post Intel-Powered Netra.AI Uses Deep Learning to Reduce Diabetic Vision Loss appeared first on Unite.AI . ]]> <content:encoded > <![CDATA[<img width="720" height="480" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.eTEe~1cccd/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/03/eyes-5248678__480.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" /> A new development coming from a collaboration between Sankara Eye Foundation and the Singapore-based Leben Care could have big implications for diabetic retinopathy (DR), especially in India. An Intel-powered, cloud-based artificial intelligence (AI) solution relies on deep learning to identify retinal conditions in a short period of time. The accuracy matches that of human doctors, and the system, caled Netra.AI, can drastically reduce screening burdens. Prakash Mallya is vice president and managing director of Sales, Marketing, and Communications Group at Intel India.  “The use of AI to improve disease detection and prevention is a critical step for the healthcare industry and a giant leap for humankind,” Mallya says. “India has one of the largest diabetic populations in the world and diabetic retinopathy is the major cause for vision loss and blindness in persons of working age. With Netra.AI, Sankara Eye Foundation and Leben Care have leveraged the power of Intel Xeon Scalable processors and built-in Intel Deep Learning (DL) Boost to accurately detect DR and enable timely treatment to effectively combat avoidable vision impairment and blindness in diabetic patients.” Impact in India This new development is especially important for India as the nation has one of the largest diabetic populations in the world. Current estimates put the total number of cases at 98 million by 2030, and according to research, DR is a leading cause of blindness and vision loss in adults.  Early detection and treatment plays a major role in stopping damage, but the country, especially in rural regions, suffers from a lack of trained retinal specialists. Without the required resources, it is difficult to reach an effective screening. Dr. Kaushik Murali is president of Medical Administration, Quality &amp; Education at Sankara Eye Foundation in India.  “Technology and AI are democratizing healthcare access, especially in screening for ailments,” said Dr. Mural. “Our team at Sankara Eye Foundation has focused on our vision to eliminate needless blindness from India. The current solution, Netra.AI — where we had a key role in the design and development with Leben Care — uses robust AI-enabled platforms from Intel. It is an example of how like-minded collaborators can create meaningful and impactful solutions for various challenges that face humanity.” Netra.AI  The Intel-powered Netra.AI works by analyzing images from portable and technician-operated camera devices, which provide immediate DR grading results via a cloud-based web portal. With the use of AI algorithms and neural networks, Netra.AI helps detect DR stages. The team behind the new development believes it can eventually be used in other areas, such as analyzing other retinal conditions and glaucoma. It can also help reduce the amount of time healthcare specialists must dedicate to screening, enabling resources to be focused in more important areas. The Netra.AI system has screened 3,093 patients in India and identified 742 at-risk individuals. It puts together detailed reports in as little as two minutes, and immediate diagnosis can be achieved. It relies on Intel Xeon Scalable processors with built-in Intel Deep Learning Boost and Intel Advanced Vector Extension 512 acceleration.  &nbsp; The post Intel-Powered Netra.AI Uses Deep Learning to Reduce Diabetic Vision Loss appeared first on Unite.AI . ]]> 
<p>

<h2>
Base = <a href=""></a>
</h2>

<h2>
Links
</h2>
<p>
<a href="https://www.unite.ai/researchers-aim-to-boost-drug-discovery-speed-by-calculating-binding-efficiencies-with-ai/">https://www.unite.ai/researchers-aim-to-boost-drug-discovery-speed-by-calculating-binding-efficiencies-with-ai/</a> ( Researchers Aim To Boost Drug Discovery Speed By Calculating Binding Efficiencies With AI )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://news.mit.edu/2021/drug-discovery-binding-affinity-0315">https://news.mit.edu/2021/drug-discovery-binding-affinity-0315</a> ( Researchers from MIT )
</p>
<p>
<a href="https://pubs.acs.org/doi/abs/10.1021/acs.jpclett.1c00189">https://pubs.acs.org/doi/abs/10.1021/acs.jpclett.1c00189</a> ( assess the molecular binding affinity of a drug )
</p>
<p>
<a href="https://www.unite.ai/what-is-machine-learning/">https://www.unite.ai/what-is-machine-learning/</a> ( machine learning )
</p>
<p>
<a href="https://www.unite.ai/what-is-computer-vision/">https://www.unite.ai/what-is-computer-vision/</a> ( computer vision )
</p>
<p>
<a href="https://blogs.sciencemag.org/pipeline/archives/2021/03/19/ai-and-drug-discovery-attacking-the-right-problems">https://blogs.sciencemag.org/pipeline/archives/2021/03/19/ai-and-drug-discovery-attacking-the-right-problems</a> ( argued in a blog on ScienceMag.org )
</p>
<p>
<a href="https://www.unite.ai/researchers-aim-to-boost-drug-discovery-speed-by-calculating-binding-efficiencies-with-ai/">https://www.unite.ai/researchers-aim-to-boost-drug-discovery-speed-by-calculating-binding-efficiencies-with-ai/</a> ( Researchers Aim To Boost Drug Discovery Speed By Calculating Binding Efficiencies With AI )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/kushal-chakrabarti-vp-of-research-and-data-science-at-opendoor-interview-series/">https://www.unite.ai/kushal-chakrabarti-vp-of-research-and-data-science-at-opendoor-interview-series/</a> ( Kushal Chakrabarti, VP of Research and Data Science at Opendoor &#8211; Interview Series )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/what-is-data-science/">https://www.unite.ai/what-is-data-science/</a> ( Data Science )
</p>
<p>
<a href="https://www.opendoor.com/">https://www.opendoor.com/</a> ( Opendoor )
</p>
<p>
<a href="https://www.unite.ai/what-is-machine-learning/">https://www.unite.ai/what-is-machine-learning/</a> ( machine learning )
</p>
<p>
<a href="http://opendoor.com/">http://opendoor.com/</a> ( Opendoor )
</p>
<p>
<a href="https://www.unite.ai/what-is-deep-learning/">https://www.unite.ai/what-is-deep-learning/</a> ( deep learning )
</p>
<p>
<a href="http://opendoor.com/careers">http://opendoor.com/careers</a> ( here )
</p>
<p>
<a href="https://www.unite.ai/kushal-chakrabarti-vp-of-research-and-data-science-at-opendoor-interview-series/">https://www.unite.ai/kushal-chakrabarti-vp-of-research-and-data-science-at-opendoor-interview-series/</a> ( Kushal Chakrabarti, VP of Research and Data Science at Opendoor &#8211; Interview Series )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/researchers-develop-ai-backed-method-of-seed-analysis/">https://www.unite.ai/researchers-develop-ai-backed-method-of-seed-analysis/</a> ( Researchers Develop AI-Backed Method of Seed Analysis )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="http://dx.doi.org/10.3389/fpls.2020.577851">http://dx.doi.org/10.3389/fpls.2020.577851</a> ( AI-driven method of seed quality analysis )
</p>
<p>
<a href="https://phys.org/news/2021-03-technique-based-artificial-intelligence-automation.html">https://phys.org/news/2021-03-technique-based-artificial-intelligence-automation.html</a> ( According to Phys.org )
</p>
<p>
<a href="https://www.unite.ai/what-is-computer-vision/">https://www.unite.ai/what-is-computer-vision/</a> ( computer vision )
</p>
<p>
<a href="https://www.unite.ai/what-is-machine-learning/">https://www.unite.ai/what-is-machine-learning/</a> ( machine learning )
</p>
<p>
<a href="https://www.unite.ai/researchers-develop-ai-backed-method-of-seed-analysis/">https://www.unite.ai/researchers-develop-ai-backed-method-of-seed-analysis/</a> ( Researchers Develop AI-Backed Method of Seed Analysis )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/datagen-secures-18-million-in-investments-to-create-synthetic-data-for-ais/">https://www.unite.ai/datagen-secures-18-million-in-investments-to-create-synthetic-data-for-ais/</a> ( DataGen Secures $18 Million in Investments to Create Synthetic Data for AIs )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.datagen.tech/">https://www.datagen.tech/</a> ( DataGen )
</p>
<p>
<a href="https://fortune.com/2021/03/16/artificial-intelligence-startups-datagen-ai-synthetic-data-training-israeli-companies/">https://fortune.com/2021/03/16/artificial-intelligence-startups-datagen-ai-synthetic-data-training-israeli-companies/</a> ( recently raised $18.5 million dollars )
</p>
<p>
<a href="https://www.unite.ai/what-is-synthetic-data/">https://www.unite.ai/what-is-synthetic-data/</a> ( synthetic data )
</p>
<p>
<a href="https://www.unite.ai/what-is-machine-learning/">https://www.unite.ai/what-is-machine-learning/</a> ( machine learning )
</p>
<p>
<a href="https://fortune.com/company/gartner">https://fortune.com/company/gartner</a> ( Gartner )
</p>
<p>
<a href="https://fortune.com/2021/03/16/artificial-intelligence-startups-datagen-ai-synthetic-data-training-israeli-companies/">https://fortune.com/2021/03/16/artificial-intelligence-startups-datagen-ai-synthetic-data-training-israeli-companies/</a> ( via Fortune: )
</p>
<p>
<a href="https://www.unite.ai/datagen-secures-18-million-in-investments-to-create-synthetic-data-for-ais/">https://www.unite.ai/datagen-secures-18-million-in-investments-to-create-synthetic-data-for-ais/</a> ( DataGen Secures $18 Million in Investments to Create Synthetic Data for AIs )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/alex-sappok-ceo-of-raysecur-interview-series/">https://www.unite.ai/alex-sappok-ceo-of-raysecur-interview-series/</a> ( Alex Sappok, CEO of RaySecur &#8211; Interview Series )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://raysecur.com/">https://raysecur.com/</a> ( Raysecur )
</p>
<p>
<a href="https://www.unite.ai/what-is-machine-learning/">https://www.unite.ai/what-is-machine-learning/</a> ( machine learning )
</p>
<p>
<a href="https://www.unite.ai/what-is-transfer-learning/">https://www.unite.ai/what-is-transfer-learning/</a> ( transfer learning )
</p>
<p>
<a href="https://raysecur.com/">https://raysecur.com/</a> ( Raysecur )
</p>
<p>
<a href="https://www.unite.ai/alex-sappok-ceo-of-raysecur-interview-series/">https://www.unite.ai/alex-sappok-ceo-of-raysecur-interview-series/</a> ( Alex Sappok, CEO of RaySecur &#8211; Interview Series )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/ibm-recognizes-40-innovative-female-leaders-in-annual-women-leaders-in-ai-program/">https://www.unite.ai/ibm-recognizes-40-innovative-female-leaders-in-annual-women-leaders-in-ai-program/</a> ( IBM Recognizes 40 Innovative Female Leaders in Annual Women Leaders in AI Program )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/what-is-natural-language-processing/">https://www.unite.ai/what-is-natural-language-processing/</a> ( natural language processing )
</p>
<p>
<a href="https://www.unite.ai/what-is-machine-learning/">https://www.unite.ai/what-is-machine-learning/</a> ( Machine Learning )
</p>
<p>
<a href="https://www.unite.ai/what-is-data-science/">https://www.unite.ai/what-is-data-science/</a> ( Data Science )
</p>
<p>
<a href="https://www.ibm.com/watson/women-leaders-in-ai">https://www.ibm.com/watson/women-leaders-in-ai</a> ( IBM’s site )
</p>
<p>
<a href="https://www.unite.ai/ibm-recognizes-40-innovative-female-leaders-in-annual-women-leaders-in-ai-program/">https://www.unite.ai/ibm-recognizes-40-innovative-female-leaders-in-annual-women-leaders-in-ai-program/</a> ( IBM Recognizes 40 Innovative Female Leaders in Annual Women Leaders in AI Program )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/researchers-developing-self-walking-robotic-exoskeletons/">https://www.unite.ai/researchers-developing-self-walking-robotic-exoskeletons/</a> ( Researchers Developing Self-Walking Robotic Exoskeletons  )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/what-is-computer-vision/">https://www.unite.ai/what-is-computer-vision/</a> ( computer vision )
</p>
<p>
<a href="https://ieeexplore.ieee.org/abstract/document/9351558">https://ieeexplore.ieee.org/abstract/document/9351558</a> ( Simulation of Stand-to-Sit Biomechanics for Robotic Exoskeletons and Prostheses with Energy Regeneration )
</p>
<p>
<a href="https://www.cnet.com/roadshow/news/honda-exoskeleton-ces-production-strength/">https://www.cnet.com/roadshow/news/honda-exoskeleton-ces-production-strength/</a> ( Honda announced in 2019 that they were developing exoskeletons to augment human strength )
</p>
<p>
<a href="https://www.newscientist.com/article/2226447-people-in-japan-are-wearing-exoskeletons-to-keep-working-as-they-age/">https://www.newscientist.com/article/2226447-people-in-japan-are-wearing-exoskeletons-to-keep-working-as-they-age/</a> ( exoskeletons were being used in Japan to help people continue working as they age )
</p>
<p>
<a href="https://www.unite.ai/researchers-developing-self-walking-robotic-exoskeletons/">https://www.unite.ai/researchers-developing-self-walking-robotic-exoskeletons/</a> ( Researchers Developing Self-Walking Robotic Exoskeletons  )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/dr-george-aronoff-chief-medical-officer-at-dosis-inc-interview-series/">https://www.unite.ai/dr-george-aronoff-chief-medical-officer-at-dosis-inc-interview-series/</a> ( Dr. George Aronoff, Chief Medical Officer at Dosis, Inc &#8211; Interview Series )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.dosisinc.com/">https://www.dosisinc.com/</a> ( Dosis )
</p>
<p>
<a href="https://www.dosisinc.com/">https://www.dosisinc.com/</a> ( Dosis )
</p>
<p>
<a href="https://www.unite.ai/dr-george-aronoff-chief-medical-officer-at-dosis-inc-interview-series/">https://www.unite.ai/dr-george-aronoff-chief-medical-officer-at-dosis-inc-interview-series/</a> ( Dr. George Aronoff, Chief Medical Officer at Dosis, Inc &#8211; Interview Series )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/tobias-rijken-co-founder-cto-at-kheiron-medical-technologies-interview-series/">https://www.unite.ai/tobias-rijken-co-founder-cto-at-kheiron-medical-technologies-interview-series/</a> ( Tobias Rijken, Co-Founder &#038; CTO at Kheiron Medical Technologies &#8211; Interview Series )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.kheironmed.com">https://www.kheironmed.com</a> ( Kheiron Medical Technologies )
</p>
<p>
<a href="https://www.unite.ai/what-is-machine-learning/">https://www.unite.ai/what-is-machine-learning/</a> ( machine learning )
</p>
<p>
<a href="https://www.unite.ai/what-is-reinforcement-learning/">https://www.unite.ai/what-is-reinforcement-learning/</a> ( reinforcement learning )
</p>
<p>
<a href="https://www.kheironmed.com/news/new-collaboration-formed-to-develop-ai-for-breast-cancer-screening-on-a-racially-diverse-population-of-patients">https://www.kheironmed.com/news/new-collaboration-formed-to-develop-ai-for-breast-cancer-screening-on-a-racially-diverse-population-of-patients</a> ( announced )
</p>
<p>
<a href="https://www.kheironmed.com">https://www.kheironmed.com</a> ( Kheiron Medical Technologies. )
</p>
<p>
<a href="https://www.unite.ai/tobias-rijken-co-founder-cto-at-kheiron-medical-technologies-interview-series/">https://www.unite.ai/tobias-rijken-co-founder-cto-at-kheiron-medical-technologies-interview-series/</a> ( Tobias Rijken, Co-Founder &#038; CTO at Kheiron Medical Technologies &#8211; Interview Series )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/quantum-technology-can-speed-up-learning-process-of-machines/">https://www.unite.ai/quantum-technology-can-speed-up-learning-process-of-machines/</a> ( Quantum Technology Can Speed-Up Learning Process of Machines )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.nature.com/articles/s41586-021-03242-7">https://www.nature.com/articles/s41586-021-03242-7</a> ( Nature )
</p>
<p>
<a href="https://www.unite.ai/quantum-technology-can-speed-up-learning-process-of-machines/">https://www.unite.ai/quantum-technology-can-speed-up-learning-process-of-machines/</a> ( Quantum Technology Can Speed-Up Learning Process of Machines )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/researchers-create-ai-powered-real-time-3d-holograms-on-smartphones/">https://www.unite.ai/researchers-create-ai-powered-real-time-3d-holograms-on-smartphones/</a> ( Researchers Create AI-Powered Real-Time 3D Holograms On Smartphones )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://spectrum.ieee.org/tech-talk/computing/software/realtime-hologram">https://spectrum.ieee.org/tech-talk/computing/software/realtime-hologram</a> ( an AI model developed by researchers at MIT )
</p>
<p>
<a href="https://www.nature.com/articles/s41586-020-03152-0">https://www.nature.com/articles/s41586-020-03152-0</a> ( developed by the MIT team )
</p>
<p>
<a href="https://spectrum.ieee.org/tech-talk/computing/software/realtime-hologram">https://spectrum.ieee.org/tech-talk/computing/software/realtime-hologram</a> ( According to IEEE Spectrum )
</p>
<p>
<a href="https://www.unite.ai/what-is-deep-learning/">https://www.unite.ai/what-is-deep-learning/</a> ( deep learning )
</p>
<p>
<a href="https://www.unite.ai/what-are-convolutional-neural-networks/">https://www.unite.ai/what-are-convolutional-neural-networks/</a> ( convolutional neural network )
</p>
<p>
<a href="https://www.unite.ai/researchers-create-ai-powered-real-time-3d-holograms-on-smartphones/">https://www.unite.ai/researchers-create-ai-powered-real-time-3d-holograms-on-smartphones/</a> ( Researchers Create AI-Powered Real-Time 3D Holograms On Smartphones )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/julianna-ianni-vice-president-ai-research-development-proscia-interview-series/">https://www.unite.ai/julianna-ianni-vice-president-ai-research-development-proscia-interview-series/</a> ( Julianna Ianni, Vice President, AI Research &#038; Development, Proscia &#8211; Interview Series )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://proscia.com">https://proscia.com</a> ( Proscia )
</p>
<p>
<a href="https://www.unite.ai/what-is-machine-learning/">https://www.unite.ai/what-is-machine-learning/</a> ( machine learning )
</p>
<p>
<a href="https://www.unite.ai/what-is-deep-learning/">https://www.unite.ai/what-is-deep-learning/</a> ( deep learning )
</p>
<p>
<a href="https://www.unite.ai/what-is-computer-vision/">https://www.unite.ai/what-is-computer-vision/</a> ( computer vision )
</p>
<p>
<a href="http://proscia.com">http://proscia.com</a> ( learn more about Proscia )
</p>
<p>
<a href="https://proscia.com/careers/">https://proscia.com/careers/</a> ( view our open roles )
</p>
<p>
<a href="https://www.unite.ai/julianna-ianni-vice-president-ai-research-development-proscia-interview-series/">https://www.unite.ai/julianna-ianni-vice-president-ai-research-development-proscia-interview-series/</a> ( Julianna Ianni, Vice President, AI Research &#038; Development, Proscia &#8211; Interview Series )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/mit-research-team-designs-ai-network-to-resist-adversarial-examples/">https://www.unite.ai/mit-research-team-designs-ai-network-to-resist-adversarial-examples/</a> ( MIT Research Team Designs AI Network To Resist Adversarial Examples )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://news.mit.edu/2021/artificial-intelligence-adversarial-0308">https://news.mit.edu/2021/artificial-intelligence-adversarial-0308</a> ( as developed a deep-learning algorithm )
</p>
<p>
<a href="https://www.unite.ai/what-is-computer-vision/">https://www.unite.ai/what-is-computer-vision/</a> ( computer vision )
</p>
<p>
<a href="https://www.unite.ai/what-is-deep-reinforcement-learning/">https://www.unite.ai/what-is-deep-reinforcement-learning/</a> ( Deep Reinforcement Learning )
</p>
<p>
<a href="https://ieeexplore.ieee.org/document/9354500">https://ieeexplore.ieee.org/document/9354500</a> ( CARRL. )
</p>
<p>
<a href="https://www.unite.ai/what-is-reinforcement-learning/">https://www.unite.ai/what-is-reinforcement-learning/</a> ( reinforcement learning )
</p>
<p>
<a href="https://news.mit.edu/2021/artificial-intelligence-adversarial-0308">https://news.mit.edu/2021/artificial-intelligence-adversarial-0308</a> ( via MIT News: )
</p>
<p>
<a href="https://www.unite.ai/mit-research-team-designs-ai-network-to-resist-adversarial-examples/">https://www.unite.ai/mit-research-team-designs-ai-network-to-resist-adversarial-examples/</a> ( MIT Research Team Designs AI Network To Resist Adversarial Examples )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/researchers-use-brain-machine-interface-to-generate-attractive-faces-based-on-personal-preferences/">https://www.unite.ai/researchers-use-brain-machine-interface-to-generate-attractive-faces-based-on-personal-preferences/</a> ( Researchers Use Brain-Machine Interface To Generate Attractive Faces Based On Personal Preferences )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://eurekalert.org/pub_releases/2021-03/uoh-bii030221.php">https://eurekalert.org/pub_releases/2021-03/uoh-bii030221.php</a> ( has created an AI )
</p>
<p>
<a href="https://www2.helsinki.fi/en/news/data-science-news/beauty-is-in-the-brain-of-the-beholder-an-ai-generates-personally-attractive-images-by-reading-brain-data">https://www2.helsinki.fi/en/news/data-science-news/beauty-is-in-the-brain-of-the-beholder-an-ai-generates-personally-attractive-images-by-reading-brain-data</a> ( intended to generate images of attractive face )
</p>
<p>
<a href="https://www.unite.ai/what-is-machine-learning/">https://www.unite.ai/what-is-machine-learning/</a> ( machine learning )
</p>
<p>
<a href="https://eurekalert.org/pub_releases/2021-03/uoh-bii030221.php">https://eurekalert.org/pub_releases/2021-03/uoh-bii030221.php</a> ( via EurekaAlert: )
</p>
<p>
<a href="https://www.unite.ai/what-are-neural-networks/">https://www.unite.ai/what-are-neural-networks/</a> ( artificial neural network )
</p>
<p>
<a href="https://www.unite.ai/what-is-computer-vision/">https://www.unite.ai/what-is-computer-vision/</a> ( Computer vision )
</p>
<p>
<a href="https://www.unite.ai/researchers-use-brain-machine-interface-to-generate-attractive-faces-based-on-personal-preferences/">https://www.unite.ai/researchers-use-brain-machine-interface-to-generate-attractive-faces-based-on-personal-preferences/</a> ( Researchers Use Brain-Machine Interface To Generate Attractive Faces Based On Personal Preferences )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/intel-powered-netra-ai-uses-deep-learning-to-reduce-diabetic-vision-loss/">https://www.unite.ai/intel-powered-netra-ai-uses-deep-learning-to-reduce-diabetic-vision-loss/</a> ( Intel-Powered Netra.AI Uses Deep Learning to Reduce Diabetic Vision Loss )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
<p>
<a href="https://www.unite.ai/what-is-deep-learning/">https://www.unite.ai/what-is-deep-learning/</a> ( deep learning )
</p>
<p>
<a href="https://www.unite.ai/intel-powered-netra-ai-uses-deep-learning-to-reduce-diabetic-vision-loss/">https://www.unite.ai/intel-powered-netra-ai-uses-deep-learning-to-reduce-diabetic-vision-loss/</a> ( Intel-Powered Netra.AI Uses Deep Learning to Reduce Diabetic Vision Loss )
</p>
<p>
<a href="https://www.unite.ai">https://www.unite.ai</a> ( Unite.AI )
</p>
</body>
</html>

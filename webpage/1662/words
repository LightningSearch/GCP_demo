<!--mvp-fly-logo--
>
<!--mvp-fly-top-in--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-fly-top-out--
>
<!--mvp-fly-menu-top--
>
News
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
Data
Science
COVID-19
Cybersecurity
Deep
Learning
Deepfakes
Education
Environment
Ethics
Facial
Recognition
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Decision
Tree
Data
Science
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Edge
AI
&#038;
Edge
Computing
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
Generative
vs
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Linear
Regression
Long
Short-Term
Memory
(LSTM)
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Courses
Conferences
AI
Conferences
Cybersecurity
Conferences
Robotics
Conferences
Interviews
Thought
Leaders
Newsletters
Organizations
Meet
the
Team
Our
Charter
Contact
Us
<!--mvp-fly-menu-wrap--
>
Connect
with
us
<!--mvp-fly-soc-wrap--
>
<!--mvp-fly-wrap--
>
<!--mvp-search-box--
>
<!--mvp-search-but-wrap--
>
<!--mvp-search-wrap--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-nav-small-left--
>
Unite.AI
<!--mvp-nav-small-logo--
>
How
Does
Image
Classification
Work?
<!--mvp-drop-nav-title--
>
News
A
-
C
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
COVID-19
Cybersecurity
D
-
F
Data
Science
Deepfakes
Deep
Learning
Education
Ethics
Environment
Facial
Recognition
G
-
Q
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
R
-
Z
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
A
-
D
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
D
-
K
Dimensionality
Reduction
Edge
AI
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
(GAN)
Generative
vs.
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
L
-
Q
Linear
Regression
Long
Short-Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
R
-
Z
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Learning
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Conferences
Artificial
Intelligence
Cybersecurity
Robotics
Interviews
Thought
Leaders
Meet
the
Team
Contact
<!--mvp-nav-menu--
>
<!--mvp-nav-small-mid-right--
>
<!--mvp-nav-small-mid--
>
<!--mvp-nav-small-left-in--
>
<!--mvp-nav-small-left-out--
>
<!--mvp-nav-small-cont--
>
<!--mvp-nav-small-right-in--
>
<!--mvp-nav-small-right--
>
<!--mvp-nav-small-right-out--
>
<!--mvp-nav-small-wrap--
>
<!--mvp-main-box--
>
<!--mvp-main-nav-small-cont--
>
<!--mvp-main-nav-small--
>
<!--mvp-main-nav-wrap--
>
<!--mvp-main-head-wrap--
>
AI
Masterclass:
Terminology
(A
to
D)
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Terminology
(E
to
K)
Edge
AI
Ensemble
Learning
Federated
Learning
Generative
Adversarial
Network
Generative
vs.
Discriminative
Gradient
Boosting
Gradient
Descent
Few-Shot
Learning
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Terminology
(L
to
Q)
Linear
Regression
Long-Short
Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Terminology
(R
to
Z)
Reinforcement
Learning
Robotic
Process
Automation
Structured
vs
Unstructured
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
AI
101
How
Does
Image
Classification
Work?
<!--mvp-author-info-thumb--
>
Updated
7
months
ago
&nbsp;on
September
5,
2020
<!--mvp-author-info-date--
>
By
Daniel
Nelson
<!--mvp-author-info-name--
>
<!--mvp-author-info-text--
>
<!--mvp-author-info-wrap--
>
Table
Of
Contents
<!--mvp-post-img-hide--
>
How
can
your
phone
determine
what
an
object
is
just
by
taking
a
photo
of
it?
How
do
social
media
websites
automatically
tag
people
in
photos?
This
is
accomplished
through
AI-powered
image
recognition
and
classification.
The
recognition
and
classification
of
images
is
what
enables
many
of
the
most
impressive
accomplishments
of
artificial
intelligence.
Yet
how
do
computers
learn
to
detect
and
classify
images?
In
this
article,
we’ll
cover
the
general
methods
that
computers
use
to
interpret
and
detect
images
and
then
take
a
look
at
some
of
the
most
popular
methods
of
classifying
those
images.
Pixel-Level
vs.
Object-Based
Classification
Image
classification
techniques
can
mainly
be
divided
into
two
different
categories:
pixel-based
classification
and
object-based
classification.
Pixels
are
the
base
units
of
an
image,
and
the
analysis
of
pixels
is
the
primary
way
that
image
classification
is
done.
However,
classification
algorithms
can
either
use
just
the
spectral
information
within
individual
pixels
to
classify
an
image
or
examine
spatial
information
(nearby
pixels)
along
with
the
spectral
information.
Pixel-based
classification
methods
utilize
only
spectral
information
(the
intensity
of
a
pixel),
while
object-based
classification
methods
take
into
account
both
pixel
spectral
information
and
spatial
information.
There
are
different
classification
techniques
used
for
pixel-based
classification.
These
include
minimum-distance-to-mean,
maximum-likelihood,
and
minimum-Mahalanobis-distance.
These
methods
require
that
the
means
and
variances
of
the
classes
are
known,
and
they
all
operate
by
examining
the
“distance”
between
class
means
and
the
target
pixels.
Pixel-based
classification
methods
are
limited
by
the
fact
that
they
can’t
use
information
from
other
nearby
pixels.
In
contrast,
object-based
classification
methods
can
include
other
pixels
and
therefore
they
also
use
spatial
information
to
classify
items.
Note
that
“object”
just
refers
to
contiguous
regions
of
pixels
and
not
whether
or
not
there
is
a
target
object
within
that
region
of
pixels.
Preprocessing
Image
Data
For
Object
Detection
The
most
recent
and
reliable
image
classification
systems
primarily
use
object-level
classification
schemes,
and
for
these
approaches
image
data
must
be
prepared
in
specific
ways.
The
objects/regions
need
to
be
selected
and
preprocessed.
Before
an
image,
and
the
objects/regions
within
that
image,
can
be
classified
the
data
that
comprises
that
image
has
to
be
interpreted
by
the
computer.
Images
need
to
be
preprocessed
and
readied
for
input
into
the
classification
algorithm,
and
this
is
done
through
object
detection.
This
is
a
critical
part
of
readying
the
data
and
preparing
the
images
to
train
the
machine
learning
classifier.
Object
detection
is
done
with
a
variety
of
methods
and
techniques.
To
begin
with,
whether
or
not
there
are
multiple
objects
of
interest
or
a
single
object
of
interest
impacts
how
the
image
preprocessing
is
handled.
If
there
is
just
one
object
of
interest,
the
image
undergoes
image
localization.
The
pixels
that
comprise
the
image
have
numerical
values
that
are
interpreted
by
the
computer
and
used
to
display
the
proper
colors
and
hues.
An
object
known
as
a
bounding
box
is
drawn
around
the
object
of
interest,
which
helps
the
computer
know
what
part
of
the
image
is
important
and
what
pixel
values
define
the
object.
If
there
are
multiple
objects
of
interest
in
the
image,
a
technique
called
object
detection
is
used
to
apply
these
bounding
boxes
to
all
the
objects
within
the
image.
Photo:
Adrian
Rosebrock
via
Wikimedia
Commons,
CC
BY
SA
4.0
(https://commons.wikimedia.org/wiki/File:Intersection_over_Union_-_object_detection_bounding_boxes.jpg)
Another
method
of
preprocessing
is
image
segmentation.
Image
segmentation
functions
by
dividing
the
whole
image
into
segments
based
on
similar
features.
Different
regions
of
the
image
will
have
similar
pixel
values
in
comparison
to
other
regions
of
the
image,
so
these
pixels
are
grouped
together
into
image
masks
that
correspond
to
the
shape
and
boundaries
of
the
relevant
objects
within
the
image.
Image
segmentation
helps
the
computer
isolate
the
features
of
the
image
that
will
help
it
classify
an
object,
much
like
bounding
boxes
do,
but
they
provide
much
more
accurate,
pixel-level
labels.
After
the
object
detection
or
image
segmentation
has
been
completed,
labels
are
applied
to
the
regions
in
question.
These
labels
are
fed,
along
with
the
values
of
the
pixels
comprising
the
object,
into
the
machine
learning
algorithms
that
will
learn
patterns
associated
with
the
different
labels.
Machine
Learning
Algorithms
Once
the
data
has
been
prepared
and
labeled,
the
data
is
fed
into
a
machine
learning
algorithm,
which
trains
on
the
data.
We’ll
cover
some
of
the
most
common
kinds
of
machine
learning
image
classification
algorithms
below.
K-Nearest
Neighbors
K-Nearest
Neighbors
is
a
classification
algorithm
that
examines
the
closest
training
examples
and
looks
at
their
labels
to
ascertain
the
most
probable
label
for
a
given
test
example.
When
it
comes
to
image
classification
using
KNN,
the
feature
vectors
and
labels
of
the
training
images
are
stored
and
just
the
feature
vector
is
passed
into
the
algorithm
during
testing.
The
training
and
testing
feature
vectors
are
then
compared
against
each
other
for
similarity.
KNN-based
classification
algorithms
are
extremely
simple
and
they
deal
with
multiple
classes
quite
easily.
However,
KNN
calculates
similarity
based
on
all
features
equally.
This
means
that
it
can
be
prone
to
misclassification
when
provided
with
images
where
only
a
subset
of
the
features
is
important
for
the
classification
of
the
image.
Support
Vector
Machines
Support
Vector
Machines
are
a
classification
method
that
places
points
in
space
and
then
draws
dividing
lines
between
the
points,
placing
objects
in
different
classes
depending
on
which
side
of
the
dividing
plane
the
points
fall
on.
Support
Vector
Machines
are
capable
of
doing
nonlinear
classification
through
the
use
of
a
technique
known
as
the
kernel
trick.
While
SVM
classifiers
are
often
very
accurate,
a
substantial
drawback
to
SVM
classifiers
is
that
they
tend
to
be
limited
by
both
size
and
speed,
with
speed
suffering
as
size
increases.
Multi-Layer
Perceptrons
(Neural
Nets)
Multi-layer
perceptrons,
also
called
neural
network
models,
are
machine
learning
algorithms
inspired
by
the
human
brain.
Multilayer
perceptrons
are
composed
of
various
layers
that
are
joined
together
with
each
other,
much
like
neurons
in
the
human
brain
are
linked
together.
Neural
networks
make
assumptions
about
how
the
input
features
are
related
to
the
data’s
classes
and
these
assumptions
are
adjusted
over
the
course
of
training.
Simple
neural
network
models
like
the
multi-layer
perceptron
are
capable
of
learning
non-linear
relationships,
and
as
a
result,
they
can
be
much
more
accurate
than
other
models.
However,
MLP
models
suffer
from
some
notable
issues
like
the
presence
of
non-convex
loss
functions.
Deep
Learning
Algorithms
(CNNs)
Photo:
APhex34
via
Wikimedia
Commons,
CC
BY
SA
4.0
(https://commons.wikimedia.org/wiki/File:Typical_cnn.png)
The
most
commonly
used
image
classification
algorithm
in
recent
times
is
the
Convolutional
Neural
Network
(CNNs).
CNNs
are
customized
versions
of
neural
networks
that
combine
the
multilayer
neural
networks
with
specialized
layers
that
are
capable
of
extracting
the
features
most
important
and
relevant
to
the
classification
of
an
object.
CNNs
can
automatically
discover,
generate,
and
learn
features
of
images.
This
greatly
reduces
the
need
to
manually
label
and
segment
images
to
prepare
them
for
machine
learning
algorithms.
They
also
have
an
advantage
over
MLP
networks
because
they
can
deal
with
non-convex
loss
functions.
Convolutional
Neural
Networks
get
their
name
from
the
fact
that
they
create
“convolutions”.
CNNs
operate
by
taking
a
filter
and
sliding
it
over
an
image.
You
can
think
of
this
as
viewing
sections
of
a
landscape
through
a
moveable
window,
concentrating
on
just
the
features
that
are
viewable
through
the
window
at
any
one
time.
The
filter
contains
numerical
values
which
are
multiplied
with
the
values
of
the
pixels
themselves.
The
result
is
a
new
frame,
or
matrix,
full
of
numbers
that
represent
the
original
image.
This
process
is
repeated
for
a
chosen
number
of
filters,
and
then
the
frames
are
joined
together
into
a
new
image
that
is
slightly
smaller
and
less
complex
than
the
original
image.
A
technique
called
pooling
is
used
to
select
just
the
most
important
values
within
the
image,
and
the
goal
is
for
the
convolutional
layers
to
eventually
extract
just
the
most
salient
parts
of
the
image
that
will
help
the
neural
network
recognize
the
objects
in
the
image.
Convolutional
Neural
Networks
are
comprised
of
two
different
parts.
The
convolutional
layers
are
what
extract
the
features
of
the
image
and
convert
them
into
a
format
that
the
neural
network
layers
can
interpret
and
learn
from.
The
early
convolutional
layers
are
responsible
for
extracting
the
most
basic
elements
of
the
image,
like
simple
lines
and
boundaries.
The
middle
convolutional
layers
begin
to
capture
more
complex
shapes,
like
simple
curves
and
corners.
The
later,
deeper
convolutional
layers
extract
the
high-level
features
of
the
image,
which
are
what
is
passed
into
the
neural
network
portion
of
the
CNN,
and
are
what
the
classifier
learns.
<!--mvp-content-main--
>
Related
Topics:
Computer
Vision
image
classification
<!--mvp-post-tags--
>
<!--posts-nav-link--
>
Up
Next
What
Is
Synthetic
Data?
<!--mvp-prev-next-text--
>
<!--mvp-next-cont-in--
>
<!--mvp-prev-next-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-next-post-wrap--
>
Don&#039;t
Miss
Do
you
recommend
Recommendation
Engines?
<!--mvp-prev-next-text--
>
<!--mvp-prev-cont-in--
>
<!--mvp-prev-cont-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-prev-post-wrap--
>
<!--mvp-prev-next-wrap--
>
<!--mvp-author-box-img--
>
Daniel
Nelson
<!--mvp-author-box-soc-wrap--
>
<!--mvp-author-box-head--
>
<!--mvp-author-box-in--
>
<!--mvp-author-box-out--
>
Blogger
and
programmer
with
specialties
in
Machine
Learning
and
Deep
Learning
topics.
Daniel
hopes
to
help
others
use
the
power
of
AI
for
social
good.
<!--mvp-author-box-text--
>
<!--author__item--
>
<!--mvp-author-box-wrap--
>
<!--mvp-org-logo--
>
<!--mvp-org-wrap--
>
<!--mvp-content-bot--
>
<!--mvp-content-body-top--
>
You
may
like
<!--mvp-related-img--
>
AI
Pose
Estimation
in
Fitness
Application
<!--mvp-related-text--
>
<!--mvp-related-img--
>
Researchers
Develop
New
Tool
to
Fight
Bias
in
Computer
Vision
<!--mvp-related-text--
>
<!--mvp-related-img--
>
AI
Vision
Model
Could
Prevent
Fatal
Encounters
Between
Humans
And
Elephants
<!--mvp-related-text--
>
<!--mvp-related-img--
>
AI
and
IoT:
Transportation
Management
in
Smart
Cities
<!--mvp-related-text--
>
<!--mvp-related-img--
>
Researchers
Use
AI
To
Investigate
How
Reflections
Differ
From
Original
Images
<!--mvp-related-text--
>
<!--mvp-related-img--
>
AI
Models
to
Help
Identify
Invasive
Species
of
Plants
Across
the
UK
<!--mvp-related-text--
>
<!--mvp-related-posts--
>
<!--mvp-cont-read-wrap--
>
<!--mvp-content-body--
>
<!--mvp-post-soc-in--
>
<!--mvp-post-soc-out--
>
<!--mvp-content-wrap--
>
<!--mvp-post-content--
>
<!--mvp-post-main-in--
>
<!--mvp-post-main-out--
>
<!--mvp-post-main--
>
<!--mvp-main-box--
>
<!--mvp-article-cont--
>
<!--mvp-article-wrap--
>
<!--mvp-main-body-wrap--
>
Meet
the
Team
Our
Charter
Press
Tools
Contact
Us
Advertiser
Disclosure
:
Unite.AI
is
committed
to
rigorous
editorial
standards
to
provide
our
readers
with
accurate
information
and
news.
We
may
receive
compensation
when
you
click
on
links
to
products
we
reviewed.
Copyright
©
2021
Unite.AI
Editorial
Policy
Privacy
Policy
Terms
and
Conditions
<!--mvp-site-main--
>
<!--mvp-site-wall--
>
<!--mvp-site--
>
<!--mvp-fly-top--
>
<!--mvp-fly-fade--
>

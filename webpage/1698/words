<!--mvp-fly-logo--
>
<!--mvp-fly-top-in--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-fly-top-out--
>
<!--mvp-fly-menu-top--
>
News
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
Data
Science
COVID-19
Cybersecurity
Deep
Learning
Deepfakes
Education
Environment
Ethics
Facial
Recognition
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Decision
Tree
Data
Science
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Edge
AI
&#038;
Edge
Computing
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
Generative
vs
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Linear
Regression
Long
Short-Term
Memory
(LSTM)
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Courses
Conferences
AI
Conferences
Cybersecurity
Conferences
Robotics
Conferences
Interviews
Thought
Leaders
Newsletters
Organizations
Meet
the
Team
Our
Charter
Contact
Us
<!--mvp-fly-menu-wrap--
>
Connect
with
us
<!--mvp-fly-soc-wrap--
>
<!--mvp-fly-wrap--
>
<!--mvp-search-box--
>
<!--mvp-search-but-wrap--
>
<!--mvp-search-wrap--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-nav-small-left--
>
Unite.AI
<!--mvp-nav-small-logo--
>
What
is
Meta-Learning?
<!--mvp-drop-nav-title--
>
News
A
-
C
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
COVID-19
Cybersecurity
D
-
F
Data
Science
Deepfakes
Deep
Learning
Education
Ethics
Environment
Facial
Recognition
G
-
Q
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
R
-
Z
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
A
-
D
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
D
-
K
Dimensionality
Reduction
Edge
AI
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
(GAN)
Generative
vs.
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
L
-
Q
Linear
Regression
Long
Short-Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
R
-
Z
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Learning
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Conferences
Artificial
Intelligence
Cybersecurity
Robotics
Interviews
Thought
Leaders
Meet
the
Team
Contact
<!--mvp-nav-menu--
>
<!--mvp-nav-small-mid-right--
>
<!--mvp-nav-small-mid--
>
<!--mvp-nav-small-left-in--
>
<!--mvp-nav-small-left-out--
>
<!--mvp-nav-small-cont--
>
<!--mvp-nav-small-right-in--
>
<!--mvp-nav-small-right--
>
<!--mvp-nav-small-right-out--
>
<!--mvp-nav-small-wrap--
>
<!--mvp-main-box--
>
<!--mvp-main-nav-small-cont--
>
<!--mvp-main-nav-small--
>
<!--mvp-main-nav-wrap--
>
<!--mvp-main-head-wrap--
>
AI
Masterclass:
Terminology
(A
to
D)
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Terminology
(E
to
K)
Edge
AI
Ensemble
Learning
Federated
Learning
Generative
Adversarial
Network
Generative
vs.
Discriminative
Gradient
Boosting
Gradient
Descent
Few-Shot
Learning
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Terminology
(L
to
Q)
Linear
Regression
Long-Short
Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Terminology
(R
to
Z)
Reinforcement
Learning
Robotic
Process
Automation
Structured
vs
Unstructured
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
AI
101
What
is
Meta-Learning?
<!--mvp-author-info-thumb--
>
Updated
7
months
ago
&nbsp;on
August
23,
2020
<!--mvp-author-info-date--
>
By
Daniel
Nelson
<!--mvp-author-info-name--
>
<!--mvp-author-info-text--
>
<!--mvp-author-info-wrap--
>
Table
Of
Contents
<!--mvp-post-img-hide--
>
What
is
Meta-Learning?
One
of
the
fastest-growing
areas
of
research
in
machine
learning
is
the
area
of
meta-learning
.
Meta-learning,
in
the
machine
learning
context,
is
the
use
of
machine
learning
algorithms
to
assist
in
the
training
and
optimization
of
other
machine
learning
models.
As
meta-learning
is
becoming
more
and
more
popular
and
more
meta-learning
techniques
are
being
developed,
it’s
beneficial
to
have
an
understanding
of
what
meta-learning
is
and
to
have
a
sense
of
the
various
ways
it
can
be
applied.
Let’s
examine
the
ideas
behind
meta-learning,
types
of
meta-learning
,
as
well
as
some
of
the
ways
meta-learning
can
be
used.
The
term
meta-learning
was
coined
by
Donald
Maudsley
to
describe
a
process
by
which
people
begin
to
shape
what
they
learn,
becoming
“increasingly
in
control
of
habits
of
perception,
inquiry,
learning,
and
growth
that
they
have
internalized&#8221;.
Later,
cognitive
scientists
and
psychologists
would
describe
meta-learning
as
“learning
how
to
learn”.
For
the
machine
learning
version
of
meta-learning,
the
general
idea
of
“learning
how
to
learn”
is
applied
to
AI
systems.
In
the
AI
sense,
meta-learning
is
the
ability
of
an
artificially
intelligent
machine
to
learn
how
to
carry
out
various
complex
tasks,
taking
the
principles
it
used
to
learn
one
task
and
applying
it
to
other
tasks.
AI
systems
typically
have
to
be
trained
to
accomplish
a
task
through
the
mastering
of
many
small
subtasks.
This
training
can
take
a
long
time
and
AI
agents
don’t
easily
transfer
the
knowledge
learned
during
one
task
to
another
task.
Creating
meta-learning
models
and
techniques
can
help
AI
learn
to
generalize
learning
methods
and
acquire
new
skills
quicker.
Types
of
Meta-Learning
Optimizer
Meta-Learning
Meta-learning
is
often
employed
to
optimize
the
performance
of
an
already
existing
neural
network.
Optimizer
meta-learning
methods
typically
function
by
tweaking
the
hyperparameters
of
a
different
neural
network
in
order
to
improve
the
performance
of
the
base
neural
network.
The
result
is
that
the
target
network
should
become
better
at
performing
the
task
it
is
being
trained
on.
One
example
of
a
meta-learning
optimizer
is
the
use
of
a
network
to
improve
gradient
descent
results.
Few-Shots
Meta-Learning
A
few-shots
meta-learning
approach
is
one
where
a
deep
neural
network
is
engineered
which
is
capable
of
generalizing
from
the
training
datasets
to
unseen
datasets.
An
instance
of
few-shot
classification
is
similar
to
a
normal
classification
task,
but
instead,
the
data
samples
are
entire
datasets.
The
model
is
trained
on
many
different
learning
tasks/datasets
and
then
it’s
optimized
for
peak
performance
on
the
multitude
of
training
tasks
and
unseen
data.
In
this
approach,
a
single
training
sample
is
split
up
into
multiple
classes.
This
means
that
each
training
sample/dataset
could
potentially
be
made
up
of
two
classes,
for
a
total
of
4-shots.
In
this
case,
the
total
training
task
could
be
described
as
a
4-shot
2-class
classification
task.
In
few-shot
learning,
the
idea
is
that
the
individual
training
samples
are
minimalistic
and
that
the
network
can
learn
to
identify
objects
after
having
seen
just
a
few
pictures.
This
is
much
like
how
a
child
learns
to
distinguish
objects
after
seeing
just
a
couple
of
pictures.
This
approach
has
been
used
to
create
techniques
like
one-shot
generative
models
and
memory
augmented
neural
networks.
Metric
Meta-Learning
Metric
based
meta-learning
is
the
utilization
of
neural
networks
to
determine
if
a
metric
is
being
used
effectively
and
if
the
network
or
networks
are
hitting
the
target
metric.
Metric
meta-learning
is
similar
to
few-shot
learning
in
that
just
a
few
examples
are
used
to
train
the
network
and
have
it
learn
the
metric
space.
The
same
metric
is
used
across
the
diverse
domain
and
if
the
networks
diverge
from
the
metric
they
are
considered
to
be
failing.
Recurrent
Model
Meta-Learning
Recurrent
model
meta-learning
is
the
application
of
meta-learning
techniques
to
Recurrent
Neural
Networks
and
the
similar
Long
Short-Term
Memory
networks.
This
technique
operates
by
training
the
RNN
/
LSTM
model
to
sequentially
learn
a
dataset
and
then
using
this
trained
model
as
a
basis
for
another
learner.
The
meta-learner
takes
on
board
the
specific
optimization
algorithm
that
was
used
to
train
the
initial
model.
The
inherited
parameterization
of
the
meta-learner
enables
it
to
quickly
initialize
and
converge,
but
still
be
able
to
update
for
new
scenarios.
How
Does
Meta-Learning
Work?
The
exact
way
that
meta-learning
is
conducted
varies
depending
on
the
model
and
the
nature
of
the
task
at
hand.
However,
in
general,
a
meta-learning
task
involves
copying
over
the
parameters
of
the
first
network
into
the
parameters
of
the
second
network/the
optimizer.
There
are
two
training
processes
in
meta-learning.
The
meta-learning
model
is
typically
trained
after
several
steps
of
training
on
the
base
model
have
been
carried
out.
After
the
forward,
backward,
and
optimization
steps
that
train
the
base
model,
the
forward
training
pass
is
carried
out
for
the
optimization
model.
For
example,
after
three
or
four
steps
of
training
on
the
base
model,
a
meta-loss
is
computed.
After
the
meta-loss
is
computed,
the
gradients
are
computed
for
each
meta-parameter.
After
this
occurs,
the
meta-parameters
in
the
optimizer
are
updated.
One
possibility
for
calculating
the
meta-loss
is
to
finish
the
forward
training
pass
of
the
initial
model
and
then
combine
the
losses
that
have
already
been
computed.
The
meta-optimizer
could
even
be
another
meta-learner,
though
at
a
certain
point
a
discrete
optimizer
like
ADAM
or
SGD
must
be
used.
Many
deep
learning
models
can
have
hundreds
of
thousands
or
even
millions
of
parameters.
Creating
a
meta-learner
that
has
an
entirely
new
set
of
parameters
would
be
computationally
expensive,
and
for
this
reason,
a
tactic
called
coordinate-sharing
is
typically
used.
Coordinate-sharing
involves
engineering
the
meta-learner/optimizer
so
that
it
learns
a
single
parameter
from
the
base
model
and
then
just
clones
that
parameter
in
place
of
all
of
the
other
parameters.
The
result
is
that
the
parameters
the
optimizer
possesses
don’t
depend
on
the
parameters
of
the
model.
<!--mvp-content-main--
>
Related
Topics:
101
meta-learning
<!--mvp-post-tags--
>
<!--posts-nav-link--
>
Up
Next
What
is
Backpropagation?
<!--mvp-prev-next-text--
>
<!--mvp-next-cont-in--
>
<!--mvp-prev-next-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-next-post-wrap--
>
Don&#039;t
Miss
What
are
CNNs
(Convolutional
Neural
Networks)?
<!--mvp-prev-next-text--
>
<!--mvp-prev-cont-in--
>
<!--mvp-prev-cont-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-prev-post-wrap--
>
<!--mvp-prev-next-wrap--
>
<!--mvp-author-box-img--
>
Daniel
Nelson
<!--mvp-author-box-soc-wrap--
>
<!--mvp-author-box-head--
>
<!--mvp-author-box-in--
>
<!--mvp-author-box-out--
>
Blogger
and
programmer
with
specialties
in
Machine
Learning
and
Deep
Learning
topics.
Daniel
hopes
to
help
others
use
the
power
of
AI
for
social
good.
<!--mvp-author-box-text--
>
<!--author__item--
>
<!--mvp-author-box-wrap--
>
<!--mvp-org-logo--
>
<!--mvp-org-wrap--
>
<!--mvp-content-bot--
>
<!--mvp-content-body-top--
>
You
may
like
<!--mvp-related-img--
>
What
is
Few-Shot
Learning?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
Are
Transformer
Neural
Networks?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
Edge
AI
&#038;
Edge
Computing?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
Ensemble
Learning?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
Dimensionality
Reduction?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
a
Generative
Adversarial
Network
(GAN)?
<!--mvp-related-text--
>
<!--mvp-related-posts--
>
<!--mvp-cont-read-wrap--
>
<!--mvp-content-body--
>
<!--mvp-post-soc-in--
>
<!--mvp-post-soc-out--
>
<!--mvp-content-wrap--
>
<!--mvp-post-content--
>
<!--mvp-post-main-in--
>
<!--mvp-post-main-out--
>
<!--mvp-post-main--
>
<!--mvp-main-box--
>
<!--mvp-article-cont--
>
<!--mvp-article-wrap--
>
<!--mvp-main-body-wrap--
>
Meet
the
Team
Our
Charter
Press
Tools
Contact
Us
Advertiser
Disclosure
:
Unite.AI
is
committed
to
rigorous
editorial
standards
to
provide
our
readers
with
accurate
information
and
news.
We
may
receive
compensation
when
you
click
on
links
to
products
we
reviewed.
Copyright
©
2021
Unite.AI
Editorial
Policy
Privacy
Policy
Terms
and
Conditions
<!--mvp-site-main--
>
<!--mvp-site-wall--
>
<!--mvp-site--
>
<!--mvp-fly-top--
>
<!--mvp-fly-fade--
>

<!--mvp-fly-logo--
>
<!--mvp-fly-top-in--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-fly-top-out--
>
<!--mvp-fly-menu-top--
>
News
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
Data
Science
COVID-19
Cybersecurity
Deep
Learning
Deepfakes
Education
Environment
Ethics
Facial
Recognition
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Decision
Tree
Data
Science
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Edge
AI
&#038;
Edge
Computing
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
Generative
vs
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Linear
Regression
Long
Short-Term
Memory
(LSTM)
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Courses
Conferences
AI
Conferences
Cybersecurity
Conferences
Robotics
Conferences
Interviews
Thought
Leaders
Newsletters
Organizations
Meet
the
Team
Our
Charter
Contact
Us
<!--mvp-fly-menu-wrap--
>
Connect
with
us
<!--mvp-fly-soc-wrap--
>
<!--mvp-fly-wrap--
>
<!--mvp-search-box--
>
<!--mvp-search-but-wrap--
>
<!--mvp-search-wrap--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-nav-small-left--
>
Unite.AI
<!--mvp-nav-small-logo--
>
What
is
Backpropagation?
<!--mvp-drop-nav-title--
>
News
A
-
C
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
COVID-19
Cybersecurity
D
-
F
Data
Science
Deepfakes
Deep
Learning
Education
Ethics
Environment
Facial
Recognition
G
-
Q
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
R
-
Z
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
A
-
D
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
D
-
K
Dimensionality
Reduction
Edge
AI
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
(GAN)
Generative
vs.
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
L
-
Q
Linear
Regression
Long
Short-Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
R
-
Z
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Learning
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Conferences
Artificial
Intelligence
Cybersecurity
Robotics
Interviews
Thought
Leaders
Meet
the
Team
Contact
<!--mvp-nav-menu--
>
<!--mvp-nav-small-mid-right--
>
<!--mvp-nav-small-mid--
>
<!--mvp-nav-small-left-in--
>
<!--mvp-nav-small-left-out--
>
<!--mvp-nav-small-cont--
>
<!--mvp-nav-small-right-in--
>
<!--mvp-nav-small-right--
>
<!--mvp-nav-small-right-out--
>
<!--mvp-nav-small-wrap--
>
<!--mvp-main-box--
>
<!--mvp-main-nav-small-cont--
>
<!--mvp-main-nav-small--
>
<!--mvp-main-nav-wrap--
>
<!--mvp-main-head-wrap--
>
AI
Masterclass:
Terminology
(A
to
D)
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Terminology
(E
to
K)
Edge
AI
Ensemble
Learning
Federated
Learning
Generative
Adversarial
Network
Generative
vs.
Discriminative
Gradient
Boosting
Gradient
Descent
Few-Shot
Learning
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Terminology
(L
to
Q)
Linear
Regression
Long-Short
Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Terminology
(R
to
Z)
Reinforcement
Learning
Robotic
Process
Automation
Structured
vs
Unstructured
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
AI
101
What
is
Backpropagation?
<!--mvp-author-info-thumb--
>
Updated
7
months
ago
&nbsp;on
August
23,
2020
<!--mvp-author-info-date--
>
By
Daniel
Nelson
<!--mvp-author-info-name--
>
<!--mvp-author-info-text--
>
<!--mvp-author-info-wrap--
>
Table
Of
Contents
<!--mvp-post-img-hide--
>
What
is
Backpropagation?
Deep
learning
systems
are
able
to
learn
extremely
complex
patterns,
and
they
accomplish
this
by
adjusting
their
weights.
How
are
the
weights
of
a
deep
neural
network
adjusted
exactly?
They
are
adjusted
through
a
process
called
backpropagation
.
Without
backpropagation,
deep
neural
networks
wouldn’t
be
able
to
carry
out
tasks
like
recognizing
images
and
interpreting
natural
language.
Understanding
how
backpropagation
works
is
critical
to
understanding
deep
neural
networks
in
general,
so
let’s
delve
into
backpropagation
and
see
how
the
process
is
used
to
adjust
a
network’s
weights.
Backpropagation
can
be
difficult
to
understand,
and
the
calculations
used
to
carry
out
backpropagation
can
be
quite
complex.
This
article
will
endeavor
to
give
you
an
intuitive
understanding
of
backpropagation,
using
little
in
the
way
of
complex
math.
However,
some
discussion
of
the
math
behind
backpropagation
is
necessary.
The
Goal
of
Backpropagation
Let’s
start
by
defining
the
goal
of
backpropagation.
The
weights
of
a
deep
neural
network
are
the
strength
of
connections
between
units
of
a
neural
network.
When
the
neural
network
is
established
assumptions
are
made
about
how
the
units
in
one
layer
are
connected
to
the
layers
joined
with
it.
As
the
data
moves
through
the
neural
network,
the
weights
are
calculated
and
assumptions
are
made.
When
the
data
reaches
the
final
layer
of
the
network,
a
prediction
is
made
about
how
the
features
are
related
to
the
classes
in
the
dataset.
The
difference
between
the
predicted
values
and
the
actual
values
is
the
loss/error,
and
the
goal
of
backpropagation
is
to
reduce
the
loss
.
This
is
accomplished
by
adjusting
the
weights
of
the
network,
making
the
assumptions
more
like
the
true
relationships
between
the
input
features.
Training
A
Deep
Neural
Network
Before
backpropagation
can
be
done
on
a
neural
network
,
the
regular/forward
training
pass
of
a
neural
network
must
be
carried
out.
When
a
neural
network
is
created,
a
set
of
weights
is
initialized.
The
value
of
the
weights
will
be
altered
as
the
network
is
trained.
The
forward
training
pass
of
a
neural
network
can
be
conceived
of
as
three
discrete
steps:
neuron
activation,
neuron
transfer,
and
forward
propagation.
When
training
a
deep
neural
network,
we
need
to
make
use
of
multiple
mathematical
functions.
Neurons
in
a
deep
neural
network
are
comprised
of
the
incoming
data
and
an
activation
function,
which
determines
the
value
necessary
to
activate
the
node.
The
activation
value
of
a
neuron
is
calculated
with
several
components,
being
a
weighted
sum
of
the
inputs.
The
weights
and
input
values
depend
on
the
index
of
the
nodes
being
used
to
calculate
the
activation.
Another
number
must
be
taken
into
account
when
calculating
the
activation
value,
a
bias
value.
Bias
values
don’t
fluctuate,
so
they
aren’t
multiplied
together
with
the
weight
and
inputs,
they
are
just
added.
All
of
this
means
that
the
following
equation
could
be
used
to
calculate
the
activation
value:
Activation
=
sum(weight
*
input)
+
bias
After
the
neuron
is
activated,
an
activation
function
is
used
to
determine
what
the
output
of
the
actual
output
of
the
neuron
will
be.
Different
activation
functions
are
optimal
for
different
learning
tasks,
but
commonly
used
activation
functions
include
the
sigmoid
function,
the
Tanh
function,
and
the
ReLU
function.
Once
the
outputs
of
the
neuron
are
calculated
by
running
the
activation
value
through
the
desired
activation
function,
forward
propagation
is
done.
Forward
propagation
is
just
taking
the
outputs
of
one
layer
and
making
them
the
inputs
of
the
next
layer.
The
new
inputs
are
then
used
to
calculate
the
new
activation
functions,
and
the
output
of
this
operation
passed
on
to
the
following
layer.
This
process
continues
all
the
way
through
to
the
end
of
the
neural
network.
Backpropagation
in
the
Network
The
process
of
backpropagation
takes
in
the
final
decisions
of
a
model’s
training
pass,
and
then
it
determines
the
errors
in
these
decisions.
The
errors
are
calculated
by
contrasting
the
outputs/decisions
of
the
network
and
the
expected/desired
outputs
of
the
network.
Once
the
errors
in
the
network’s
decisions
have
been
calculated,
this
information
is
backpropagated
through
the
network
and
the
parameters
of
the
network
are
altered
along
the
way.
The
method
that
is
used
to
update
the
weights
of
the
network
is
based
in
calculus,
specifically,
it’s
based
in
the
chain-rule.
However,
an
understanding
of
calculus
isn’t
necessary
to
understand
the
idea
of
behind
backpropagation.
Just
know
that
when
an
output
value
is
provided
from
a
neuron,
the
slope
of
the
output
value
is
calculated
with
a
transfer
function,
producing
a
derived
output.
When
doing
backpropagation,
the
error
for
a
specific
neuron
is
calculated
according
to
the
following
formula
:
error
=
(expected_output
&#8211;
actual_output)
*
slope
of
neuron’s
output
value
When
operating
on
the
neurons
in
the
output
layer,
the
class
value
is
used
as
the
expected
value.
After
the
error
has
been
calculated,
the
error
is
used
as
the
input
for
the
neurons
in
the
hidden
layer,
meaning
that
the
error
for
this
hidden
layer
is
the
weighted
errors
of
the
neurons
found
within
the
output
layer.
The
error
calculations
travel
backward
through
the
network
along
the
weights
network.
After
the
errors
for
the
network
have
been
calculated,
the
weights
in
the
network
must
be
updated.
As
mentioned,
calculating
the
error
involves
determining
the
slope
of
the
output
value.
After
the
slope
has
been
calculated,
a
process
known
as
gradient
descent
can
be
used
to
adjust
the
weights
in
the
network.
A
gradient
is
a
slope,
whose
angle/steepness
can
be
measured.
Slope
is
calculated
by
plotting
“y
over”
or
the
“rise”
over
the
“run”.
In
the
case
of
the
neural
network
and
the
error
rate,
the
“y”
is
the
calculated
error,
while
the
“x”
is
the
network’s
parameters.
The
network’s
parameters
have
a
relationship
to
the
calculated
error
values,
and
as
the
network’s
weights
are
adjusted
the
error
increases
or
decreases.
“Gradient
descent”
is
the
process
of
updating
the
weights
so
that
the
error
rate
decreases.
Backpropagation
is
used
to
predict
the
relationship
between
the
neural
network’s
parameters
and
the
error
rate,
which
sets
up
the
network
for
gradient
descent.
Training
a
network
with
gradient
descent
involved
calculating
the
weights
through
forward
propagation,
backpropagating
the
error,
and
then
updating
the
weights
of
the
network.
<!--mvp-content-main--
>
Related
Topics:
101
backpropagation
<!--mvp-post-tags--
>
<!--posts-nav-link--
>
Up
Next
What
is
Gradient
Descent?
<!--mvp-prev-next-text--
>
<!--mvp-next-cont-in--
>
<!--mvp-prev-next-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-next-post-wrap--
>
Don&#039;t
Miss
What
is
Meta-Learning?
<!--mvp-prev-next-text--
>
<!--mvp-prev-cont-in--
>
<!--mvp-prev-cont-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-prev-post-wrap--
>
<!--mvp-prev-next-wrap--
>
<!--mvp-author-box-img--
>
Daniel
Nelson
<!--mvp-author-box-soc-wrap--
>
<!--mvp-author-box-head--
>
<!--mvp-author-box-in--
>
<!--mvp-author-box-out--
>
Blogger
and
programmer
with
specialties
in
Machine
Learning
and
Deep
Learning
topics.
Daniel
hopes
to
help
others
use
the
power
of
AI
for
social
good.
<!--mvp-author-box-text--
>
<!--author__item--
>
<!--mvp-author-box-wrap--
>
<!--mvp-org-logo--
>
<!--mvp-org-wrap--
>
<!--mvp-content-bot--
>
<!--mvp-content-body-top--
>
You
may
like
<!--mvp-related-img--
>
What
is
Few-Shot
Learning?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
Are
Transformer
Neural
Networks?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
Edge
AI
&#038;
Edge
Computing?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
Ensemble
Learning?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
Dimensionality
Reduction?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
a
Generative
Adversarial
Network
(GAN)?
<!--mvp-related-text--
>
<!--mvp-related-posts--
>
<!--mvp-cont-read-wrap--
>
<!--mvp-content-body--
>
<!--mvp-post-soc-in--
>
<!--mvp-post-soc-out--
>
<!--mvp-content-wrap--
>
<!--mvp-post-content--
>
<!--mvp-post-main-in--
>
<!--mvp-post-main-out--
>
<!--mvp-post-main--
>
<!--mvp-main-box--
>
<!--mvp-article-cont--
>
<!--mvp-article-wrap--
>
<!--mvp-main-body-wrap--
>
Meet
the
Team
Our
Charter
Press
Tools
Contact
Us
Advertiser
Disclosure
:
Unite.AI
is
committed
to
rigorous
editorial
standards
to
provide
our
readers
with
accurate
information
and
news.
We
may
receive
compensation
when
you
click
on
links
to
products
we
reviewed.
Copyright
©
2021
Unite.AI
Editorial
Policy
Privacy
Policy
Terms
and
Conditions
<!--mvp-site-main--
>
<!--mvp-site-wall--
>
<!--mvp-site--
>
<!--mvp-fly-top--
>
<!--mvp-fly-fade--
>

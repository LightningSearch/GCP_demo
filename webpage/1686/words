<!--mvp-fly-logo--
>
<!--mvp-fly-top-in--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-fly-top-out--
>
<!--mvp-fly-menu-top--
>
News
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
Data
Science
COVID-19
Cybersecurity
Deep
Learning
Deepfakes
Education
Environment
Ethics
Facial
Recognition
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Decision
Tree
Data
Science
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Edge
AI
&#038;
Edge
Computing
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
Generative
vs
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Linear
Regression
Long
Short-Term
Memory
(LSTM)
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Courses
Conferences
AI
Conferences
Cybersecurity
Conferences
Robotics
Conferences
Interviews
Thought
Leaders
Newsletters
Organizations
Meet
the
Team
Our
Charter
Contact
Us
<!--mvp-fly-menu-wrap--
>
Connect
with
us
<!--mvp-fly-soc-wrap--
>
<!--mvp-fly-wrap--
>
<!--mvp-search-box--
>
<!--mvp-search-but-wrap--
>
<!--mvp-search-wrap--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-nav-small-left--
>
Unite.AI
<!--mvp-nav-small-logo--
>
What
are
RNNs
and
LSTMs
in
Deep
Learning?
<!--mvp-drop-nav-title--
>
News
A
-
C
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
COVID-19
Cybersecurity
D
-
F
Data
Science
Deepfakes
Deep
Learning
Education
Ethics
Environment
Facial
Recognition
G
-
Q
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
R
-
Z
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
A
-
D
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
D
-
K
Dimensionality
Reduction
Edge
AI
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
(GAN)
Generative
vs.
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
L
-
Q
Linear
Regression
Long
Short-Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
R
-
Z
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Learning
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Conferences
Artificial
Intelligence
Cybersecurity
Robotics
Interviews
Thought
Leaders
Meet
the
Team
Contact
<!--mvp-nav-menu--
>
<!--mvp-nav-small-mid-right--
>
<!--mvp-nav-small-mid--
>
<!--mvp-nav-small-left-in--
>
<!--mvp-nav-small-left-out--
>
<!--mvp-nav-small-cont--
>
<!--mvp-nav-small-right-in--
>
<!--mvp-nav-small-right--
>
<!--mvp-nav-small-right-out--
>
<!--mvp-nav-small-wrap--
>
<!--mvp-main-box--
>
<!--mvp-main-nav-small-cont--
>
<!--mvp-main-nav-small--
>
<!--mvp-main-nav-wrap--
>
<!--mvp-main-head-wrap--
>
AI
Masterclass:
Terminology
(A
to
D)
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Terminology
(E
to
K)
Edge
AI
Ensemble
Learning
Federated
Learning
Generative
Adversarial
Network
Generative
vs.
Discriminative
Gradient
Boosting
Gradient
Descent
Few-Shot
Learning
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Terminology
(L
to
Q)
Linear
Regression
Long-Short
Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Terminology
(R
to
Z)
Reinforcement
Learning
Robotic
Process
Automation
Structured
vs
Unstructured
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
AI
101
What
are
RNNs
and
LSTMs
in
Deep
Learning?
<!--mvp-author-info-thumb--
>
Updated
7
months
ago
&nbsp;on
August
23,
2020
<!--mvp-author-info-date--
>
By
Daniel
Nelson
<!--mvp-author-info-name--
>
<!--mvp-author-info-text--
>
<!--mvp-author-info-wrap--
>
Table
Of
Contents
<!--mvp-post-img-hide--
>
Many
of
the
most
impressive
advances
in
natural
language
processing
and
AI
chatbots
are
driven
by
Recurrent
Neural
Networks
(RNNs)
and
Long
Short-Term
Memory
(LSTM)
networks
.
RNNs
and
LSTMs
are
special
neural
network
architectures
that
are
able
to
process
sequential
data,
data
where
chronological
ordering
matters.
LSTMs
are
essentially
improved
versions
of
RNNs
,
capable
of
interpreting
longer
sequences
of
data.
Let’s
take
a
look
at
how
RNNs
and
LSTMS
are
structured
and
how
they
enable
the
creation
of
sophisticated
natural
language
processing
systems.
What
are
Feed-Forward
Neural
Networks?
So
before
we
talk
about
how
Long
Short-Term
Memory
(LSTM)
and
Convolutional
Neural
Networks
(CNN)
work,
we
should
discuss
the
format
of
a
neural
network
in
general.
A
neural
network
is
intended
to
examine
data
and
learn
relevant
patterns,
so
that
these
patterns
can
be
applied
to
other
data
and
new
data
can
be
classified.
Neural
networks
are
divided
into
three
sections:
an
input
layer,
a
hidden
layer
(or
multiple
hidden
layers),
and
an
output
layer.
The
input
layer
is
what
takes
in
the
data
into
the
neural
network,
while
the
hidden
layers
are
what
learn
the
patterns
in
the
data.
The
hidden
layers
in
the
dataset
are
connected
to
the
input
and
output
layers
by
“weights”
and
“biases”
which
are
just
assumptions
of
how
the
data
points
are
related
to
each
other.
These
weights
are
adjusted
during
training.
As
the
network
trains,
the
model’s
guesses
about
the
training
data
(the
output
values)
are
compared
against
the
actual
training
labels.
During
the
course
of
training,
the
network
should
(hopefully)
get
more
accurate
at
predicting
relationships
between
data
points,
so
it
can
accurately
classify
new
data
points.
Deep
neural
networks
are
networks
that
have
more
layers
in
the
middle/more
hidden
layers.
The
more
hidden
layers
and
more
neurons/nodes
the
model
has,
the
better
the
model
can
recognize
patterns
in
the
data.
Regular,
feed-forward
neural
networks,
like
the
ones
I’ve
described
above
are
often
called
“dense
neural
networks”.
These
dense
neural
networks
are
combined
with
different
network
architectures
that
specialize
in
interpreting
different
kinds
of
data.
What
are
RNNs
(Recurrent
Neural
Networks)?
Recurrent
Neural
Networks
take
the
general
principle
of
feed-forward
neural
networks
and
enable
them
to
handle
sequential
data
by
giving
the
model
an
internal
memory
.
The
“Recurrent”
portion
of
the
RNN
name
comes
from
the
fact
that
the
input
and
outputs
loop.
Once
the
output
of
the
network
is
produced,
the
output
is
copied
and
returned
to
the
network
as
input.
When
making
a
decision,
not
only
the
current
input
and
output
are
analyzed,
but
the
previous
input
is
also
considered.
To
put
that
another
way,
if
the
initial
input
for
the
network
is
X
and
the
output
is
H,
both
H
and
X1
(the
next
input
in
the
data
sequence)
are
fed
into
the
network
for
the
next
round
of
learning.
In
this
way,
the
context
of
the
data
(the
previous
inputs)
is
preserved
as
the
network
trains.
The
result
of
this
architecture
is
that
RNNs
are
capable
fo
handling
sequential
data.
However,
RNNs
suffer
from
a
couple
of
issues.
RNNs
suffer
from
the
vanishing
gradient
and
exploding
gradient
problems.
The
length
of
sequences
that
an
RNN
can
interpret
are
rather
limited,
especially
in
comparison
to
LSTMs.
What
are
LSTMs
(Long
Short-Term
Memory
Networks)?
Long
Short-Term
Memory
networks
can
be
considered
extensions
of
RNNs,
once
more
applying
the
concept
of
preserving
the
context
of
inputs.
However,
LSTMs
have
been
modified
in
several
important
ways
that
allow
them
to
interpret
past
data
with
superior
methods.
The
alterations
made
to
LSTMs
deal
with
the
vanishing
gradient
problem
and
enable
LSTMs
to
consider
much
longer
input
sequences.
LSTM
models
are
made
up
of
three
different
components,
or
gates
.
There’s
an
input
gate,
an
output
gate,
and
a
forget
gate
.
Much
like
RNNs,
LSTMs
take
inputs
from
the
previous
timestep
into
account
when
modifying
the
model’s
memory
and
input
weights.
The
input
gate
makes
decisions
about
which
values
are
important
and
should
be
let
through
the
model.
A
sigmoid
function
is
used
in
the
input
gate,
which
makes
determinations
about
which
values
to
pass
on
through
the
recurrent
network.
Zero
drops
the
value,
while
1
preserves
it.
A
TanH
function
is
used
here
as
well,
which
decides
how
important
to
the
model
the
input
values
are,
ranging
from
-1
to
1.
After
the
current
inputs
and
memory
state
are
accounted
for,
the
output
gate
decides
which
values
to
push
to
the
next
time
step.
In
the
output
gate,
the
values
are
analyzed
and
assigned
an
importance
ranging
from
-1
to
1.
This
regulates
the
data
before
it
is
carried
on
to
the
next
time-step
calculation. 
Finally,
the
job
of
the
forget
gate
is
to
drop
information
that
the
model
deems
unnecessary
to
make
a
decision
about
the
nature
of
the
input
values.
The
forget
gate
uses
a
sigmoid
function
on
the
values,
outputting
numbers
between
0
(forget
this)
and
1
(keep
this).
An
LSTM
neural
network
is
made
out
of
both
special
LSTM
layers
that
can
interpret
sequential
word
data
and
the
densely
connected
like
those
described
above.
Once
the
data
moves
through
the
LSTM
layers,
it
proceeds
into
the
densely
connected
layers.
<!--mvp-content-main--
>
Related
Topics:
101
Long
Short-Term
Memory
network
LSTM
recurrent
neural
network
RNN
<!--mvp-post-tags--
>
<!--posts-nav-link--
>
Up
Next
What
is
Bayes
Theorem?
<!--mvp-prev-next-text--
>
<!--mvp-next-cont-in--
>
<!--mvp-prev-next-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-next-post-wrap--
>
Don&#039;t
Miss
What
is
a
KNN
(K-Nearest
Neighbors)?
<!--mvp-prev-next-text--
>
<!--mvp-prev-cont-in--
>
<!--mvp-prev-cont-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-prev-post-wrap--
>
<!--mvp-prev-next-wrap--
>
<!--mvp-author-box-img--
>
Daniel
Nelson
<!--mvp-author-box-soc-wrap--
>
<!--mvp-author-box-head--
>
<!--mvp-author-box-in--
>
<!--mvp-author-box-out--
>
Blogger
and
programmer
with
specialties
in
Machine
Learning
and
Deep
Learning
topics.
Daniel
hopes
to
help
others
use
the
power
of
AI
for
social
good.
<!--mvp-author-box-text--
>
<!--author__item--
>
<!--mvp-author-box-wrap--
>
<!--mvp-org-logo--
>
<!--mvp-org-wrap--
>
<!--mvp-content-bot--
>
<!--mvp-content-body-top--
>
You
may
like
<!--mvp-related-img--
>
What
is
Few-Shot
Learning?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
Are
Transformer
Neural
Networks?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
Researchers
Give
Robotic
AI
Spontaneous
Behavior
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
Edge
AI
&#038;
Edge
Computing?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
Ensemble
Learning?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
Dimensionality
Reduction?
<!--mvp-related-text--
>
<!--mvp-related-posts--
>
<!--mvp-cont-read-wrap--
>
<!--mvp-content-body--
>
<!--mvp-post-soc-in--
>
<!--mvp-post-soc-out--
>
<!--mvp-content-wrap--
>
<!--mvp-post-content--
>
<!--mvp-post-main-in--
>
<!--mvp-post-main-out--
>
<!--mvp-post-main--
>
<!--mvp-main-box--
>
<!--mvp-article-cont--
>
<!--mvp-article-wrap--
>
<!--mvp-main-body-wrap--
>
Meet
the
Team
Our
Charter
Press
Tools
Contact
Us
Advertiser
Disclosure
:
Unite.AI
is
committed
to
rigorous
editorial
standards
to
provide
our
readers
with
accurate
information
and
news.
We
may
receive
compensation
when
you
click
on
links
to
products
we
reviewed.
Copyright
©
2021
Unite.AI
Editorial
Policy
Privacy
Policy
Terms
and
Conditions
<!--mvp-site-main--
>
<!--mvp-site-wall--
>
<!--mvp-site--
>
<!--mvp-fly-top--
>
<!--mvp-fly-fade--
>

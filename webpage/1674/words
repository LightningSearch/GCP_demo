<!--mvp-fly-logo--
>
<!--mvp-fly-top-in--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-fly-top-out--
>
<!--mvp-fly-menu-top--
>
News
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
Data
Science
COVID-19
Cybersecurity
Deep
Learning
Deepfakes
Education
Environment
Ethics
Facial
Recognition
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Decision
Tree
Data
Science
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Edge
AI
&#038;
Edge
Computing
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
Generative
vs
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Linear
Regression
Long
Short-Term
Memory
(LSTM)
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Courses
Conferences
AI
Conferences
Cybersecurity
Conferences
Robotics
Conferences
Interviews
Thought
Leaders
Newsletters
Organizations
Meet
the
Team
Our
Charter
Contact
Us
<!--mvp-fly-menu-wrap--
>
Connect
with
us
<!--mvp-fly-soc-wrap--
>
<!--mvp-fly-wrap--
>
<!--mvp-search-box--
>
<!--mvp-search-but-wrap--
>
<!--mvp-search-wrap--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-nav-small-left--
>
Unite.AI
<!--mvp-nav-small-logo--
>
What
is
a
KNN
(K-Nearest
Neighbors)?
<!--mvp-drop-nav-title--
>
News
A
-
C
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
COVID-19
Cybersecurity
D
-
F
Data
Science
Deepfakes
Deep
Learning
Education
Ethics
Environment
Facial
Recognition
G
-
Q
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
R
-
Z
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
A
-
D
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
D
-
K
Dimensionality
Reduction
Edge
AI
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
(GAN)
Generative
vs.
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
L
-
Q
Linear
Regression
Long
Short-Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
R
-
Z
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Learning
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Conferences
Artificial
Intelligence
Cybersecurity
Robotics
Interviews
Thought
Leaders
Meet
the
Team
Contact
<!--mvp-nav-menu--
>
<!--mvp-nav-small-mid-right--
>
<!--mvp-nav-small-mid--
>
<!--mvp-nav-small-left-in--
>
<!--mvp-nav-small-left-out--
>
<!--mvp-nav-small-cont--
>
<!--mvp-nav-small-right-in--
>
<!--mvp-nav-small-right--
>
<!--mvp-nav-small-right-out--
>
<!--mvp-nav-small-wrap--
>
<!--mvp-main-box--
>
<!--mvp-main-nav-small-cont--
>
<!--mvp-main-nav-small--
>
<!--mvp-main-nav-wrap--
>
<!--mvp-main-head-wrap--
>
AI
Masterclass:
Terminology
(A
to
D)
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Terminology
(E
to
K)
Edge
AI
Ensemble
Learning
Federated
Learning
Generative
Adversarial
Network
Generative
vs.
Discriminative
Gradient
Boosting
Gradient
Descent
Few-Shot
Learning
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Terminology
(L
to
Q)
Linear
Regression
Long-Short
Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Terminology
(R
to
Z)
Reinforcement
Learning
Robotic
Process
Automation
Structured
vs
Unstructured
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
AI
101
What
is
a
KNN
(K-Nearest
Neighbors)?
<!--mvp-author-info-thumb--
>
Updated
7
months
ago
&nbsp;on
August
23,
2020
<!--mvp-author-info-date--
>
By
Daniel
Nelson
<!--mvp-author-info-name--
>
<!--mvp-author-info-text--
>
<!--mvp-author-info-wrap--
>
Table
Of
Contents
<!--mvp-post-img-hide--
>
What
is
K-Nearest
Neighbors
(KNN)?
K-Nearest
Neighbors
is
a
machine
learning
technique
and
algorithm
that
can
be
used
for
both
regression
and
classification
tasks
.
K-Nearest
Neighbors
examines
the
labels
of
a
chosen
number
of
data
points
surrounding
a
target
data
point,
in
order
to
make
a
prediction
about
the
class
that
the
data
point
falls
into.
K-Nearest
Neighbors
(KNN)
is
a
conceptually
simple
yet
very
powerful
algorithm,
and
for
those
reasons,
it’s
one
of
the
most
popular
machine
learning
algorithms.
Let’s
take
a
deep
dive
into
the
KNN
algorithm
and
see
exactly
how
it
works.
Having
a
good
understanding
of
how
KNN
operates
will
let
you
appreciated
the
best
and
worst
use
cases
for
KNN.
Overview
of
K-Nearest
Neighbors
(KNN)
Photo:
Antti
Ajanki
AnAj
via
Wikimedia
Commons,
CC
BY
SA
3.0
(https://commons.wikimedia.org/wiki/File:KnnClassification.svg)
Let’s
visualize
a
dataset
on
a
2D
plane.
Picture
a
bunch
of
data
points
on
a
graph,
spread
out
along
the
graph
in
small
clusters.
KNN
examines
the
distribution
of
the
data
points
and,
depending
on
the
arguments
given
to
the
model,
it
separates
the
data
points
into
groups.
These
groups
are
then
assigned
a
label.
The
primary
assumption
that
a
KNN
model
makes
is
that
data
points/instances
which
exist
in
close
proximity
to
each
other
are
highly
similar,
while
if
a
data
point
is
far
away
from
another
group
it’s
dissimilar
to
those
data
points.
A
KNN
model
calculates
similarity
using
the
distance
between
two
points
on
a
graph.
The
greater
the
distance
between
the
points,
the
less
similar
they
are.
There
are
multiple
ways
of
calculating
the
distance
between
points,
but
the
most
common
distance
metric
is
just
Euclidean
distance
(the
distance
between
two
points
in
a
straight
line).
KNN
is
a
supervised
learning
algorithm,
meaning
that
the
examples
in
the
dataset
must
have
labels
assigned
to
them/their
classes
must
be
known.
There
are
two
other
important
things
to
know
about
KNN.
First,
KNN
is
a
non-parametric
algorithm.
This
means
that
no
assumptions
about
the
dataset
are
made
when
the
model
is
used.
Rather,
the
model
is
constructed
entirely
from
the
provided
data.
Second,
there
is
no
splitting
of
the
dataset
into
training
and
test
sets
when
using
KNN.
KNN
makes
no
generalizations
between
a
training
and
testing
set,
so
all
the
training
data
is
also
used
when
the
model
is
asked
to
make
predictions.
How
a
KNN
Algorithm
Operates
A
KNN
algorithm
goes
through
three
main
phases
as
it
is
carried
out:
Setting
K
to
the
chosen
number
of
neighbors.
Calculating
the
distance
between
a
provided/test
example
and
the
dataset
examples.
Sorting
the
calculated
distances.
Getting
the
labels
of
the
top
K
entries.
Returning
a
prediction
about
the
test
example.
In
the
first
step,
K
is
chosen
by
the
user
and
it
tells
the
algorithm
how
many
neighbors
(how
many
surrounding
data
points)
should
be
considered
when
rendering
a
judgment
about
the
group
the
target
example
belongs
to.
In
the
second
step,
note
that
the
model
checks
the
distance
between
the
target
example
and
every
example
in
the
dataset.
The
distances
are
then
added
into
a
list
and
sorted.
Afterward,
the
sorted
list
is
checked
and
the
labels
for
the
top
K
elements
are
returned.
In
other
words,
if
K
is
set
to
5,
the
model
checks
the
labels
of
the
top
5
closest
data
points
to
the
target
data
point.
When
rendering
a
prediction
about
the
target
data
point,
it
matters
if
the
task
is
a
regression
or
classification
task.
For
a
regression
task,
the
mean
of
the
top
K
labels
is
used,
while
the
mode
of
the
top
K
labels
is
used
in
the
case
of
classification.
The
exact
mathematical
operations
used
to
carry
out
KNN
differ
depending
on
the
chosen
distance
metric.
If
you
would
like
to
learn
more
about
how
the
metrics
are
calculated,
you
can
read
about
some
of
the
most
common
distance
metrics,
such
as
Euclidean
,
Manhattan
,
and
Minkowski
.
Why
The
Value
Of
K
Matters
The
main
limitation
when
using
KNN
is
that
in
an
improper
value
of
K
(the
wrong
number
of
neighbors
to
be
considered)
might
be
chosen.
If
this
happen,
the
predictions
that
are
returned
can
be
off
substantially.
It’s
very
important
that,
when
using
a
KNN
algorithm,
the
proper
value
for
K
is
chosen.
You
want
to
choose
a
value
for
K
that
maximizes
the
model’s
ability
to
make
predictions
on
unseen
data
while
reducing
the
number
of
errors
it
makes.
Photo:
Agor153
via
Wikimedia
Commons,
CC
BY
SA
3.0
(https://en.wikipedia.org/wiki/File:Map1NN.png)
Lower
values
of
K
mean
that
the
predictions
rendered
by
the
KNN
are
less
stable
and
reliable.
To
get
an
intuition
of
why
this
is
so,
consider
a
case
where
we
have
7
neighbors
around
a
target
data
point.
Let’s
assume
that
the
KNN
model
is
working
with
a
K
value
of
2
(we’re
asking
it
to
look
at
the
two
closest
neighbors
to
make
a
prediction).
If
the
vast
majority
of
the
neighbors
(five
out
of
seven)
belong
to
the
Blue
class,
but
the
two
closest
neighbors
just
happen
to
be
Red,
the
model
will
predict
that
the
query
example
is
Red.
Despite
the
model’s
guess,
in
such
a
scenario
Blue
would
be
a
better
guess.
If
this
is
the
case,
why
not
just
choose
the
highest
K
value
we
can?
This
is
because
telling
the
model
to
consider
too
many
neighbors
will
also
reduce
accuracy.
As
the
radius
that
the
KNN
model
considers
increases,
it
will
eventually
start
considering
data
points
that
are
closer
to
other
groups
than
they
are
the
target
data
point
and
misclassification
will
start
occurring.
For
example,
even
if
the
point
that
was
initially
chosen
was
in
one
of
the
red
regions
above,
if
K
was
set
too
high,
the
model
would
reach
into
the
other
regions
to
consider
points.
When
using
a
KNN
model,
different
values
of
K
are
tried
to
see
which
value
gives
the
model
the
best
performance.
KNN
Pros
And
Cons
Let’s
examine
some
of
the
pros
and
cons
of
the
KNN
model.
Pros:
KNN
can
be
used
for
both
regression
and
classification
tasks,
unlike
some
other
supervised
learning
algorithms.
KNN
is
highly
accurate
and
simple
to
use.
It’s
easy
to
interpret,
understand,
and
implement.
KNN
doesn’t
make
any
assumptions
about
the
data,
meaning
it
can
be
used
for
a
wide
variety
of
problems.
Cons:
KNN
stores
most
or
all
of
the
data,
which
means
that
the
model
requires
a
lot
of
memory
and
its
computationally
expensive.
Large
datasets
can
also
cause
predictions
to
be
take
a
long
time.
KNN
proves
to
be
very
sensitive
to
the
scale
of
the
dataset
and
it
can
be
thrown
off
by
irrelevant
features
fairly
easily
in
comparison
to
other
models.
Summary
of
K-Nearest
Neighbors
(KNN)
K-Nearest
Neighbors
is
one
of
the
simplest
machine
learning
algorithms.
Despite
how
simple
KNN
is,
in
concept,
it’s
also
a
powerful
algorithm
that
gives
fairly
high
accuracy
on
most
problems.
When
you
use
KNN,
be
sure
to
experiment
with
various
values
of
K
in
order
to
find
the
number
that
provides
the
highest
accuracy.
<!--mvp-content-main--
>
Related
Topics:
101
k-nearest
neighbors
KNN
Machine
learning
algorithms
<!--mvp-post-tags--
>
<!--posts-nav-link--
>
Up
Next
What
are
RNNs
and
LSTMs
in
Deep
Learning?
<!--mvp-prev-next-text--
>
<!--mvp-next-cont-in--
>
<!--mvp-prev-next-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-next-post-wrap--
>
Don&#039;t
Miss
What
is
Linear
Regression?
<!--mvp-prev-next-text--
>
<!--mvp-prev-cont-in--
>
<!--mvp-prev-cont-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-prev-post-wrap--
>
<!--mvp-prev-next-wrap--
>
<!--mvp-author-box-img--
>
Daniel
Nelson
<!--mvp-author-box-soc-wrap--
>
<!--mvp-author-box-head--
>
<!--mvp-author-box-in--
>
<!--mvp-author-box-out--
>
Blogger
and
programmer
with
specialties
in
Machine
Learning
and
Deep
Learning
topics.
Daniel
hopes
to
help
others
use
the
power
of
AI
for
social
good.
<!--mvp-author-box-text--
>
<!--author__item--
>
<!--mvp-author-box-wrap--
>
<!--mvp-org-logo--
>
<!--mvp-org-wrap--
>
<!--mvp-content-bot--
>
<!--mvp-content-body-top--
>
You
may
like
<!--mvp-related-img--
>
What
is
Few-Shot
Learning?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
A
Quick
Guide
to
Understanding
a
KNN
Algorithm
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
Are
Transformer
Neural
Networks?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
AI
Tool
Enables
Movie
Ratings
Before
Shooting
First
Scene
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
Edge
AI
&#038;
Edge
Computing?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
Ensemble
Learning?
<!--mvp-related-text--
>
<!--mvp-related-posts--
>
<!--mvp-cont-read-wrap--
>
<!--mvp-content-body--
>
<!--mvp-post-soc-in--
>
<!--mvp-post-soc-out--
>
<!--mvp-content-wrap--
>
<!--mvp-post-content--
>
<!--mvp-post-main-in--
>
<!--mvp-post-main-out--
>
<!--mvp-post-main--
>
<!--mvp-main-box--
>
<!--mvp-article-cont--
>
<!--mvp-article-wrap--
>
<!--mvp-main-body-wrap--
>
Meet
the
Team
Our
Charter
Press
Tools
Contact
Us
Advertiser
Disclosure
:
Unite.AI
is
committed
to
rigorous
editorial
standards
to
provide
our
readers
with
accurate
information
and
news.
We
may
receive
compensation
when
you
click
on
links
to
products
we
reviewed.
Copyright
©
2021
Unite.AI
Editorial
Policy
Privacy
Policy
Terms
and
Conditions
<!--mvp-site-main--
>
<!--mvp-site-wall--
>
<!--mvp-site--
>
<!--mvp-fly-top--
>
<!--mvp-fly-fade--
>

<!--mvp-fly-logo--
>
<!--mvp-fly-top-in--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-fly-top-out--
>
<!--mvp-fly-menu-top--
>
News
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
Data
Science
COVID-19
Cybersecurity
Deep
Learning
Deepfakes
Education
Environment
Ethics
Facial
Recognition
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Decision
Tree
Data
Science
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Edge
AI
&#038;
Edge
Computing
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
Generative
vs
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Linear
Regression
Long
Short-Term
Memory
(LSTM)
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Courses
Conferences
AI
Conferences
Cybersecurity
Conferences
Robotics
Conferences
Interviews
Thought
Leaders
Newsletters
Organizations
Meet
the
Team
Our
Charter
Contact
Us
<!--mvp-fly-menu-wrap--
>
Connect
with
us
<!--mvp-fly-soc-wrap--
>
<!--mvp-fly-wrap--
>
<!--mvp-search-box--
>
<!--mvp-search-but-wrap--
>
<!--mvp-search-wrap--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-nav-small-left--
>
Unite.AI
<!--mvp-nav-small-logo--
>
Vahid
Behzadan,
Director
of
Secured
and
Assured
Intelligent
Learning
(SAIL)
Lab
&#8211;
Interview
Series
<!--mvp-drop-nav-title--
>
News
A
-
C
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
COVID-19
Cybersecurity
D
-
F
Data
Science
Deepfakes
Deep
Learning
Education
Ethics
Environment
Facial
Recognition
G
-
Q
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
R
-
Z
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
A
-
D
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
D
-
K
Dimensionality
Reduction
Edge
AI
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
(GAN)
Generative
vs.
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
L
-
Q
Linear
Regression
Long
Short-Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
R
-
Z
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Learning
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Conferences
Artificial
Intelligence
Cybersecurity
Robotics
Interviews
Thought
Leaders
Meet
the
Team
Contact
<!--mvp-nav-menu--
>
<!--mvp-nav-small-mid-right--
>
<!--mvp-nav-small-mid--
>
<!--mvp-nav-small-left-in--
>
<!--mvp-nav-small-left-out--
>
<!--mvp-nav-small-cont--
>
<!--mvp-nav-small-right-in--
>
<!--mvp-nav-small-right--
>
<!--mvp-nav-small-right-out--
>
<!--mvp-nav-small-wrap--
>
<!--mvp-main-box--
>
<!--mvp-main-nav-small-cont--
>
<!--mvp-main-nav-small--
>
<!--mvp-main-nav-wrap--
>
<!--mvp-main-head-wrap--
>
Interviews
Vahid
Behzadan,
Director
of
Secured
and
Assured
Intelligent
Learning
(SAIL)
Lab
&#8211;
Interview
Series
<!--mvp-author-info-thumb--
>
Published
11
months
ago
&nbsp;on
April
27,
2020
<!--mvp-author-info-date--
>
By
Antoine
Tardif
<!--mvp-author-info-name--
>
<!--mvp-author-info-text--
>
<!--mvp-author-info-wrap--
>
Table
Of
Contents
<!--mvp-post-feat-img--
>
Vahid
is
an
Assistant
Professor
of
Computer
Science
and
Data
Science
at
the
University
of
New
Haven.
He
is
also
director
of
the 
Secure
and
Assured
Intelligent
Learning
(SAIL)
Lab
His
research
interests
include
safety
and
security
of
intelligent
systems,
psychological
modeling
of
AI
safety
problems,
security
of
complex
adaptive
systems,
game
theory,
multi-agent
systems,
and
cyber-security.
You
have
an
extensive
background
in
cybersecurity
and
keeping
AI
safe.
Can
you
share
your
journey
in
how
you
became
attracted
to
both
fields?
My
research
trajectory
has
been
fueled
by
two
core
interests
of
mine:
finding
out
how
things
break,
and
learning
about
the
mechanics
of
the
human
mind. I
have
been
actively
involved
in
cybersecurity
since
my
early
teen
years,
and
consequently
built
my
early
research
agenda
around
the
classical
problems
of
this
domain.
Few
years
into
my
graduate
studies,
I
stumbled
upon
a
rare
opportunity
to
change
my
area
of
research.
At
that
time,
I
had
just
come
across
the
early
works
of
Szegedy
and
Goodfellow
on
adversarial
example
attacks,
and
found
the
idea
of
attacking
machine
learning
very
intriguing.
As
I
looked
deeper
into
this
problem,
I
came
to
learn
about
the
more
general
field
of AI
safety
and
security,
and
found
it
to
encompass
many
of
my
core
interests,
such
as
cybersecurity,
cognitive
sciences,
economics,
and
philosophy. I
also
came
to
believe
that
research
in
this
area
is
not
only
fascinating,
but
also
vital
for
ensuring
the
long-term
benefits
and
safety
of
the
AI
revolution.
&nbsp;
You’re
the
director
of
the
Secure
and
Assured
Intelligent
Learning
(SAIL)
Lab
which
works
towards
laying
concrete
foundations
for
the
safety
and
security
of
intelligent
machines.
Could
you
go
into
some
details
regarding
work
undertaken
by
SAIL?
At
SAIL,
my
students
and
I
work
on
problems
that
lie
in
the
intersection
of
security,
AI,
and
complex
systems.
The
primary
focus
of
our
research
is
on
investigating
the
safety
and
security
of
intelligent
systems,
from
both
the
theoretical
and
the
applied
perspectives.
On
the
theoretical
side,
we
are
currently
investigating
the
value-alignment
problem
in
multi-agent
settings
and
are
developing
mathematical
tools
to
evaluate
and
optimize
the
objectives
of
AI
agents
with
regards
to
stability
and
robust
alignments.
On
the
practical
side,
some
of
our
projects
explore
the
security
vulnerabilities
of
the
cutting-edge
AI
technologies,
such
as
autonomous
vehicles
and
algorithmic
trading,
and
aim
to
develop
techniques
for
evaluating
and
improving
the
resilience
of
such
technologies
to
adversarial
attacks.
We
also
work
on
the
applications
of
machine
learning
in
cybersecurity,
such
as
automated
penetration
testing,
early
detection
of
intrusion
attempts,
and
automated
threat
intelligence
collection
and
analysis
from
open
sources
of
data
such
as
social
media.
&nbsp;
You
recently
led
an
effort
to
propose
the 
modeling
of
AI
safety
problems
as
psychopathological
disorders
.
Could
you
explain
what
this
is?
This
project
addresses
the
rapidly
growing
complexity
of
AI
agents
and
systems:
it
is
already
very
difficult
to
diagnose,
predict,
and
control
unsafe
behaviors
of
reinforcement
learning
agents
in
non-trivial
settings
by
simply
looking
at
their
low-level
configurations.
In
this
work,
we
emphasize
the
need
for
higher-level
abstractions
in
investigating
such
problems.
Inspired
by
the
scientific
approaches
to
behavioral
problems
in
humans,
we
propose
psychopathology
as
a
useful
high-level
abstraction
for
modeling
and
analyzing
emergent
deleterious
behaviors
in
AI
and
AGI.
As
a
proof
of
concept,
we
study
the
AI
safety
problem
of
reward
hacking
in
an
RL
agent
learning
to
play
the
classic
game
of
Snake.
We
show
that
if
we
add
a
&#8220;drug&#8221;
seed
to
the
environment,
the
agent
learns
a
sub-optimal
behavior
that
can
be
described
via
neuroscientific
models
of
addiction.
This
work
also
proposes
control
methodologies
based
on
the
treatment
approaches
used
in
psychiatry.
For
instance,
we
propose
the
use
of
artificially-generated
reward
signals
as
analogues
of
medication
therapy
for
modifying
the
deleterious
behavior
of
agents.
&nbsp;
Do
you
have
any
concerns
with
AI
safety
when
it
comes
to
autonomous
vehicles?
Autonomous
vehicles
are
becoming
prominent
examples
of
deploying
AI
in
cyber-physical
systems.
Considering
the
fundamental
susceptibility
of
current
machine
learning
technologies
to
mistakes
and
adversarial
attacks,
I
am
deeply
concerned
about
the
safety
and
security
of
even
semi-autonomous
vehicles. Also,
the
field
of
autonomous
driving
suffers
from
a
serious
lack
of
safety
standards
and
evaluation
protocols.
However,
I
remain
hopeful. Similar
to
natural
intelligence,
AI
will
also
be
prone
to
making
mistakes.
Yet,
the
objective
of
self-driving
cars
can
still
be
satisfied
if
the
rates
and
impact
of
such
mistakes
are
made
to
be
lower
than
those
of
human
drivers. We
are
witnessing
growing
efforts
to
address
these
issues
in
the
industry
and
academia,
as
well
as
the
governments.
&nbsp;
Hacking
street
signs
 with
stickers
or
using
other
means
can
confuse
the
computer
vision
module
of
an
autonomous
vehicle.
How
big
of
an
issue
do
you
believe
this
is?
These
stickers,
and Adversarial
Examples
in
general,
give
rise
to
fundamental
challenges
in
the
robustness
of
machine
learning
models.
To
quote
George
E.
P.
Box,
&#8220;all
models
are
wrong,
but
some
are
useful&#8221;.
Adversarial
examples
exploit
this
&#8220;wrong&#8221;ness
of
models,
which
is
due
to
their
abstractive
nature,
as
well
as
the
limitations
of
sampled
data
upon
which
they
are
trained.
Recent
efforts
in
the
domain
of
adversarial
machine
learning
have
resulted
in
tremendous
strides
towards
increasing
the
resilience
of
deep
learning
models
to
such
attacks.
From
a
security
point
of
view,
there
will
always
be
a
way
to
fool
machine
learning
models.
However,
the
practical
objective
of
securing
machine
learning
models
is
to
increase
the
cost
of
implementing
such
attacks
to
the
point
of
economic
infeasibility.
&nbsp;
Your
focus
is
on
the
safety
and
security
features
of
both
deep
learning
and
deep
reinforcement
learning
.
Why
is
this
so
important?
Reinforcement
Learning
(RL)
is
the
prominent
method
of
applying
machine
learning
to
control
problems,
which
by
definition
involve
the
manipulation
of
their
environment.
Therefore,
I
believe
systems
that
are
based
on
RL
have
significantly
higher
risks
of
causing
major
damages
in
the
real-world
compared
to
other
machine
learning
methods
such
as
classification.
This
problem
is
further
exacerbated
with
the
integration
of
Deep
learning
in
RL,
which
enables
the
adoption
of
RL
in
highly
complex
settings.
Also,
it
is
my
opinion
that
the
RL
framework
is
closely
related
to
the
underlying
mechanisms
of
cognition
in
human
intelligence,
and
studying
its
safety
and
vulnerabilities
can
lead
to
better
insights
into
the
limits
of
decision-making
in
our
minds.
&nbsp;
Do
you
believe
that
we
are
close
to
achieving
Artificial
General
Intelligence
(AGI)?
This
is
a
notoriously
hard
question
to
answer.
I
believe
that
we
currently
have
the
building
blocks
of
some
architectures
that
can
facilitate
the
emergence
of
AGI.
However,
it
may
take
a
few
more
years
or
decades
to
improve
upon
these
architectures
and
enhance
the
cost-efficiency
of
training
and
maintaining
these
architectures.
Over
the
coming
years,
our
agents
are
going
to
grow
more
intelligent
at
a
rapidly
growing
rate.
I
don&#8217;t
think
the
emergence
of
AGI
will
be
announced
in
the
form
of
a
[scientifically
valid]
headline,
but
as
the
result
of
gradual
progress.
Also,
I
think
we
still
do
not
have
a
widely
accepted
methodology
to
test
and
detect
the
existence
of
an
AGI,
and
this
may
delay
our
realization
of
the
first
instances
of
AGI.
&nbsp;
How
do
we
maintain
safety
in
an
AGI
system
that
is
capable
of
thinking
for
itself
and
will
most
likely
be
exponentially
more
intelligent
than
humans?
I
believe
that
the
grant
unified
theory
of
intelligent
behavior
is
economics
and
the
study
of
how
agents
act
and
interact
to
achieve
what
they
want.
The
decisions
and
actions
of
humans
are
determined
by
their
objectives,
their
information,
and
the
available
resources.
Societies
and
collaborative
efforts
are
emergent
from
its
benefits
for
individual
members
of
such
groups.
Another
example
is
the
criminal
code,
that
deters
certain
decisions
by
attaching
a
high
cost
to
actions
that
may
harm
the
society. In
the
same
way,
I
believe
that
controlling
the
incentives
and
resources
can
enable
the
emergence
a
state
of
equilibrium
between
humans
and
instances
of
AGI.
Currently,
the
AI
safety
community
investigates
this
thesis
under
the
umbrella
of
value-alignment
problems.
&nbsp;
One
of
the
areas
you
closely
follow
is
counterterrorism.
Do
you
have
concerns
with
terrorists
taking
over
AI
or
AGI
systems?
There
are
numerous
concerns
about
the
misuse
of
AI
technologies.
In
the
case
of
terrorist
operations,
the
major
concern
is
the
ease
with
which
terrorists
can
develop
and
carry
out
autonomous
attacks.
A
growing
number
of
my
colleagues
are
actively
warning
against
the
risks
of
developing
autonomous
weapons
(see 
https://autonomousweapons.org/
 ).
One
of
the
main
problems
with
AI-enabled
weaponry
is
in
the
difficulty
of
controlling
the
underlying
technology:
AI
is
at
the
forefront
of
open-source
research,
and
anyone
with
access
to
the
internet
and
consumer-grade
hardware
can
develop
harmful
AI
systems.
I
suspect
that
the
emergence
of
autonomous
weapons
is
inevitable,
and
believe
that
there
will
soon
be
a
need
for
new
technological
solutions
to
counter
such
weapons.
This
can
result
in
a
cat-and-mouse
cycle
that
fuels
the
evolution
of
AI-enabled
weapons,
which
may
give
rise
to
serious
existential
risks
in
the
long-term.
&nbsp;
What
can
we
do
to
keep
AI
systems
safe
from
these
adversarial
agents?
The
first
and
foremost
step
is
education:
All
AI
engineers
and
practitioners
need
to
learn
about
the
vulnerabilities
of
AI
technologies,
and
consider
the
relevant
risks
in
the
design
and
implementation
of
their
systems.
As
for
more
technical
recommendations,
there
are
various
proposals
and
solution
concepts
that
can
be
employed.
For
example,
training
machine
learning
agents
in
adversarial
settings
can
improve
their
resilience
and
robustness
against
evasion
and
policy
manipulation
attacks
(e.g.,
see
my
paper
titled
&#8220;
Whatever
Does
Not
Kill
Deep
Reinforcement
Learning,
Makes
it
Stronger
&#8220;).
Another
solution
is
to
directly
account
for
the
risk
of
adversarial
attacks
in
the
architecture
of
the
agent
(e.g.,
Bayesian
approaches
to
risk
modeling).
There
is
however
a
major
gap
in
this
area,
and
it&#8217;s
the
need
for
universal
metrics
and
methodologies
for
evaluating
the
robustness
of
AI
agents
against
adversarial
attacks.
Current
solutions
are
mostly
ad
hoc,
and
fail
to
provide
general
measures
of
resilience
against
all
types
of
attacks.
&nbsp;
Is
there
anything
else
that
you
would
like
to
share
about
any
of
these
topics?
In
2014,
Scully
et
al.
published
a
paper
at
the
NeurIPS
conference
with
a
very
enlightening
topic:
&#8220;
Machine
Learning:
The
High-Interest
Credit
Card
of
Technical
Debt
&#8220;.
Even
with
all
the
advancements
of
the
field
in
the
past
few
years,
this
statement
has
yet
to
lose
its
validity.
Current
state
of
AI
and
machine
learning
is
nothing
short
of
awe-inspiring,
but
we
are
yet
to
fill
a
significant
number
of
major
gaps
in
both
the
foundation
and
the
engineering
dimensions
of
AI.
This
fact,
in
my
opinion,
is
the
most
important
takeaway
of
our
conversation.
I
of
course
do
not
mean
to
discourage
the
commercial
adoption
of
AI
technologies,
but
only
wish
to
enable
the
engineering
community
to
account
for
the
risks
and
limits
of
current
AI
technologies
in
their
decisions.
I
really
enjoyed
learning
about
the
safety
and
security
challenges
about
different
types
of
AI
systems.
This
is
trully
something
that
individuals,
corporations,
and
governments
need
to
become
aware
of.
Readers
who
wish
to
learn
more
should
visit
Secure
and
Assured
Intelligent
Learning
(SAIL)
Lab
.
<!--mvp-content-main--
>
Related
Topics:
AGI
autonomous
weapons
cybersecurity
safety
terrorism
<!--mvp-post-tags--
>
<!--posts-nav-link--
>
Up
Next
Paolo
Pirjanian,
CEO
and
Founder
of
Embodied
&#8211;
Interview
Series
<!--mvp-prev-next-text--
>
<!--mvp-next-cont-in--
>
<!--mvp-prev-next-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-next-post-wrap--
>
Don&#039;t
Miss
Dr.
Eric
Dusseux,
CEO
of
BIONIK
Laboratories
&#8211;
Interview
Series
<!--mvp-prev-next-text--
>
<!--mvp-prev-cont-in--
>
<!--mvp-prev-cont-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-prev-post-wrap--
>
<!--mvp-prev-next-wrap--
>
<!--mvp-author-box-img--
>
Antoine
Tardif
<!--mvp-author-box-soc-wrap--
>
<!--mvp-author-box-head--
>
<!--mvp-author-box-in--
>
<!--mvp-author-box-out--
>
Antoine
Tardif
is
a
Futurist
who
is
passionate
about
the
future
of
AI
and
robotics.
He
is
the
CEO
of
BlockVentures.com
,
and
has
invested
in
over
50
AI
&
blockchain
projects.
He
is
the
Co-Founder
of
Securities.io
a
news
website
focusing
on
digital
assets,
digital
securities
and
investing.
He
is
a
founding
partner
of
unite.AI
&
a
member
of
the
Forbes
Technology
Council.
<!--mvp-author-box-text--
>
<!--author__item--
>
<!--mvp-author-box-wrap--
>
<!--mvp-org-logo--
>
<!--mvp-org-wrap--
>
<!--mvp-content-bot--
>
<!--mvp-content-body-top--
>
You
may
like
<!--mvp-related-img--
>
Pieter
VanIperen,
Founder
&#038;
Managing
Partner
of
PWV
Consultants
&#8211;
Cybersecurity
Interviews
<!--mvp-related-text--
>
<!--mvp-related-img--
>
How
Asimov&#8217;s
Three
Laws
of
Robotics
Impacts
AI
<!--mvp-related-text--
>
<!--mvp-related-img--
>
Researchers
Say
Humans
Would
Not
Be
Able
To
Control
Superintelligent
AI
<!--mvp-related-text--
>
<!--mvp-related-img--
>
Acronis
SCS
and
Leading
Academics
Partner
to
Develop
AI-based
Risk
Scoring
Model
<!--mvp-related-text--
>
<!--mvp-related-img--
>
How
AI
Will
Impact
Both
Cybersecurity
and
Cyber
Attacks
<!--mvp-related-text--
>
<!--mvp-related-img--
>
Top
10
Cybersecurity
Software
Programs
and
Tools
<!--mvp-related-text--
>
<!--mvp-related-posts--
>
<!--mvp-cont-read-wrap--
>
<!--mvp-content-body--
>
<!--mvp-post-soc-in--
>
<!--mvp-post-soc-out--
>
<!--mvp-content-wrap--
>
<!--mvp-post-content--
>
<!--mvp-post-main-in--
>
AI
News
Researchers
Aim
To
Boost
Drug
Discovery
Speed
By
Calculating
Binding
Efficiencies
With
AI
Kushal
Chakrabarti,
VP
of
Research
and
Data
Science
at
Opendoor
&#8211;
Interview
Series
Researchers
Develop
AI-Backed
Method
of
Seed
Analysis
DataGen
Secures
$18
Million
in
Investments
to
Create
Synthetic
Data
for
AIs
Alex
Sappok,
CEO
of
RaySecur
&#8211;
Interview
Series
<!--mvp-side-wrap--
>
<!--mvp-post-main-out--
>
<!--mvp-post-main--
>
<!--mvp-main-box--
>
<!--mvp-article-cont--
>
<!--mvp-article-wrap--
>
<!--mvp-main-body-wrap--
>
Meet
the
Team
Our
Charter
Press
Tools
Contact
Us
Advertiser
Disclosure
:
Unite.AI
is
committed
to
rigorous
editorial
standards
to
provide
our
readers
with
accurate
information
and
news.
We
may
receive
compensation
when
you
click
on
links
to
products
we
reviewed.
Copyright
©
2021
Unite.AI
Editorial
Policy
Privacy
Policy
Terms
and
Conditions
<!--mvp-site-main--
>
<!--mvp-site-wall--
>
<!--mvp-site--
>
<!--mvp-fly-top--
>
<!--mvp-fly-fade--
>

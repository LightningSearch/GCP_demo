<!--mvp-fly-logo--
>
<!--mvp-fly-top-in--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-fly-top-out--
>
<!--mvp-fly-menu-top--
>
News
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
Data
Science
COVID-19
Cybersecurity
Deep
Learning
Deepfakes
Education
Environment
Ethics
Facial
Recognition
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Decision
Tree
Data
Science
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Edge
AI
&#038;
Edge
Computing
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
Generative
vs
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Linear
Regression
Long
Short-Term
Memory
(LSTM)
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Courses
Conferences
AI
Conferences
Cybersecurity
Conferences
Robotics
Conferences
Interviews
Thought
Leaders
Newsletters
Organizations
Meet
the
Team
Our
Charter
Contact
Us
<!--mvp-fly-menu-wrap--
>
Connect
with
us
<!--mvp-fly-soc-wrap--
>
<!--mvp-fly-wrap--
>
<!--mvp-search-box--
>
<!--mvp-search-but-wrap--
>
<!--mvp-search-wrap--
>
<!--mvp-fly-but-wrap--
>
<!--mvp-nav-small-left--
>
Unite.AI
<!--mvp-nav-small-logo--
>
What
is
a
Decision
Tree?
<!--mvp-drop-nav-title--
>
News
A
-
C
Artificial
General
Intelligence
Artificial
Neural
Networks
Autonomous
Vehicles
Brain
Machine
Interface
COVID-19
Cybersecurity
D
-
F
Data
Science
Deepfakes
Deep
Learning
Education
Ethics
Environment
Facial
Recognition
G
-
Q
Healthcare
Investments
Manufacturing
Natural
Language
Processing
Quantum
Computing
R
-
Z
Regulation
Reinforcement
Learning
Robotics
Speech
Recognition
Startups
Surveillance
Virtual
Assistants
AI
101
A
-
D
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
D
-
K
Dimensionality
Reduction
Edge
AI
Ensemble
Learning
Federated
Learning
Few-Shot
Learning
Generative
Adversarial
Network
(GAN)
Generative
vs.
Discriminative
Models
Gradient
Boosting
Gradient
Descent
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
L
-
Q
Linear
Regression
Long
Short-Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Recurrent
Neural
Networks
R
-
Z
Reinforcement
Learning
Robotic
Process
Automation
(RPA)
Structured
vs
Unstructured
Data
Supervised
vs
Unsupervised
Learning
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
Certifications
Blockchain
Cloud
Cybersecurity
Data
Science
Machine
Learning
Natural
Language
Processing
Python
Robotic
Process
Automation
TensorFlow
Conferences
Artificial
Intelligence
Cybersecurity
Robotics
Interviews
Thought
Leaders
Meet
the
Team
Contact
<!--mvp-nav-menu--
>
<!--mvp-nav-small-mid-right--
>
<!--mvp-nav-small-mid--
>
<!--mvp-nav-small-left-in--
>
<!--mvp-nav-small-left-out--
>
<!--mvp-nav-small-cont--
>
<!--mvp-nav-small-right-in--
>
<!--mvp-nav-small-right--
>
<!--mvp-nav-small-right-out--
>
<!--mvp-nav-small-wrap--
>
<!--mvp-main-box--
>
<!--mvp-main-nav-small-cont--
>
<!--mvp-main-nav-small--
>
<!--mvp-main-nav-wrap--
>
<!--mvp-main-head-wrap--
>
AI
Masterclass:
Terminology
(A
to
D)
Autoencoder
Backpropagation
Bayes
Theorem
Big
Data
Computer
Vision
Confusion
Matrix
Convolutional
Neural
Networks
Cybersecurity
Data
Science
Decision
Tree
Deepfakes
Deep
Learning
Deep
Reinforcement
Learning
Dimensionality
Reduction
Terminology
(E
to
K)
Edge
AI
Ensemble
Learning
Federated
Learning
Generative
Adversarial
Network
Generative
vs.
Discriminative
Gradient
Boosting
Gradient
Descent
Few-Shot
Learning
Image
Classification
K-Means
Clustering
K-Nearest
Neighbors
Terminology
(L
to
Q)
Linear
Regression
Long-Short
Term
Memory
Machine
Learning
Meta-Learning
Nanobots
Natural
Language
Processing
Natural
Language
Understanding
Neural
Networks
Overfitting
Quantum
Computers
Terminology
(R
to
Z)
Reinforcement
Learning
Robotic
Process
Automation
Structured
vs
Unstructured
Supervised
vs
Unsupervised
Support
Vector
Machines
Synthetic
Data
Text
Classification
Transfer
Learning
Transformer
Neural
Networks
Turing
Test
AI
101
What
is
a
Decision
Tree?
<!--mvp-author-info-thumb--
>
Updated
7
months
ago
&nbsp;on
August
23,
2020
<!--mvp-author-info-date--
>
By
Daniel
Nelson
<!--mvp-author-info-name--
>
<!--mvp-author-info-text--
>
<!--mvp-author-info-wrap--
>
Table
Of
Contents
<!--mvp-post-img-hide--
>
What
is
a
Decision
Tree?
A
decision
tree
is
a
useful
machine
learning
algorithm
used
for
both
regression
and
classification
tasks.
The
name
“decision
tree”
comes
from
the
fact
that
the
algorithm
keeps
dividing
the
dataset
down
into
smaller
and
smaller
portions
until
the
data
has
been
divided
into
single
instances,
which
are
then
classified.
If
you
were
to
visualize
the
results
of
the
algorithm,
the
way
the
categories
are
divided
would
resemble
a
tree
and
many
leaves.
That’s
a
quick
definition
of
a
decision
tree,
but
let’s
take
a
deep
dive
into
how
decision
trees
work.
Having
a
better
understanding
of
how
decision
trees
operate,
as
well
as
their
use
cases,
will
assist
you
in
knowing
when
to
utilize
them
during
your
machine
learning
projects.
Format
of
a
Decision
Tree
A
decision
tree
is
a
lot
like
a
flowchart.
To
utilize
a
flowchart
you
start
at
the
starting
point,
or
root,
of
the
chart
and
then
based
on
how
you
answer
the
filtering
criteria
of
that
starting
node
you
move
to
one
of
the
next
possible
nodes.
This
process
is
repeated
until
an
ending
is
reached.
Decision
trees
operate
in
essentially
the
same
manner,
with
every
internal
node
in
the
tree
being
some
sort
of
test/filtering
criteria.
The
nodes
on
the
outside,
the
endpoints
of
the
tree,
are
the
labels
for
the
datapoint
in
question
and
they
are
dubbed
“leaves”.
The
branches
that
lead
from
the
internal
nodes
to
the
next
node
are
features
or
conjunctions
of
features.
The
rules
used
to
classify
the
datapoints
are
the
paths
that
run
from
the
root
to
the
leaves.
Algorithms
for
Decision
Trees
Decision
trees
operate
on
an
algorithmic
approach
which
splits
the
dataset
up
into
individual
data
points
based
on
different
criteria.
These
splits
are
done
with
different
variables,
or
the
different
features
of
the
dataset.
For
example,
if
the
goal
is
to
determine
whether
or
not
a
dog
or
cat
is
being
described
by
the
input
features,
variables
the
data
is
split
on
might
be
things
like
“claws”
and
“barks”.
So
what
algorithms
are
used
to
actually
split
the
data
into
branches
and
leaves?
There
are
various
methods
that
can
be
used
to
split
a
tree
up,
but
the
most
common
method
of
splitting
is
probably
a
technique
referred
to
as
“
recursive
binary
split
”.
When
carrying
out
this
method
of
splitting,
the
process
starts
at
the
root
and
the
number
of
features
in
the
dataset
represents
the
possible
number
of
possible
splits.
A
function
is
used
to
determine
how
much
accuracy
every
possible
split
will
cost,
and
the
split
is
made
using
the
criteria
that
sacrifices
the
least
accuracy.
This
process
is
carried
out
recursively
and
sub-groups
are
formed
using
the
same
general
strategy.
In
order
to
determine
the
cost
of
the
split
,
a
cost
function
is
used.
A
different
cost
function
is
used
for
regression
tasks
and
classification
tasks.
The
goal
of
both
cost
functions
is
to
determine
which
branches
have
the
most
similar
response
values,
or
the
most
homogenous
branches.
Consider
that
you
want
test
data
of
a
certain
class
to
follow
certain
paths
and
this
makes
intuitive
sense.
In
terms
of
the
regression
cost
function
for
recursive
binary
split,
the
algorithm
used
to
calculate
the
cost
is
as
follows:
sum(y
&#8211;
prediction)^2
The
prediction
for
a
particular
group
of
data
points
is
the
mean
of
the
responses
of
the
training
data
for
that
group.
All
the
data
points
are
run
through
the
cost
function
to
determine
the
cost
for
all
the
possible
splits
and
the
split
with
the
lowest
cost
is
selected.
Regarding
the
cost
function
for
classification,
the
function
is
as
follows:
G
=
sum(pk
*
(1
&#8211;
pk))
This
is
the
Gini
score,
and
it
is
a
measurement
of
the
effectiveness
of
a
split,
based
on
how
many
instances
of
different
classes
are
in
the
groups
resulting
from
the
split.
In
other
words,
it
quantifies
how
mixed
the
groups
are
after
the
split.
An
optimal
split
is
when
all
the
groups
resulting
from
the
split
consist
only
of
inputs
from
one
class.
If
an
optimal
split
has
been
created
the
“pk”
value
will
be
either
0
or
1
and
G
will
be
equal
to
zero.
You
might
be
able
to
guess
that
the
worst-case
split
is
one
where
there
is
a
50-50
representation
of
the
classes
in
the
split,
in
the
case
of
binary
classification.
In
this
case,
the
“pk”
value
would
be
0.5
and
G
would
also
be
0.5.
The
splitting
process
is
terminated
when
all
the
data
points
have
been
turned
into
leaves
and
classified.
However,
you
may
want
to
stop
the
growth
of
the
tree
early.
Large
complex
trees
are
prone
to
overfitting,
but
several
different
methods
can
be
used
to
combat
this.
One
method
of
reducing
overfitting
is
to
specify
a
minimum
number
of
data
points
that
will
be
used
to
create
a
leaf.
Another
method
of
controlling
for
overfitting
is
restricting
the
tree
to
a
certain
maximum
depth,
which
controls
how
long
a
path
can
stretch
from
the
root
to
a
leaf.
Another
process
involved
in
the
creation
of
decision
trees
is
pruning.
Pruning
can
help
increase
the
performance
of
a
decision
tree
by
stripping
out
branches
containing
features
that
have
little
predictive
power/little
importance
for
the
model.
In
this
way,
the
complexity
of
the
tree
is
reduced,
it
becomes
less
likely
to
overfit,
and
the
predictive
utility
of
the
model
is
increased.
When
conducting
pruning,
the
process
can
start
at
either
the
top
of
the
tree
or
the
bottom
of
the
tree.
However,
the
easiest
method
of
pruning
is
to
start
with
the
leaves
and
attempt
to
drop
the
node
that
contains
the
most
common
class
within
that
leaf.
If
the
accuracy
of
the
model
doesn’t
deteriorate
when
this
is
done,
then
the
change
is
preserved.
There
are
other
techniques
used
to
carry
out
pruning,
but
the
method
described
above
&#8211;
reduced
error
pruning
&#8211;
is
probably
the
most
common
method
of
decision
tree
pruning.
Considerations
For
Using
Decision
Trees
Decision
trees
are
often
useful
when
classification
needs
to
be
carried
out
but
computation
time
is
a
major
constraint.
Decision
trees
can
make
it
clear
which
features
in
the
chosen
datasets
wield
the
most
predictive
power.
Furthermore,
unlike
many
machine
learning
algorithms
where
the
rules
used
to
classify
the
data
may
be
hard
to
interpret,
decision
trees
can
render
interpretable
rules.
Decision
trees
are
also
able
to
make
use
of
both
categorical
and
continuous
variables
which
means
that
less
preprocessing
is
needed,
compared
to
algorithms
that
can
only
handle
one
of
these
variable
types.
Decision
trees
tend
not
to
perform
very
well
when
used
to
determine
the
values
of
continuous
attributes.
Another
limitation
of
decision
trees
is
that,
when
doing
classification,
if
there
are
few
training
examples
but
many
classes
the
decision
tree
tends
to
be
inaccurate.
<!--mvp-content-main--
>
Related
Topics:
101
Decision
Tree
<!--mvp-post-tags--
>
<!--posts-nav-link--
>
Up
Next
What
is
Transfer
Learning?
<!--mvp-prev-next-text--
>
<!--mvp-next-cont-in--
>
<!--mvp-prev-next-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-next-post-wrap--
>
Don&#039;t
Miss
What
is
Reinforcement
Learning?
<!--mvp-prev-next-text--
>
<!--mvp-prev-cont-in--
>
<!--mvp-prev-cont-out--
>
<!--mvp-prev-next-cont--
>
<!--mvp-prev-post-wrap--
>
<!--mvp-prev-next-wrap--
>
<!--mvp-author-box-img--
>
Daniel
Nelson
<!--mvp-author-box-soc-wrap--
>
<!--mvp-author-box-head--
>
<!--mvp-author-box-in--
>
<!--mvp-author-box-out--
>
Blogger
and
programmer
with
specialties
in
Machine
Learning
and
Deep
Learning
topics.
Daniel
hopes
to
help
others
use
the
power
of
AI
for
social
good.
<!--mvp-author-box-text--
>
<!--author__item--
>
<!--mvp-author-box-wrap--
>
<!--mvp-org-logo--
>
<!--mvp-org-wrap--
>
<!--mvp-content-bot--
>
<!--mvp-content-body-top--
>
You
may
like
<!--mvp-related-img--
>
What
is
Few-Shot
Learning?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
Are
Transformer
Neural
Networks?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
Edge
AI
&#038;
Edge
Computing?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
Ensemble
Learning?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
Dimensionality
Reduction?
<!--mvp-related-text--
>
<!--mvp-related-img--
>
What
is
a
Generative
Adversarial
Network
(GAN)?
<!--mvp-related-text--
>
<!--mvp-related-posts--
>
<!--mvp-cont-read-wrap--
>
<!--mvp-content-body--
>
<!--mvp-post-soc-in--
>
<!--mvp-post-soc-out--
>
<!--mvp-content-wrap--
>
<!--mvp-post-content--
>
<!--mvp-post-main-in--
>
<!--mvp-post-main-out--
>
<!--mvp-post-main--
>
<!--mvp-main-box--
>
<!--mvp-article-cont--
>
<!--mvp-article-wrap--
>
<!--mvp-main-body-wrap--
>
Meet
the
Team
Our
Charter
Press
Tools
Contact
Us
Advertiser
Disclosure
:
Unite.AI
is
committed
to
rigorous
editorial
standards
to
provide
our
readers
with
accurate
information
and
news.
We
may
receive
compensation
when
you
click
on
links
to
products
we
reviewed.
Copyright
©
2021
Unite.AI
Editorial
Policy
Privacy
Policy
Terms
and
Conditions
<!--mvp-site-main--
>
<!--mvp-site-wall--
>
<!--mvp-site--
>
<!--mvp-fly-top--
>
<!--mvp-fly-fade--
>

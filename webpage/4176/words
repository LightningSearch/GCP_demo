Skip
to
content
Sign&nbsp;up
Sign&nbsp;up
Why
GitHub?
Features
&rarr;
Mobile
&rarr;
Actions
&rarr;
Codespaces
&rarr;
Packages
&rarr;
Security
&rarr;
Code
review
&rarr;
Project
management
&rarr;
Integrations
&rarr;
GitHub
Sponsors
&rarr;
Customer
stories
&rarr;
Team
Enterprise
Explore
Explore
GitHub
&rarr;
Learn
and
contribute
Topics
&rarr;
Collections
&rarr;
Trending
&rarr;
Learning
Lab
&rarr;
Open
source
guides
&rarr;
Connect
with
others
The
ReadME
Project
&rarr;
Events
&rarr;
Community
forum
&rarr;
GitHub
Education
&rarr;
GitHub
Stars
program
&rarr;
Marketplace
Pricing
Plans
&rarr;
Compare
plans
&rarr;
Contact
Sales
&rarr;
Education
&rarr;
-->
In
this
repository
All
GitHub
↵
Jump
to
↵
No
suggested
jump
to
results
In
this
repository
All
GitHub
↵
Jump
to
↵
In
this
organization
All
GitHub
↵
Jump
to
↵
In
this
repository
All
GitHub
↵
Jump
to
↵
Sign
in
Sign
up
Sign
up
{{
message
}}
<include-fragment
class="js-notification-shelf-include-fragment"
data-base-src="https://github.com/notifications/beta/shelf">
NVIDIA
/
TensorRT
Notifications
Star
3.5k
Fork
803
Code
Issues
195
Pull
requests
22
Actions
Projects
0
Security
<include-fragment
src="/NVIDIA/TensorRT/security/overall-count"
accept="text/fragment+html">
Insights
More
<details-menu
role="menu"
class="dropdown-menu
dropdown-menu-sw
">
Code
Issues
Pull
requests
Actions
Projects
Security
Insights
release/5.1
Switch
branches/tags
<input-demux
data-action="tab-container-change:input-demux#storeInput
tab-container-changed:input-demux#updateInput">
<tab-container
class="d-flex
flex-column
js-branches-tags-tabs"
style="min-height:
0;">
Branches
Tags
<ref-selector
type="branch"
data-targets="input-demux.sinks"
data-action="
input-entered:ref-selector#inputEntered
tab-selected:ref-selector#tabSelected
focus-list:ref-selector#focusFirstListMember
"
query-endpoint="/NVIDIA/TensorRT/refs"
cache-key="v0:1615404731.549014"
current-committish="cmVsZWFzZS81LjE="
default-branch="bWFzdGVy"
name-with-owner="TlZJRElBL1RlbnNvclJU"
>
Nothing
to
show
{{
refName
}}
default
View
all
branches
<ref-selector
type="tag"
data-action="
input-entered:ref-selector#inputEntered
tab-selected:ref-selector#tabSelected
focus-list:ref-selector#focusFirstListMember
"
data-targets="input-demux.sinks"
query-endpoint="/NVIDIA/TensorRT/refs"
cache-key="v0:1615404731.549014"
current-committish="cmVsZWFzZS81LjE="
default-branch="bWFzdGVy"
name-with-owner="TlZJRElBL1RlbnNvclJU"
>
Nothing
to
show
{{
refName
}}
default
View
all
tags
TensorRT
/
demo
/
BERT
/
Go
to
file
TensorRT
/
demo
/
BERT
/
This
branch
is
17
commits
ahead,
142
commits
behind
master.
Pull
request
Compare
Latest
commit
eweill-nv
and
rajeevsrao
Update
Dockerfile
to
fix
container
build
for
blog
&hellip;
<include-fragment
accept="text/fragment+html"
src="/NVIDIA/TensorRT/commit/ead8e240008e7a9983792a0c0285c6660f59c848/rollup?direction=sw"
class="d-inline">
ead8e24
<relative-time
datetime="2019-08-23T18:43:06Z"
class="no-wrap">Aug
23,
2019
Update
Dockerfile
to
fix
container
build
for
blog
Signed-off-by:
Rajeev
Rao
&lt;rajeevrao@nvidia.com&gt;
ead8e24
Git
stats
History
Files
<include-fragment
src="/NVIDIA/TensorRT/file-list/release/5.1/demo/BERT">
Permalink
Failed
to
load
latest
commit
information.
Type
Name
Latest
commit
message
Commit
time
. .
docker
&nbsp;
&nbsp;
helpers
&nbsp;
&nbsp;
layers
&nbsp;
&nbsp;
plugins
&nbsp;
&nbsp;
python
&nbsp;
&nbsp;
util
&nbsp;
&nbsp;
CMakeLists.txt
&nbsp;
&nbsp;
Dockerfile
&nbsp;
&nbsp;
LICENSE
&nbsp;
&nbsp;
README.md
&nbsp;
&nbsp;
sampleBERT.cpp
&nbsp;
&nbsp;
README.md
BERT
Example
using
the
TensorRT
C++
API
This
example
demonstrates
how
to
write
a
TensorRT
model
with
custom
plugins
to
implement
the
BERT
encoder
with
SQuAD
output
layer.
As
input,
the
code
expects
tokenized
sentences,
input
masks
and
segment
ids,
as
in
the
reference
Tensorflow
implementation
.
This
code
was
tested
on
NVIDIA
V100
and
T4
GPUs,
and
will
only
compile
to
those
architectures.
Prerequisites
To
build
the
TensorRT
OSS
components,
ensure
you
meet
the
following
package
requirements:
System
Packages
CUDA
Recommended
versions:
cuda-10.1
+
cuDNN-7.5
GNU
Make
&gt;=
v4.1
CMake
&gt;=
v3.8
Python
Recommended
versions:
Python2
&gt;=
v2.7.15
Python3
&gt;=
v3.6.5
PIP
&gt;=
v19.0
Essential
libraries
and
utilities
Git
,
pkg-config
,
Wget
,
Zlib
Optional
Packages
Containerized
builds
Docker
&gt;=
1.12
NVIDIA
Docker
&gt;=
2.0
TensorRT
Release
TensorRT
v5.1.5
Example
Workflow
The
example
provides
scripts
to
convert
fine-tuned
Tensorflow
model
checkpoints
into
a
simple
binary
format
that
can
be
read
by
sample
binary.
The
high-level
workflow
consists
of
the
following
steps:
Download
a
pre-trained
BERT
SQuAD
checkpoint
from
NGC
model
registry
(See
optional
section
if
you
would
like
to
train
your
own
model)
Convert
the
fine-tuned
checkpoint
into
our
simple
format,
described
in
the
appendix
(the
original
weights
are
assumed
to
be
float32
values)
Generate
a
test
input/output
pair
(input
sequences
are
assumed
to
be
int32
values)
Build
and
run
the
sample
1.
Download
a
pre-trained
BERT
SQuAD
checkpoint
from
NGC
model
registry
wget
-O
bert-base-squad1.1.zip
https://api.ngc.nvidia.com/v2/models/nvidia/bert_tf_v1_1_base_fp32_128/versions/2/zip
unzip
bert-base-squad1.1.zip
-d
squad_output_path
Below,
we
will
refer
to
the
location
&lt;squad
output
path&gt;/model.ckpt-&lt;number&gt;
as
shell
variable
CHECKPOINT
and
the
path
to
the
folder
that
contains
the
bert_config.json
as
BERT_PATH
.
(Optional)
Downloading
the
BERT
reference
code
and
pre-trained
language
model,
and
running
SQuAD
Fine-tuning
Please
follow
the
instructions
in
the
DeepLearningExamples
repository
for
fine-tuning
SQuAD,
which
involves
downloading
the
pre-trained
language
model
as
well
as
the
SQuAD
training
data.
Then,
in
the
scripts
folder,
there
is
run_squad.sh
script,
that
adds
a
SQuAD-specific
task
layer
to
BERT
and
performs
the
fine-tuning.
This
will
create
three
files
prefixed
with
model.ckpt-&lt;number&gt;
that
contain
the
fine-tuned
model
parameters,
in
the
specified
output
directory.
2.
Convert
the
fine-tuned
checkpoint
into
a
simple
format
Python
scripts
in
step
2
and
3
require
Tensorflow
on
the
system.
We
tested
using
tensorflow:19.06-py3
NGC
container
image.
The
SQuAD
fine-tuned
Tensorflow
checkpoint
can
be
converted
using
the
following
command:
python
helpers/convert_weights.py
-m
$CHECKPOINT
-o
&lt;weight
path&gt;/filename
This
will
generate
a
file
&lt;weight
path&gt;/&lt;filename&gt;.weights
.
The
path
that
contains
the
weights
file,
will
be
referred
to
as
WEIGHT_PATH
.
3.
Generate
an
input/output
pair
To
run
the
sample
on
random
inputs
and
compare
the
output
to
the
reference
Tensorflow
implementation,
the
following
command
produces
test
inputs
and
outputs:
python
helpers/generate_dbg.py
-f
$CHECKPOINT
-p
$BERT_PATH
-o
$OUTPUT_PATH
-s
&lt;seq.len.&gt;
-b
&lt;batch
size&gt;
Please
refer
to
the
help
of
generate_dbg.py
for
more
options.
4.
Build
and
run
the
example
The
C++
example
was
tested
using
TensorRT
OSS
docker
container
image
created
by
following
the
instruction
in
this
link
This
example
uses
cmake
and
can
be
built
with
the
following
steps:
mkdir
build
cd
build
cmake
..
make
-j
This
will
produce
an
executable
sample_bert
in
the
build
folder.
The
binary
sample_bert
requires
as
arguments
the
paths
that
contain
bert_config.json
(from
the
pre-trained
BERT
checkpoint),
bert.weights
and
test_inputs.weights_int32
and
test_outputs.weights
as
generated
by
the
steps
above.
build/sample_bert
-d
$WEIGHT_PATH
-d
$OUTPUT_PATH
--fp16
--nheads
&lt;num_attention_heads&gt;
&lt;num_attention_heads&gt;
refers
to
the
number
of
attention
heads
and
can
be
found
in
the
bert_config.json
.
Appendix
Description
of
the
binary
format
The
example
expects
weights
and
inputs
in
a
simple
tensor
dictionary
format.
It
consists
of
an
integer
in
the
first
line
N
denoting
the
number
of
entries
in
the
dictionary.
Then
there
are
N
lines,
each
line
following
the
format
[tensor
name:
String]
[element
type:
DataType]
[number
of
dimensions
D:
int]
[dim1,
dim2,
...,
dimD]
[binary
tensor
data]\n
DataType
is
the
nvinfer1
enumeration,
that
encodes
types
as
numbers.
E.g.
DataType::kFLOAT
=
0
(float32)
and
DataType::kINT32
=
3
.
The
binary
tensor
data
is
dim1
*
dim2
*
...
*
dimD
*
sizeof(type)
bytes
followed
by
a
line
break.
Methods
to
read
this
format
can
be
found
in
dataUtils.hpp
&copy;
2021
GitHub,
Inc.
Terms
Privacy
Security
Status
Docs
Contact
GitHub
Pricing
API
Training
Blog
About
You
can’t
perform
that
action
at
this
time.
You
signed
in
with
another
tab
or
window.
Reload
to
refresh
your
session.
You
signed
out
in
another
tab
or
window.
Reload
to
refresh
your
session.
<details-dialog
class="Box
Box--overlay
d-flex
flex-column
anim-fade-in
fast
hx_rsm-dialog
hx_rsm-modal">
